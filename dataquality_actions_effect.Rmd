---
title: "Data Quality Assessment Report"
subtitle: "Analysis of Completeness, Validity, Distribution Patterns, and Intervention Effects"
author: "Spheres-Lobar"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    theme: cosmo
    code_folding: hide
  pdf_document:
    latex_engine: xelatex
    keep_tex: false
params:
  # ── Facility & Time Selection ──
  puskesmas: "Puskesmas Gerung"           # Target health facility
  district: "Lombok Barat"                 # District name
  selected_date: "2025-10-31"              # Single date or "latest"
  date_from: "2025-09-01"                  # Range start (for range modes)
  date_to: "2025-10-31"                    # Range end (for range modes)
  
  # ── Staff Selection ──
  staff_name: NULL                         # Target staff name (NULL = all staff, or specify exact name from perawat_bidan_nutrisionist_sanitarian)
  
  # ── Data Source & Scope ──
  data_source: "ePuskesmas - Pelayanan Pasien"  # Source system name
  data_scope: "range_pair"                 # Options: "daily", "range", "range_pair", "week", "month"
  
  # ── Heaping Analysis Parameters ──
  heaping_mode: "range"                    # "daily" or "range"
  heaping_window_days: 3                   # Rolling window size (days)
  heaping_range_style: "rolling"           # "single" or "rolling"
  heaping_stride_days: 3                   # Stride for rolling windows
  
  # ── events file path ── 
  events_path: "C:/Users/User/Documents/events_gerung.xlsx"
---

<!--
═══════════════════════════════════════════════════════════════════════════════
DATA QUALITY ASSESSMENT & INTERVENTION IMPACT ANALYSIS REPORT
═══════════════════════════════════════════════════════════════════════════════

OVERVIEW:
  This R Markdown script provides a comprehensive, automated data quality 
  assessment and intervention impact analysis for health facility encounter data.
  It combines traditional data quality metrics with advanced time-series analysis
  to evaluate the effectiveness of data quality interventions.

═══════════════════════════════════════════════════════════════════════════════
MAIN SECTIONS & WORKFLOW:
═══════════════════════════════════════════════════════════════════════════════

PART 1: SETUP & DATA ACQUISITION (Lines 73-497)
  1.1  Global Setup & Libraries (73-107)
       - Configure knitr options and load required R packages
       - Libraries: bigrquery, dplyr, ggplot2, forecast, etc.
  
  1.2  Global Helper Functions (110-232)
       - Utility functions for date parsing, formatting, and data cleaning
       - Phone number and NIK validation helpers
       - Safe numeric conversion and type inference
  
  1.3  BigQuery Connection (236-282)
       - Authenticate with BigQuery using browser-based OAuth
       - Connect to xxxx project
       - Access xxx table
  
  1.4  Parameters & Metadata (285-338)
       - Parse YAML parameters (facility, dates, scope)
       - Normalize facility names for matching
       - Build human-readable report subtitle
  
  1.5  Build Core Query (343-400)
       - Create lazy BigQuery query with facility filter
       - Parse timestamps from filename and data fields
       - Derive encounter date (day column) for aggregation
  
  1.6  Resolve Windows (403-497)
       - Collect available encounter dates for the facility
       - Resolve requested date to nearest available
       - Build time windows: daily, range, week, month
       - Keep only latest file per day (deduplication)

PART 2: DATA STANDARDIZATION & VALIDATION (Lines 499-689)
  2.1  Schema Standardization (499-650)
       - Map heterogeneous column names to friendly aliases
       - Clean and convert numeric fields (remove units, convert to numeric)
       - Apply data scope selection (daily/week/month/range)
       - Handle comma/period decimal separators
  
  2.2  Clinical Range Guardrails (662-689)
       - Define biologically plausible ranges for clinical variables
       - Weight: 1-300 kg, Height: 30-250 cm
       - Blood pressure: Systole 50-260 mmHg, Diastole 30-150 mmHg
       - Heart Rate: 20-220 bpm, Respiratory Rate: 5-80 breaths/min
       - Temperature: 30-43°C, Waist Circumference: 10-200 cm

PART 3: SUMMARY REPORTING (Lines 693-2252)
  3.1  Summary Card UI (693-813)
       - Build HTML card with report metadata
       - Display facility name, date range, row count, staff name
       - Show last upload timestamp and data source
  
  3.2  Completeness & Validity Summary (818-1682)
       - Calculate missing data rates for each variable
       - Validate format compliance (NIK=16 digits, phone=10-13 digits)
       - Check date validity and clinical range compliance
       - Generate quantile summaries for numeric variables
       - Display interactive DataTables with color-coded thresholds
  
  3.3  Staff-Level Summary (1684-1935)
       - One detailed table per staff member (all variables)
       - Same column structure as Section 3.2
       - Shows: Rows, Missing %, Non Missing %, Unique, Not Valid %, etc.
       - Separate tables for administrative and clinical variables
       - Staff comparison summary with performance rankings
       - Identifies best performers and staff needing improvement
  
  3.4  Variable-Level Staff Comparison (1937-2252)
       - One table per variable (all staff as rows)
       - Transposed view of Section 3.3
       - Easy side-by-side comparison of staff on same variable
       - Shows which staff excel or struggle with specific measurements
       - Color-coded by data quality issues
       - Includes variability analysis across staff
       - Identifies variables with inconsistent quality

PART 4: OUTLIER DETECTION & VISUALIZATION (Lines 2254-3497)
  4.1  Outlier Detection (2254-2354)
       - Apply IQR method with configurable multiplier (default: 3.0)
       - Flag and convert outliers to NA (transparent cleaning)
       - Count outliers per variable
       - Display outlier summary table
  
  4.2  Boxplots by Interval (2356-2594)
       - Generate comparative boxplots for current vs previous periods
       - Monthly/weekly/daily comparison options
       - Show distribution changes over time
       - Use consistent scales for fair comparison
  
  4.3  Histogram Comparisons (2595-3262)
       - Create paired histograms (current vs previous window)
       - Use shared breaks for accurate comparison
       - Align Y-axis scales to highlight changes
       - Help identify distributional shifts and anomalies
  
  4.4  Staff-Level Histogram Comparison (3263-3497)
       - Generate histograms per staff member for each variable
       - Stack vertically for easy visual comparison
       - Shared bin breaks and Y-axis scales across all staff
       - Identify staff-specific measurement patterns
       - Detect heaping and rounding differences between staff
       - Support targeted training interventions

PART 5: HEAPING ANALYSIS (DIGIT PREFERENCE DETECTION) (Lines 3498-4385)
  5.1  Heaping Trace System (3500-3562)
       - Non-invasive instrumentation for calculation transparency
       - Step-by-step tracking of intermediate results
       - Optional persistence for auditing
  
  5.2  Heaping Calculation (3565-3820)
       - Extract last digits from integer measurements
       - Extract last tenths from decimal measurements
       - Calculate digit/tenth preference proportions
       - Flag "preferred" digits (appearing ≥15% threshold)
       - Generate summary tables of heaping patterns
       - Support daily vs rolling window modes
  
  5.3  Last Digit Histograms (3822-4269)
       - Visualize last digit distributions
       - Separate integer (0-9) and decimal (0.1-0.9) analyses
       - Expected uniform distribution: ~10% per digit
       - Heaping indicates potential data fabrication or rounding
  
  5.4  Heaping Debug Trace (4271-4522)
       - Display intermediate calculation steps
       - Show denominators, numerators, and proportions
       - Sanity checks to verify calculation integrity
  
  5.5  Export to Excel (4525-4566)
       - Export all heaping results to timestamped Excel file
       - Multiple sheets: preferred_digits_sum, combined_all, labs_tbl, params
       - Include parameters and export timestamp
       - Optional: embed plots as images

PART 6: INTERRUPTED TIME SERIES (ITS) ANALYSIS (Lines 4569-5545)
  6.1  ITS Setup & Helpers (4569-4620)
       - Configure ARIMAX analysis parameters
       - Load forecast, time series packages
       - Define helper functions for intervention regressors:
         * make_cumsum_series: step change (level shift)
         * make_ramp_series: gradual change (trend shift)
  
  6.2  Load Events (4622-4791)
       - Read intervention events from Excel file
       - Parse event dates, types, and intensities
       - Display events summary and timeline visualization
       - Support multiple event types (level, trend, pulse)
  
  6.3  Prepare ITS Data (4793-4811)
       - Aggregate heaping data to daily time series
       - Fill missing dates with interpolation
       - Prepare for ARIMAX modeling
  
  6.4  ITS Analysis Function (4813-5034)
       - Core ARIMAX modeling function per variable
       - Auto.arima for baseline model selection
       - Build intervention regressors (cumulative, ramp)
       - Estimate model coefficients
       - Calculate intervention effects with confidence intervals
       - Generate forecasts with intervention scenarios
  
  6.5  Run ITS for All Variables (5036-5545)
       - Loop through all clinical variables
       - Fit ARIMAX models with intervention terms
       - Extract coefficients, p-values, confidence intervals
       - Calculate cumulative effect sizes
       - Generate diagnostic plots:
         * Time series with intervention markers
         * ACF/PACF of residuals
         * Forecast with uncertainty bands
       - Compare intervention effects across variables
       - Display coefficient comparison table
       - Visualize all-variable overlay to assess consistency

═══════════════════════════════════════════════════════════════════════════════
PURPOSE & APPLICATIONS:
═══════════════════════════════════════════════════════════════════════════════

DATA QUALITY DIMENSIONS:
  1. Completeness: % of non-missing values per field
  2. Validity: Format compliance (NIK, phone) & range checks (clinical)
  3. Consistency: Outlier detection via IQR method
  4. Distribution: Heaping analysis to detect fabrication/rounding patterns

INTERVENTION IMPACT EVALUATION:
  - Measure effectiveness of data quality improvement actions
  - Quantify before/after changes in heaping patterns
  - Identify which interventions had strongest impact
  - Support evidence-based decisions for future improvements

USE CASES:
  ✓ Routine data quality monitoring for health facilities
  ✓ Pre-analysis data quality checks before epidemiological studies
  ✓ Audit trails for data integrity (traceable calculations)
  ✓ Evaluate data quality intervention programs (training, supervision)
  ✓ Identify facilities needing additional support
  ✓ Generate evidence for program managers and funders

═══════════════════════════════════════════════════════════════════════════════
DATA SOURCE:
═══════════════════════════════════════════════════════════════════════════════
  BigQuery Project: xxxx
  Dataset: raw_data
  Table: xxx
  
  Source System: ePuskesmas (electronic primary health care information system)
  Data Type: Patient encounter records (consultations, measurements)
  Geographic Scope: Lombok Barat district, Indonesia
  Time Range: Configurable via parameters (single day to multi-month)

═══════════════════════════════════════════════════════════════════════════════
EXPECTED RUNTIME:
═══════════════════════════════════════════════════════════════════════════════
  - Daily single-facility analysis: ~30-60 seconds
  - Weekly analysis: ~1-2 minutes
  - Monthly analysis: ~2-5 minutes
  - Range pair with heaping: ~3-7 minutes
  - Full ITS analysis with interventions: ~5-10 minutes
  
  (Runtime depends on data volume, date range, and BigQuery response times)

═══════════════════════════════════════════════════════════════════════════════
KEY FEATURES:
═══════════════════════════════════════════════════════════════════════════════
  ✓ Automated outlier detection & cleaning (IQR method)
  ✓ Digit preference analysis (heaping detection)
  ✓ Comparative histograms (current vs previous window)
  ✓ Comprehensive validation rules for clinical variables
  ✓ Transparent calculation tracing (for audit compliance)
  ✓ ARIMAX modeling for intervention impact assessment
  ✓ Event-based analysis (load interventions from Excel)
  ✓ Automated Excel export with timestamped results
  ✓ Interactive HTML output with collapsible sections
  ✓ Publication-ready visualizations

═══════════════════════════════════════════════════════════════════════════════
CUSTOMIZATION OPTIONS:
═══════════════════════════════════════════════════════════════════════════════
  YAML Parameters (lines 17-36):
    - puskesmas: Target health facility name
    - district: District name for reporting
    - selected_date, date_from, date_to: Time windows
    - staff_name: Filter by specific staff (NULL = all staff)
      Column source: perawat_bidan_nutrisionist_sanitarian
    - data_scope: Analysis scope (daily/week/month/range/range_pair)
    - heaping_mode, heaping_window_days: Heaping analysis options
  
  Clinical Thresholds (line ~676):
    - valid_rules tibble: Adjust min/max for clinical plausibility
  
  Heaping Sensitivity (line ~1970):
    - value_prop_threshold: Set digit preference threshold (default 15%)
  
  Excluded Variables (line ~1972):
    - exclude_vars: Remove variables from heaping analysis
  
  ITS Configuration (line ~2930):
    - events_path: Path to intervention events Excel file
    - horizon: Forecast horizon in days
    - Event columns: Customize event file schema

═══════════════════════════════════════════════════════════════════════════════
TECHNICAL NOTES:
═══════════════════════════════════════════════════════════════════════════════
  - Uses lazy evaluation with dbplyr to minimize BigQuery data transfer
  - Deduplicates by keeping latest file per day (based on file_ts)
  - Handles multiple date/timestamp formats gracefully
  - Timezone-aware (Asia/Makassar default)
  - Responsive to missing/malformed data (safe fallbacks throughout)
  - Tested on Windows 10/11, R 4.x
  - Requires valid BigQuery credentials (browser-based OAuth)

═══════════════════════════════════════════════════════════════════════════════
OUTPUT FILES:
═══════════════════════════════════════════════════════════════════════════════
  1. HTML Report: Interactive document with all analyses and visualizations
  2. Excel Export (outputs/heaping_export_YYYYMMDD_HHMMSS.xlsx):
     - preferred_digits_sum: Summary of preferred digits per variable
     - combined_all: Detailed heaping calculations
     - labs_tbl: Date window labels
     - params: Analysis parameters and metadata
  3. Optional PNG plots (outputs/plotheaping_YYYYMMDD_HHMMSS.png)

═══════════════════════════════════════════════════════════════════════════════
-->


# PART 1: SETUP & DATA ACQUISITION {.tabset}

## 1.1 Global Setup & Libraries

```{r setup-global, echo=TRUE, message=FALSE, warning=FALSE}

# ══════════════════════════════════════════════════════════════════════════════
# PART 1.1: GLOBAL SETUP & LIBRARIES
# ══════════════════════════════════════════════════════════════════════════════
# PURPOSE:
#   Configure knitr options and load all required R packages for the analysis
#
# WHAT THIS DOES:
#   1. Set global knitr options for code chunk behavior
#      - echo=FALSE: Hide R code in output (show results only)
#      - message/warning=FALSE: Suppress package loading messages
#      - fig.width/height: Set default plot dimensions
#   
#   2. Load required packages:
#      - bigrquery, DBI, dbplyr: BigQuery database connectivity
#      - dplyr, tidyr, purrr: Data manipulation (tidyverse core)
#      - stringr: String manipulation and pattern matching
#      - lubridate: Date/time parsing and manipulation
#      - ggplot2: Data visualization
#      - kableExtra, DT: Interactive tables
#      - readxl, readr: Data import
#      - htmltools: HTML generation for custom UI components
#
# DEPENDENCIES:
#   All packages must be installed via install.packages() before running
#
# NOTES:
#   - Packages are loaded silently (suppressPackageStartupMessages implicit)
#   - If any package is missing, script will fail with clear error message
# ══════════════════════════════════════════════════════════════════════════════

# Configure default behavior for all code chunks in this document
knitr::opts_chunk$set(
  echo = FALSE,           # Hide R code in rendered output (cleaner reports)
  message = FALSE,        # Suppress informational messages
  warning = FALSE,        # Suppress warning messages
  fig.width=12,           # Default figure width in inches
  fig.height=6            # Default figure height in inches
)


# ── Load Required Packages ──
# Database connectivity for Google BigQuery
library(bigrquery)        # BigQuery R interface
library(DBI)              # Database Interface (generic DB functions)
library(dbplyr)           # dplyr backend for databases (lazy evaluation)

# Data manipulation (tidyverse)
library(dplyr)            # Core data manipulation verbs (filter, mutate, summarize)
library(stringr)          # String operations (pattern matching, replacement)
library(lubridate)        # Date/time parsing and arithmetic
library(tibble)           # Modern data frames (tibbles)
library(tidyr)            # Data reshaping (pivot, nest, separate)
library(purrr)            # Functional programming tools (map, reduce)

# Data import
library(readxl)           # Read Excel files (.xlsx, .xls)
library(readr)            # Fast CSV/text file reading
library(janitor)          # Data cleaning utilities (clean_names, etc.)

# Visualization
library(ggplot2)          # Grammar of graphics plotting system
library(scales)           # Scale functions for ggplot2 (labels, breaks)

# Output formatting
library(kableExtra)       # Enhanced tables for HTML/PDF output
library(htmltools)        # HTML generation utilities
library(DT)               # Interactive DataTables (sortable, filterable)
library(knitr)            # Dynamic report generation (knit R Markdown)


```


## 1.2 Global Helper Functions

```{r global-helpers, echo=TRUE, message=FALSE, warning=FALSE}
# ══════════════════════════════════════════════════════════════════════════════
# PART 1.2: GLOBAL HELPER FUNCTIONS
# ══════════════════════════════════════════════════════════════════════════════
# PURPOSE:
#   Define reusable utility functions used throughout the analysis
#
# FUNCTION CATEGORIES:
#   1. Operators: Custom infix operators (e.g., %||%)
#   2. Date/Time Helpers: Safe parsing, formatting with timezone awareness
#   3. Numeric Helpers: Safe conversion, integer detection
#   4. String Helpers: Normalization, digit extraction
#   5. Phone/NIK Normalization: Indonesian ID and phone number standardization
#   6. Formatting Helpers: Percentages, pretty variable names
#   7. Validation & Type Inference: Automatic type detection for validation
#   8. Object Safety Checks: Test existence and validity of objects
#
# WHY DEFINE HERE:
#   - Avoids code duplication across chunks
#   - Ensures consistent behavior (e.g., all dates use same parsing logic)
#   - Makes code more maintainable and testable
#   - Centralizes configuration (e.g., PHONE_MIN_LEN)
#
# NOTES:
#   - All functions use safe fallbacks (never crash on bad input)
#   - Timezone default: Asia/Makassar (Indonesia Eastern Time)
#   - Functions handle NA, NULL, and edge cases gracefully
# ══════════════════════════════════════════════════════════════════════════════

# ── 1. Custom Operators ──
# Null-coalescing operator: return y if x is NULL or empty, otherwise x
# Usage: user_input %||% default_value
`%||%` <- function(x, y) if (is.null(x) || length(x) == 0) y else x

# ── 2. Date/Time Helpers ──
# Safe date conversion with fallback (never errors)
# Returns fallback (default: NA_Date_) if conversion fails
safe_as_date <- function(x, fallback = NA_Date_) {
  result <- tryCatch(
    as.Date(x), 
    error = function(e) fallback, 
    warning = function(w) fallback
  )
  if (is.na(result)) fallback else result
}

# Format date as "DD Mon YYYY" with Indonesian month abbreviations
# Example: "15 Sep 2025"
fmt_date <- function(x) {
  if (is.null(x) || all(is.na(x))) return("—")
  s <- format(as.Date(x), "%d %b %Y")
  # Indonesian month abbreviations
  repl <- c("Jan"="Jan","Feb"="Feb","Mar"="Mar","Apr"="Apr","May"="Mei",
            "Jun"="Jun","Jul"="Jul","Aug"="Agu","Sep"="Sep","Oct"="Okt",
            "Nov"="Nov","Dec"="Des")
  for (k in names(repl)) s <- gsub(paste0("\\b", k, "\\b"), repl[[k]], s)
  s
}

fmt_datetime <- function(x, tz = "Asia/Makassar") {
  if (is.null(x) || all(is.na(x))) return("—")
  x <- as.POSIXct(x, tz = tz)
  s <- strftime(x, "%d %b %Y, %H:%M", tz = tz)
  repl <- c("Jan"="Jan","Feb"="Feb","Mar"="Mar","Apr"="Apr","May"="Mei",
            "Jun"="Jun","Jul"="Jul","Aug"="Agu","Sep"="Sep","Oct"="Okt",
            "Nov"="Nov","Dec"="Des")
  for (k in names(repl)) s <- gsub(paste0("\\b", k, "\\b"), repl[[k]], s)
  s
}

parse_date_safely <- function(x) {
  if (inherits(x, "Date"))   return(x)
  if (inherits(x, "POSIXt")) return(as.Date(x))
  y <- suppressWarnings(lubridate::parse_date_time(
    as.character(x),
    orders = c("Ymd","ymd","dmy","dmY","mdy","Y-m-d","d-m-Y","m/d/Y",
               "Ymd HMS","ymd HMS","dmy HMS","mdy HMS")
  ))
  as.Date(y)
}

# ── Numeric Helpers ──
safe_num <- function(x) {
  s <- as.character(x)
  s <- ifelse(stringr::str_detect(s, ",") & !stringr::str_detect(s, "\\."),
              stringr::str_replace_all(s, ",", "."), s)
  suppressWarnings(readr::parse_number(s))
}

is_integerish <- function(x, tol = 1e-8) {
  is.finite(x) & abs(x - round(x)) < tol
}

# ── String Helpers ──
normalize <- function(x) {
  x <- tolower(as.character(x))
  if (requireNamespace("stringi", quietly = TRUE)) {
    x <- stringi::stri_trans_general(x, "Latin-ASCII")
  }
  gsub("[^a-z0-9]", "", x)
}

digits_only <- function(x) stringr::str_replace_all(x, "[^0-9]", "")

# ── Phone Number Normalization ──
# Phone length range (configurable)
PHONE_MIN_LEN <- 10L
PHONE_MAX_LEN <- 13L

normalize_phone <- function(x) {
  y <- as.character(x)
  y <- stringr::str_replace_all(y, "\\s|[()\\-]", "")  # strip spaces (),-
  y <- stringr::str_replace(y, "^\\+62", "0")          # +62xxxxx -> 0xxxxx
  y <- stringr::str_replace(y, "^62", "0")             # 62xxxxx  -> 0xxxxx
  digits_only(y)
}

# ── Formatting Helpers ──
fmt_pct <- function(x) ifelse(is.na(x), NA_character_, sprintf("%.1f%%", 100*x))

pretty_var <- function(x) {
  keep_upper <- c("NIK","BPJS","ID","UGD","IGD","KIA","ANC","MTBS","BMI","SPO2")
  x <- stringr::str_replace_all(x, "[._]+", " ")
  x <- stringr::str_replace_all(x, stringr::regex("\\bTgl\\.?\\b", ignore_case = TRUE), "Tanggal")
  x <- stringr::str_replace_all(x, stringr::regex("\\bNo\\b",      ignore_case = TRUE), "No.")
  x <- stringr::str_squish(x)
  ifelse(toupper(x) %in% keep_upper, toupper(x), tools::toTitleCase(tolower(x)))
}

# ── Validation & Type Inference ──
infer_type <- function(var) {
  v <- tolower(var)
  dplyr::case_when(
    v == "nik" ~ "nik",
    stringr::str_detect(v, "telp|telepon|hp|phone") ~ "phone",
    stringr::str_detect(v, "tgl\\.?lahir|tanggal\\s*lahir|dob|birth") ~ "date",
    TRUE ~ "text"
  )
}

# ── Object Safety Checks ──
safe_exists <- function(obj_name) {
  exists(obj_name, inherits = FALSE) && 
    !is.null(get0(obj_name)) && 
    length(get0(obj_name)) > 0
}

safe_df_check <- function(df) {
  !is.null(df) && is.data.frame(df) && nrow(df) > 0
}

```



## 1.3 BigQuery Connection & Authentication

```{r bigquery-connect, echo=TRUE, message=FALSE, warning=FALSE}
# ══════════════════════════════════════════════════════════════════════════════
# PART 1.3: BIGQUERY CONNECTION & AUTHENTICATION
# ══════════════════════════════════════════════════════════════════════════════
# PURPOSE:
#   Establish secure connection to Google BigQuery database
#
# WHAT THIS DOES:
#   1. Authenticate using browser-based OAuth flow (interactive)
#   2. Connect to BigQuery project: xxxx
#   3. Access dataset: raw_data
#   4. Reference table: xxx
#
# AUTHENTICATION:
#   - Uses bq_auth() which opens browser for Google login
#   - Credentials cached locally after first authentication
#   - Requires Google account with BigQuery access permissions
#
# ERROR HANDLING:
#   - Validates authentication success
#   - Checks project/dataset accessibility
#   - Tests table access with small query
#   - Provides clear error messages if connection fails
#
# PREREQUISITES:
#   - Valid Google Cloud Platform account
#   - BigQuery project must exist with proper billing enabled
#   - User must have at least "BigQuery Data Viewer" role
#   - Internet connectivity required
#
# NOTES:
#   - Connection object (con) is reused throughout the script
#   - Table reference (epus_tbl) is a lazy query (no data loaded yet)
#   - Billing project is same as data project (xxxx)
# ══════════════════════════════════════════════════════════════════════════════

# ---- Authenticate (interactive browser popup) ----
tryCatch({
  bq_auth()
}, error = function(e) {
  stop("BigQuery authentication failed: ", e$message, 
       "\nPlease ensure you have valid credentials and internet connection.")
})

project_id <- "" # change this for example stellarxxxxx
dataset_id <- "dashboard"  # change this also

# ---- Establish connection with error handling ----
con <- tryCatch({
  dbConnect(
    bigrquery::bigquery(),
    project = project_id,
    dataset = dataset_id,
    billing = project_id
  )
}, error = function(e) {
  stop("BigQuery connection failed: ", e$message, 
       "\nProject: ", project_id,
       "\nDataset: ", dataset_id,
       "\nPlease check:\n",
       "  1. Project ID and dataset name are correct\n",
       "  2. You have appropriate permissions\n",
       "  3. Billing is enabled for the project")
})

# ---- Reference table with validation ----
table_name <- "outpatient_pp"
tryCatch({
  epus_tbl <- tbl(con, table_name)
  # Test query to verify table access
  epus_tbl %>% head(1) %>% collect()
}, error = function(e) {
  stop("Cannot access table '", table_name, "': ", e$message,
       "\nPlease verify the table exists and you have read permissions.")
})


```


## 1.4 Parameters & Report Metadata

```{r params-metadata, echo=TRUE, message=FALSE, warning=FALSE}
# ══════════════════════════════════════════════════════════════════════════════
# PART 1.4: PARAMETERS & REPORT METADATA
# ══════════════════════════════════════════════════════════════════════════════
# PURPOSE:
#   Parse and validate YAML parameters, prepare report metadata
#
# WHAT THIS DOES:
#   1. Read parameters from YAML header (params object)
#   2. Extract facility name (puskesmas) and district
#   3. Parse date inputs (selected_date, date_from, date_to)
#   4. Validate date ranges (ensure date_from <= date_to)
#   5. Normalize facility name for database matching (PKM_target)
#   6. Build human-readable report subtitle
#
# PARAMETERS PROCESSED:
#   - puskesmas: Target health facility name (e.g., "Puskesmas Labuapi")
#   - district: District name (e.g., "Lombok Barat")
#   - selected_date: Single analysis date or "latest" for most recent
#   - date_from, date_to: Date range for multi-day analyses
#   - staff_name: Filter analysis to specific staff member (NULL = all staff)
#     * Source column: perawat_bidan_nutrisionist_sanitarian
#     * Exact name matching (normalized for robustness)
#     * Example: "Dr. Ahmad" or NULL for all staff
#   - data_scope: Analysis scope (daily/week/month/range/range_pair)
#   - heaping_mode, heaping_window_days, etc.: Heaping analysis config
#
# NORMALIZATION:
#   - Facility names are normalized to lowercase alphanumeric only
#   - This ensures robust matching despite spelling/spacing variations
#   - Example: "Puskesmas Labuapi" → "puskesmaslabuapi"
#
# DATE VALIDATION:
#   - Validates date formats and converts to Date objects
#   - Swaps date_from/date_to if provided in wrong order
#   - Handles "latest" keyword for selected_date
#
# OUTPUT VARIABLES:
#   - PKM_target: Normalized facility name for filtering
#   - report_subtitle: Human-readable metadata string for display
#   - resolved dates: date_from, date_to, selected_date
# ══════════════════════════════════════════════════════════════════════════════

# ---- Coerce params from YAML with validation ----
puskesmas_name <- as.character(params$puskesmas %||% "")
district_name  <- as.character(params$district  %||% "")

if (!nzchar(puskesmas_name)) {
  warning("Parameter 'puskesmas' is empty. Report may not filter correctly.")
}

selected_date <- if (is.null(params$selected_date) || identical(params$selected_date, "latest")) {
  NA_Date_
} else {
  safe_as_date(params$selected_date, fallback = NA_Date_)
}

date_from <- if (!is.null(params$date_from) && nzchar(params$date_from)) {
  safe_as_date(params$date_from, fallback = NA_Date_)
} else NA_Date_

date_to <- if (!is.null(params$date_to) && nzchar(params$date_to)) {
  safe_as_date(params$date_to, fallback = NA_Date_)
} else NA_Date_

# ---- Validate date range ----
if (!is.na(date_from) && !is.na(date_to) && date_from > date_to) {
  warning("date_from (", date_from, ") is after date_to (", date_to, "). Swapping them.")
  temp <- date_from
  date_from <- date_to
  date_to <- temp
}

# ---- Process staff_name parameter ----
staff_name_param <- if (is.null(params$staff_name)) {
  NULL
} else {
  as.character(params$staff_name)
}

# If staff_name is provided, normalize it for matching (similar to facility)
staff_name_normalized <- if (!is.null(staff_name_param) && nzchar(staff_name_param)) {
  normalize(staff_name_param)
} else {
  NULL
}

# ---- Build report subtitle ----
staff_label <- if (!is.null(staff_name_param) && nzchar(staff_name_param)) {
  paste0(" | Staff: ", staff_name_param)
} else {
  ""
}

report_subtitle <- if (!is.na(date_from) && !is.na(date_to)) {
  paste0("Facility: ", puskesmas_name,
         " | District: ", district_name,
         staff_label,
         " | Window: ", format(date_from, "%Y-%m-%d"), " → ", format(date_to, "%Y-%m-%d"))
} else {
  paste0("Facility: ", puskesmas_name,
         " | District: ", district_name,
         staff_label,
         " | Data date: ", if (is.na(selected_date)) "latest" else format(selected_date, "%Y-%m-%d"))
}

# ---- Normalize puskesmas name for matching ----
PKM_target <- normalize(params$puskesmas)

```




## 1.5 Build Core Query

```{r build-core-query, echo=TRUE, message=FALSE, warning=FALSE}
# ══════════════════════════════════════════════════════════════════════════════
# PART 1.5: BUILD LAZY BIGQUERY CORE DATASET
# ══════════════════════════════════════════════════════════════════════════════
# PURPOSE:
#   Create a lazy BigQuery query with essential transformations and filters
#
# WHAT THIS DOES:
#   1. Add normalized facility name column (PKM_norm) for matching
#   2. Add normalized staff name column (Staff_Name_norm) for filtering
#   3. Parse timestamps from filename (format: YYYY-MM-DD HH_MM_SS.mmm)
#   4. Parse timestamps from tanggal field (multiple format fallbacks)
#   5. Derive encounter date (day column) from timestamp fields
#   6. Filter to target facility (server-side filtering for efficiency)
#   7. Optionally filter to specific staff member if staff_name param provided
#
# LAZY EVALUATION:
#   - Uses dbplyr to build SQL query WITHOUT executing it
#   - No data transfer from BigQuery yet (happens in next chunk)
#   - Allows BigQuery to optimize query execution
#   - Minimizes data transfer costs and time
#
# TIMESTAMP EXTRACTION:
#   - file_ts: Extracted from file_name using regex pattern
#     Format: "2025-09-30 14_25_30.123" → timestamp
#   - tanggal_ts: Parsed from tanggal field with multiple format attempts
#   - day: Derived from either field (robust fallback)
#
# FACILITY & STAFF FILTERING:
#   - Normalized comparison ensures consistent matching
#   - Server-side filter (executed in BigQuery) = fast & efficient
#   - Only target facility data will be collected
#   - If staff_name provided, further filters to that staff member
#   - Both filters reduce data transfer significantly
#
# OUTPUT:
#   - core: Lazy query object (tbl_lazy) ready for collection
#   - core_with_poli: Alias for downstream compatibility
#
# PERFORMANCE:
#   - Filtering at database level reduces network transfer
#   - Typical reduction: 90%+ of data eliminated before download
# ══════════════════════════════════════════════════════════════════════════════

if (!exists("PKM_target") || !nzchar(PKM_target)) {
  stop("PKM_target not defined. Check parameters chunk executed correctly.")
}

core <- epus_tbl %>%
  mutate(
    # Normalize facility name for matching
    PKM      = dbplyr::sql("TRIM(puskesmas_name)"),
    PKM_norm = dbplyr::sql("REGEXP_REPLACE(LOWER(TRIM(puskesmas_name)), r'[^a-z0-9]', '')"),
    
    # Normalize staff name for matching (from perawat_bidan_nutrisionist_sanitarian column)
    Staff_Name = dbplyr::sql("TRIM(perawat_bidan_nutrisionist_sanitarian)"),
    Staff_Name_norm = dbplyr::sql("REGEXP_REPLACE(LOWER(TRIM(perawat_bidan_nutrisionist_sanitarian)), r'[^a-z0-9]', '')"),

    # Extract timestamp from filename (format: YYYY-MM-DD HH_MM_SS.mmm)
    file_ts = dbplyr::sql(
      "PARSE_TIMESTAMP(
         '%Y-%m-%d %H:%M:%E*S',
         REPLACE(
           REGEXP_EXTRACT(file_name, r'\\d{4}-\\d{2}-\\d{2} \\d{2}_\\d{2}_\\d{2}\\.\\d{3}'),
           '_', ':'
         ),
         'Asia/Makassar'
       )"
    ),

    # Parse tanggal field with multiple format fallbacks
    tanggal_ts = dbplyr::sql("
      COALESCE(
        PARSE_TIMESTAMP('%Y-%m-%d %H:%M:%S', CAST(tanggal AS STRING), 'Asia/Makassar'),
        PARSE_TIMESTAMP('%Y-%m-%d %H:%M',    CAST(tanggal AS STRING), 'Asia/Makassar'),
        SAFE_CAST(tanggal AS TIMESTAMP)
      )
    ")
  ) %>%
  mutate(
    # Derive date of encounter (day column used throughout)
    day = dbplyr::sql("
      COALESCE(
        SAFE_CAST(tanggal AS DATE),
        DATE(tanggal_ts, 'Asia/Makassar')
      )
    ")
  ) %>%
  # Filter to target facility (server-side filtering for efficiency)
  filter(PKM_norm == !!PKM_target)

# Add staff filter if staff_name is specified
if (!is.null(staff_name_normalized) && nzchar(staff_name_normalized)) {
  core <- core %>%
    filter(Staff_Name_norm == !!staff_name_normalized)
}

# Verify query builds successfully
if (!inherits(core, "tbl_lazy")) {
  stop("Core query building failed. Check BigQuery connection and table structure.")
}

core_with_poli <- core

```

## 1.6 Resolve Date Windows & Collect Data

```{r resolve-windows, echo=TRUE, message=FALSE, warning=FALSE}
# ══════════════════════════════════════════════════════════════════════════════
# PART 1.6: RESOLVE WORKING DATE & WINDOWS
# ══════════════════════════════════════════════════════════════════════════════
# PURPOSE:
#   Determine actual analysis dates and collect data for multiple time windows
#
# WHAT THIS DOES:
#   1. Query distinct encounter dates available for this facility
#   2. Resolve requested date to nearest available date
#   3. Build multiple time windows: daily, range, week, month
#   4. Deduplicate: keep only latest file per day (by file_ts)
#   5. Collect data from BigQuery into R memory
#
# DATE RESOLUTION LOGIC:
#   - If selected_date = "latest" → use most recent date in data
#   - If selected_date is available → use exact match
#   - If selected_date not available → find nearest date (min absolute difference)
#   - Provides transparent message showing resolved date
#
# WINDOWS CREATED:
#   1. data_df (daily): Single day = resolved_date
#   2. data_range: Custom range [date_from, date_to] or fallback to single day
#   3. data_month: Full calendar month containing resolved_date
#   4. data_week: Monday-Sunday week containing resolved_date
#
# DEDUPLICATION STRATEGY:
#   - Problem: Multiple file uploads per day (corrections, resubmissions)
#   - Solution: Keep only latest file per day using file_ts_fallback
#   - file_ts_fallback = COALESCE(file_ts, tanggal_ts)
#   - Uses window functions in BigQuery (server-side = efficient)
#
# LAZY → EAGER TRANSITION:
#   - Previous chunks built lazy queries (no data transfer)
#   - This chunk calls collect() → actual data download from BigQuery
#   - Data now in R memory for subsequent analysis
#
# HELPER FUNCTION:
#   latest_per_day(src, start_date, end_date):
#     - Takes lazy query, date range
#     - Returns distinct (day, file_name) pairs for latest files
#     - Optimized with server-side window functions
#
# OUTPUT VARIABLES:
#   - avail_days: Vector of all available dates
#   - resolved_date: The actual date used for daily analysis
#   - data_df: Daily data (single day)
#   - data_range: Range data (multiple days)
#   - data_month: Monthly data
#   - data_week: Weekly data
#
# PERFORMANCE NOTES:
#   - Typical data transfer: 1,000-100,000 rows depending on window size
#   - Download time: 1-10 seconds depending on data volume and connection
# ══════════════════════════════════════════════════════════════════════════════

# ---- Optimized helper: latest file per day in [start_date, end_date] ----
# Pushes more computation to BigQuery for efficiency
latest_per_day <- function(src, start_date, end_date) {
  if (!inherits(src, "tbl_lazy")) {
    stop("latest_per_day expects a lazy BigQuery table (tbl_lazy)")
  }
  
  src %>%
    filter(day >= !!start_date, day <= !!end_date) %>%
    mutate(file_ts_fallback = coalesce(file_ts, tanggal_ts)) %>%
    # Use window functions server-side for better performance
    group_by(day) %>%
    # slice_max is supported on database backends (slice_head is not)
    slice_max(file_ts_fallback, n = 1, with_ties = FALSE) %>%
    ungroup() %>%
    select(day, file_name)
}

# Available encounter days for this PKM
avail_days <- core_with_poli %>%
  filter(!is.na(day)) %>%
  distinct(day) %>%
  arrange(day) %>%
  collect() %>%
  pull(day)

if (!length(avail_days)) {
  stop("No encounter dates found for facility: ", puskesmas_name, ".")
}

# Resolve requested date → nearest available (prefer exact; otherwise nearest by abs diff)
selected_date_in <- if (identical(params$selected_date, "latest")) NA_Date_ else as.Date(params$selected_date)
resolved_date <- if (is.na(selected_date_in)) {
  max(avail_days, na.rm = TRUE)
} else if (selected_date_in %in% avail_days) {
  selected_date_in
} else {
  avail_days[ which.min(abs(as.numeric(avail_days - selected_date_in))) ]
}

message(
  "Using day = ", fmt_date(resolved_date),
  if (!is.na(selected_date_in) && selected_date_in != resolved_date)
    paste0(" (nearest to requested ", fmt_date(selected_date_in), ")") else ""
)

# ----- Build windows
# daily
latest_file_for_day <- latest_per_day(core_with_poli, resolved_date, resolved_date)
data_df <- core_with_poli %>%
  inner_join(select(latest_file_for_day, day, file_name), by = c("day","file_name")) %>%
  collect()

# explicit range (fallback to single day if params not set)
if (!is.na(date_from) && !is.na(date_to)) {
  range_start <- date_from
  range_end   <- date_to
} else {
  range_start <- resolved_date
  range_end   <- resolved_date
}

latest_file_per_day_window <- latest_per_day(core_with_poli, range_start, range_end)
data_range <- core_with_poli %>%
  inner_join(select(latest_file_per_day_window, day, file_name), by = c("day","file_name")) %>%
  collect()

# month of resolved_date
month_start <- as.Date(sprintf("%04d-%02d-01", lubridate::year(resolved_date), lubridate::month(resolved_date)))
month_end   <- as.Date(lubridate::ceiling_date(month_start, "month") - 1)
latest_file_per_day_month <- latest_per_day(core_with_poli, month_start, month_end)
data_month <- core_with_poli %>%
  inner_join(select(latest_file_per_day_month, day, file_name), by = c("day","file_name")) %>%
  collect()

# calendar week (Mon–Sun) of resolved_date
week_start <- lubridate::floor_date(as.Date(resolved_date), unit = "week", week_start = 1)
week_end   <- week_start + lubridate::days(6)
latest_file_per_day_week <- latest_per_day(core_with_poli, week_start, week_end)
data_week <- core_with_poli %>%
  inner_join(select(latest_file_per_day_week, day, file_name), by = c("day","file_name")) %>%
  collect()



```

# PART 2: DATA STANDARDIZATION & VALIDATION {.tabset}

## 2.1 Schema Standardization & Scope Selection

```{r, echo=TRUE}
# ══════════════════════════════════════════════════════════════════════════════
# PART 2.1: STANDARDIZE SCHEMA & NUMERIC CLEANUP
# ══════════════════════════════════════════════════════════════════════════════
# PURPOSE:
#   Map heterogeneous raw column names to consistent friendly aliases
#   Clean numeric fields and select analysis scope
#
# WHAT THIS DOES:
#   1. Map raw database columns to standardized friendly names
#      Example: "berat_badan" → "Weight", "sistole" → "Systole"
#   2. Clean numeric fields:
#      - Remove non-numeric characters (units, spaces)
#      - Handle both comma and period decimal separators
#      - Convert to proper numeric type
#   3. Add Tanggal alias from day column (for backward compatibility)
#   4. Apply schema to ALL time windows (daily, week, month, range)
#   5. Select analysis scope based on params$data_scope
#   6. Alias selected scope to data_df for transparent downstream use
#
# COLUMN MAPPING STRATEGY:
#   - Define friendly → candidate mappings (clin_map, adm_map)
#   - Use fuzzy matching (normalized names) to handle variations
#   - Prefer exact matches, fall back to normalized matching
#   - Create alias columns (friendly names point to actual columns)
#
# FRIENDLY COLUMN NAMES:
#   Clinical variables:
#     - Weight, Height, Waist Circ, Systole, Diastole
#     - Heart Rate, Respiratory Rate, Temperature, Age (year)
#   
#   Administrative variables:
#     - NIK, Patient Name, Phone Number, Birth Date
#     - Address, Doctor Name, Diagnose, Staff Name
#
# NUMERIC CLEANING:
#   - Handles multiple formats: "50kg" → 50, "1,75" → 1.75
#   - Safe conversion (never errors, returns NA for invalid)
#   - Preserves original columns (non-destructive)
#
# SCOPE SELECTION:
#   Based on params$data_scope, choose one of:
#     - "daily": Single day (resolved_date)
#     - "week": Monday-Sunday containing resolved_date
#     - "month": Full calendar month
#     - "range": Custom date range [date_from, date_to]
#     - "range_pair": Same as range (pair analysis in heaping chunk)
#   
#   Selected scope aliased to data_df → all downstream code uses data_df
#
# OUTPUT:
#   - data_df: Main analysis dataset (scope-selected, schema-standardized)
#   - data_daily, data_week, data_month, data_range: All available windows
#   - attr(data_df, "scope"): Records which scope was selected
#   - Diagnostic message showing scope, date range, row count
#
# WHY THIS MATTERS:
#   - Consistent column names → simpler downstream code
#   - Scope selection → one parameter controls entire analysis window
#   - Transparent aliasing → easy to trace which data is being analyzed
# ══════════════════════════════════════════════════════════════════════════════

# Helper: numeric clean (kept for potential reuse; main cleaning happens in apply_schema)
clean_numeric <- function(df, vars) {
  v <- intersect(vars, names(df))
  if (length(v)) {
    df <- df %>%
      mutate(across(all_of(v),
        ~ suppressWarnings(as.numeric(chartr(",", ".", gsub("[^0-9,.-]", "", as.character(.))))))
      )
  }
  df
}

# Friendly variable lists (used for aliasing + numeric cleaning)
vars_to_check_adm <- c(
  "NIK","Patient Name","Phone Number","Birth Date","Address",
  "Doctor Name","Diagnose","Staff Name"
)
vars_to_check <- c("Weight","Height","Systole","Diastole","Heart Rate","Respiratory Rate","Temperature","Waist Circ")
numeric_vars_all <- c("Height","Age (year)","Weight","Waist Circ","Systole","Diastole","Respiratory Rate","Heart Rate","Temperature")

# Normalizer + resolver (friendly name → actual column present)
norm_name <- function(x) gsub("[^a-z0-9]", "", tolower(x))
resolve_columns <- function(df_names, friendly_map) {
  df_norm <- setNames(norm_name(df_names), df_names)
  out <- list()
  for (friendly in names(friendly_map)) {
    cands <- friendly_map[[friendly]]
    # 1) exact match?
    hit <- cands[cands %in% df_names]
    if (length(hit)) { out[[friendly]] <- hit[[1]]; next }
    # 2) normalized match
    cand_norm <- norm_name(cands)
    match_idx <- match(cand_norm, df_norm)
    match_idx <- match_idx[!is.na(match_idx)]
    if (length(match_idx)) out[[friendly]] <- names(df_norm)[match_idx[[1]]]
  }
  unlist(out)
}

# Friendly → candidate actual names
clin_map <- list(
  "Weight"           = c("berat_badan"),
  "Height"           = c("tinggi"),
  "Waist Circ"       = c("lingkar_perut"),
  "Systole"          = c("sistole"),
  "Diastole"         = c("diastole"),
  "Heart Rate"       = c("detak_nadi"),
  "Respiratory Rate" = c("nafas"),
  "Temperature"      = c("suhu")
)
adm_map <- list(
  "NIK"          = c("nik"),
  "Patient Name" = c("nama_pasien"),
  "Phone Number" = c("no_telp"),
  "Birth Date"   = c("tgllahir"),
  "Address"      = c("alamat","Address"),
  "Doctor Name"  = c("dokter_tenaga_medis"),
  "Diagnose"     = c("icdx_1"),
  "Staff Name"   = c("perawat_bidan_nutrisionist_sanitarian"),
  "Age (year)"   = c("umur_tahun")
)

# Apply schema (+numeric cleaning) to one data frame
apply_schema <- function(df) {
  if (is.null(df) || !nrow(df)) return(df)

  # Create `Tanggal` alias expected downstream
  if ("day" %in% names(df) && !"Tanggal" %in% names(df)) df$Tanggal <- as.Date(df$day)

  # Resolve present columns
  resolved_clin <- resolve_columns(names(df), clin_map)
  resolved_adm  <- resolve_columns(names(df), adm_map)

  # Add alias columns with friendly names
  add_alias_cols <- function(df, res_map) {
    for (friendly in names(res_map)) {
      real <- res_map[[friendly]]
      if (!(friendly %in% names(df))) df[[friendly]] <- df[[real]]
    }
    df
  }
  df <- add_alias_cols(df, resolved_clin)
  df <- add_alias_cols(df, resolved_adm)

  # Numeric cleaning for the friendly numeric columns
  num_friendly <- intersect(numeric_vars_all, names(df))
  if (length(num_friendly)) {
    df <- df %>%
      mutate(across(all_of(num_friendly),
        ~ suppressWarnings(as.numeric(chartr(",", ".", gsub("[^0-9,.-]", "", as.character(.))))))
      )
  }

  # Keep resolution meta for debugging
  attr(df, "resolved_clin") <- resolved_clin
  attr(df, "resolved_adm")  <- resolved_adm
  df
}

# ── 1) Apply schema (+numeric cleaning) to EVERY prepared window before choosing scope
if (exists("data_df"))    data_df    <- apply_schema(data_df)    # daily
if (exists("data_week"))  data_week  <- apply_schema(data_week)  # week
if (exists("data_month")) data_month <- apply_schema(data_month) # month
if (exists("data_range")) data_range <- apply_schema(data_range) # custom range

# (Optional) extra numeric cleaning pass for any additional vars not in numeric_vars_all
if (exists("data_df"))    data_df    <- clean_numeric(data_df,    numeric_vars_all)
if (exists("data_week"))  data_week  <- clean_numeric(data_week,  numeric_vars_all)
if (exists("data_month")) data_month <- clean_numeric(data_month, numeric_vars_all)
if (exists("data_range")) data_range <- clean_numeric(data_range, numeric_vars_all)


# ── 2) Choose scope AFTER cleaning, then alias to `data_df` so downstream code follows the param
choose_scoped <- function(scope) {
  scope <- tolower(trimws(as.character(scope %||% "daily")))
  out <- switch(scope,
    "week"       = if (exists("data_week"))  data_week  else NULL,
    "month"      = if (exists("data_month")) data_month else NULL,
    "range"      = if (exists("data_range")) data_range else NULL,
    "range_pair" = if (exists("data_range")) data_range else NULL,  # TODO: implement true pair logic later
    "daily"      = if (exists("data_df"))    data_df    else NULL,
    if (exists("data_df")) data_df else NULL
  )
  out
}

scope_name <- tryCatch(as.character(params$data_scope), error = function(e) "daily")
scoped <- choose_scoped(scope_name)

if (is.null(scoped) || !is.data.frame(scoped) || !nrow(scoped)) {
  warning(sprintf("No rows for scope '%s'; falling back to daily.", scope_name))
  scoped <- if (exists("data_df")) data_df else stop("No data available.")
}

# Keep an optional copy of the original daily snapshot for debugging
if (exists("data_df")) data_daily <- data_df

# KEY: make downstream code use the scoped dataset transparently
data_df <- scoped
attr(data_df, "scope") <- scope_name

# Tiny status line so reviewers see what was used
if ("day" %in% names(data_df)) {
  day_min <- suppressWarnings(format(min(as.Date(data_df$day), na.rm = TRUE), "%Y-%m-%d"))
  day_max <- suppressWarnings(format(max(as.Date(data_df$day), na.rm = TRUE), "%Y-%m-%d"))
  cat(knitr::asis_output(
    sprintf("> Using scope: **%s** | Days: %s → %s | Rows: %s\n",
            scope_name, day_min, day_max, nrow(data_df))
  ))
}


# ── Clinical Range Guardrails (Plausibility Checks) ──
# Variable names must match your *friendly* names (e.g., "Weight","Systole", ...)
# These ranges define biologically plausible values; outliers are flagged/converted to NA
# 
# Rationale for ranges:
# - Weight: 1-300 kg (infants to extreme obesity)
# - Height: 30-250 cm (infants to tallest humans)
# - Systole: 50-260 mmHg (hypotension to hypertensive crisis)
# - Diastole: 30-150 mmHg (severe hypotension to severe hypertension)
# - Heart Rate: 20-220 bpm (bradycardia to severe tachycardia)
# - Respiratory Rate: 5-80 breaths/min (bradypnea to tachypnea)
# - Temperature: 30-43°C (hypothermia to hyperthermia/fever)
# - Waist Circ: 10-200 cm (pediatric to extreme obesity)

valid_rules <- tibble::tribble(
  ~Variable,           ~min_ok, ~max_ok,
  "Weight",                  1,     150,  # kg
  "Height",                 30,     200,  # cm
  "Systole",                50,     250,  # mmHg
  "Diastole",               30,     150,  # mmHg
  "Heart Rate",             20,     150,  # bpm
  "Respiratory Rate",        5,      80,  # breaths/min
  "Temperature",            30,      43,  # °C
  "Waist Circ",             10,     150   # cm
)


```



# PART 3: SUMMARY REPORTING {.tabset}

## 3.1 Summary Card UI

```{r summary-card-ui, echo=TRUE, message=FALSE, warning=FALSE}
# ══════════════════════════════════════════════════════════════════════════════
# PART 3.1: BUILD & RENDER SUMMARY INFO CARD
# ══════════════════════════════════════════════════════════════════════════════
# PURPOSE:
#   Create an attractive HTML card displaying report metadata
#
# WHAT THIS DOES:
#   1. Define CSS styles for card component
#   2. Build info_card() function for generating HTML
#   3. Extract metadata from analysis objects
#   4. Render summary card with key information
#
# CARD CONTENTS:
#   - Puskesmas (facility) name
#   - Analysis period (start date → end date)
#   - Number of rows in dataset
#   - Last upload timestamp (most recent file)
#   - Data source system name
#
# CSS STYLING:
#   - Modern card design with rounded corners
#   - Subtle shadow for depth
#   - Grid layout for label-value pairs
#   - Responsive design (adapts to screen width)
#   - Print-friendly (simplified styles for PDF)
#
# METADATA EXTRACTION:
#   - Pulls from multiple sources with robust fallbacks
#   - Handles missing/NA values gracefully
#   - Uses fmt_date() and fmt_datetime() for consistent formatting
#
# OUTPUT:
#   - Interactive HTML card displayed at top of report
#   - Provides quick overview of analysis scope
#   - Helps readers verify they're looking at correct facility/dates
#
# WHY THIS MATTERS:
#   - Professional appearance for stakeholder reports
#   - Clear context at a glance
#   - Reduces confusion about which data is being analyzed
# ══════════════════════════════════════════════════════════════════════════════

# --- CSS Styling ---
card_css <- htmltools::tags$style(htmltools::HTML("
  :root{--card-bg:#ffffff;--card-bd:#e5e7eb;--text:#111827;--muted:#374151;--shadow:0 4px 10px rgba(0,0,0,.05);}
  .card{background:var(--card-bg);border:1px solid var(--card-bd);border-radius:14px;padding:16px 20px;margin-bottom:16px;box-shadow:var(--shadow);max-width:800px;}
  .card-title{font-size:18px;font-weight:700;color:var(--text);margin-bottom:10px;}
  .card-row{display:grid;grid-template-columns:180px 1fr;gap:8px 14px;align-items:start;}
  .card-subtitle{font-size:14px;font-weight:600;color:var(--muted);margin-top:6px;}
  .card-value{font-size:15px;font-weight:500;color:var(--text);margin-top:6px;padding:6px 10px;border:1px dashed var(--card-bd);border-radius:10px;}
  .badge{display:inline-block;font-size:12px;padding:4px 8px;border-radius:9999px;background:#eef2ff;color:#3730a3;border:1px solid #c7d2fe;}
  @media print{.card{box-shadow:none;border-color:#999}.badge{background:none;border:1px solid #666;color:#000}}
"))

# --- Card Component Function ---
info_card <- function(pkm, start_date, end_date, total_rows = NULL,
                      last_upload_ts = NULL, data_source = NULL, staff_name = NULL, extra_badge = NULL) {

  ds <- if (is.null(data_source) || length(data_source) == 0) character(0)
        else as.character(stats::na.omit(data_source))

  ds_node <- if (length(ds) == 0) {
    htmltools::tags$span("—")
  } else if (length(ds) == 1) {
    htmltools::tags$span(ds)
  } else {
    htmltools::tags$ul(class = "source-list",
      lapply(ds, function(s) htmltools::tags$li(s))
    )
  }

  show_rows <- !is.null(total_rows) && length(total_rows) == 1 && !is.na(total_rows)
  show_last <- !is.null(last_upload_ts) && length(last_upload_ts) == 1 && !is.na(last_upload_ts)
  show_staff <- !is.null(staff_name) && length(staff_name) == 1 && nzchar(staff_name)

  htmltools::tags$div(
    class = "card",
    htmltools::tags$div(
      class = "card-title",
      "Summary",
      if (!is.null(extra_badge)) htmltools::tags$span(class="badge", style="margin-left:10px;", extra_badge)
    ),
    htmltools::tags$div(class="card-row",
      htmltools::tags$div(class="card-subtitle","Puskesmas:"),
      htmltools::tags$div(class="card-value", if (is.null(pkm) || all(is.na(pkm)) || !length(pkm)) "—" else pkm),

      if (show_staff) htmltools::tagList(
        htmltools::tags$div(class="card-subtitle","Staff Name:"),
        htmltools::tags$div(class="card-value", staff_name)
      ),

      htmltools::tags$div(class="card-subtitle","Period:"),
      htmltools::tags$div(class="card-value", paste0(fmt_date(start_date), " s.d. ", fmt_date(end_date))),

      if (show_rows) htmltools::tagList(
        htmltools::tags$div(class="card-subtitle","Number of Rows:"),
        htmltools::tags$div(class="card-value", format(total_rows, big.mark = ","))
      ),

      if (show_last) htmltools::tagList(
        htmltools::tags$div(class="card-subtitle","Last Date Upload:"),
        htmltools::tags$div(class="card-value", fmt_datetime(last_upload_ts))
      ),

      htmltools::tags$div(class="card-subtitle","Data Source:"),
      htmltools::tags$div(class="card-value", ds_node)
    )
  )
}

# --- Derive window/summary values from your pipeline (robust fallbacks) ---
period_start <- get0("range_start", ifnotfound = get0("resolved_date", ifnotfound = Sys.Date()))
period_end   <- get0("range_end",   ifnotfound = get0("resolved_date", ifnotfound = Sys.Date()))

total_rows <- if (exists("data_range")) nrow(data_range) else
              if (exists("daily_rows")) nrow(daily_rows) else NA_integer_

# Avoid dplyr::coalesce() length mismatch by pooling vectors first
last_upload_ts <- tryCatch({
  if (exists("data_range") && nrow(data_range)) {
    vec <- NULL
    if ("file_ts"    %in% names(data_range)) vec <- c(vec, data_range$file_ts)
    if ("tanggal_ts" %in% names(data_range)) vec <- c(vec, data_range$tanggal_ts)
    vec <- vec[!is.na(vec)]
    if (length(vec)) max(vec) else NA
  } else NA
}, error = function(e) NA)

if (is.na(last_upload_ts) && exists("latest_file_for_day")) {
  last_upload_ts <- tryCatch(get0("latest_file_for_day")$file_ts_fallback[1], error = function(e) NA)
}

# PKM label resolution
pkm_label <- tryCatch({
  ps <- get0("params", ifnotfound = NULL)
  if (!is.null(ps) && !is.null(ps$puskesmas) && nzchar(ps$puskesmas)) {
    ps$puskesmas
  } else if (exists("data_range") && "puskesmas_name" %in% names(data_range)) {
    x <- unique(stats::na.omit(data_range$puskesmas_name))
    if (length(x)) x[1] else "—"
  } else {
    get0("puskesmas_name", ifnotfound = "—")
  }
}, error = function(e) "—")

# Data source label (safe even if params is missing)
data_source_param <- tryCatch({
  ps <- get0("params", ifnotfound = NULL)
  if (!is.null(ps) && !is.null(ps$data_source) && length(ps$data_source)) ps$data_source else NULL
}, error = function(e) NULL) %||% "ePuskesmas - Pelayanan Pasien"

# --- Render ---
card_summary <- info_card(
  pkm            = pkm_label,
  start_date     = period_start,
  end_date       = period_end,
  total_rows     = total_rows,
  last_upload_ts = last_upload_ts,
  data_source    = data_source_param,
  staff_name     = staff_name_param,
  extra_badge    = NULL
)

htmltools::browsable(htmltools::tagList(card_css, card_summary))

```

## 3.2 Completeness & Validity Summary

```{r completeness-validity-summary, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}
# ══════════════════════════════════════════════════════════════════════════════
# PART 3.2: COMPLETENESS & VALIDITY SUMMARIES
# ══════════════════════════════════════════════════════════════════════════════
# PURPOSE:
#   Calculate and display data quality metrics for all variables
#
# WHAT THIS DOES:
#   1. Calculate completeness (% non-missing) for each variable
#   2. Validate format compliance (NIK, phone numbers, dates)
#   3. Check numeric range compliance (clinical variables)
#   4. Generate quantile summaries for numeric variables
#   5. Display interactive DataTables with color-coded thresholds
#
# DATA QUALITY DIMENSIONS:
#   
#   A. COMPLETENESS:
#      - Total rows, non-missing count, distinct values
#      - % Complete calculated and displayed
#   
#   B. VALIDITY (Format & Range):
#      - NIK: Must be exactly 16 digits
#      - Phone: Must be 10-13 digits (normalized)
#      - Dates: Must be parseable and ≤ current date
#      - Clinical numerics: Must be in plausible range (see valid_rules)
#   
#   C. DISTRIBUTION (Numeric only):
#      - Min, Q1, Median, Q3, Max (5-number summary)
#      - Helps identify outliers and data quality issues
#
# VALIDATION RULES:
#   Applied from valid_rules tibble (defined earlier):
#     - Weight: 1-300 kg
#     - Height: 30-250 cm
#     - Systole: 50-260 mmHg
#     - Diastole: 30-150 mmHg
#     - Heart Rate: 20-220 bpm
#     - Respiratory Rate: 5-80 breaths/min
#     - Temperature: 30-43°C
#     - Waist Circ: 10-200 cm
#
# VARIABLE CATEGORIES:
#   - Administrative: NIK, Patient Name, Phone, Birth Date, Address, etc.
#   - Clinical: Weight, Height, BP, Heart Rate, Temp, etc.
#
# OUTPUT TABLES:
#   1. Administrative Variables Summary
#      - Completeness and validity for patient identifiers
#   2. Clinical Variables Summary
#      - Completeness, validity, and distribution statistics
#   
# INTERACTIVE FEATURES:
#   - Sortable columns (click header to sort)
#   - Searchable (filter box at top)
#   - Color coding for quality thresholds:
#     * Green: ≥95% complete
#     * Yellow: 85-94% complete
#     * Red: <85% complete
#
# WHY THIS MATTERS:
#   - Identifies fields with high missing rates
#   - Flags data entry errors (invalid formats)
#   - Detects implausible values (out-of-range)
#   - Guides targeted data quality improvement efforts
# ══════════════════════════════════════════════════════════════════════════════

if (!safe_df_check(data_df)) {
  stop("data_df is missing or empty. Check previous processing steps.")
}

# Guard date ceiling for plausibility checks (if selected_date is "latest", use today)
REF_DATE <- tryCatch(safe_as_date(params$selected_date), error = function(e) Sys.Date())
if (is.na(REF_DATE)) REF_DATE <- Sys.Date()

# ---- Type inference for validation (extends base infer_type) ----
infer_type_for_validation <- function(var) {
  v <- tolower(var)
  dplyr::case_when(
    v == "nik" ~ "nik",
    stringr::str_detect(v, "telp|telepon|hp|phone") ~ "phone",
    stringr::str_detect(v, "tgl\\.?lahir|tanggal\\s*lahir|dob|birth") ~ "date",
    var %in% vars_to_check ~ "numeric",
    TRUE ~ "text"
  )
}

# Core summarizer for one variable
summarize_var <- function(df, var) {
  if (!var %in% names(df)) return(NULL)

  tp <- infer_type_for_validation(var)
  raw <- df[[var]]
  raw_chr  <- if (is.factor(raw)) as.character(raw) else as.character(raw)
  raw_trim <- trimws(raw_chr)
  is_missing <- is.na(raw_trim) | raw_trim == ""

  rows <- length(raw)
  non_missing <- rows - sum(is_missing)
  distinct_nonmissing <- dplyr::n_distinct(raw_trim[!is_missing], na.rm = TRUE)

  issue_n <- 0L
  issue_label <- "Invalid (non-missing)"
  rule_text <- NA_character_

  # defaults for numeric summaries (only filled for clinical numeric)
  q_min <- q_q1 <- q_med <- q_q3 <- q_max <- NA_real_

  if (tp == "nik") {
    nik <- dplyr::if_else(is_missing, NA_character_, digits_only(raw_trim))
    ok  <- !is_missing & stringr::str_detect(nik, "^\\d{16}$")
    issue_n <- sum(!is_missing & !ok)
    issue_label <- "Invalid length (≠16)"

  } else if (tp == "phone") {
    ph <- dplyr::if_else(is_missing, NA_character_, normalize_phone(raw_trim))
    len <- nchar(ph)
    ok  <- !is.na(ph) & dplyr::between(len, PHONE_MIN_LEN, PHONE_MAX_LEN)
    issue_n <- sum(!is_missing & !ok)
    issue_label <- sprintf("Invalid length (%d–%d)", PHONE_MIN_LEN, PHONE_MAX_LEN)

  } else if (tp == "date") {
    d  <- parse_date_safely(raw)
    ok <- !is.na(d) & d >= as.Date("1900-01-01") & d <= REF_DATE
    issue_n <- sum(!is_missing & !ok)
    issue_label <- paste0("Invalid date (<= ", format(REF_DATE, "%Y-%m-%d"), ")")

  } else if (tp == "numeric") {
    val <- suppressWarnings(readr::parse_number(raw_trim))
    vr  <- valid_rules %>% dplyr::filter(Variable == var)
    bad <- if (nrow(vr)) (is.na(val) | val < vr$min_ok | val > vr$max_ok) else is.na(val)
    issue_n <- sum(!is_missing & bad)
    issue_label <- if (nrow(vr)) {
      rule_text <- sprintf("[%s..%s]", vr$min_ok, vr$max_ok)
      "Non-numeric / out-of-range"
    } else {
      "Non-numeric"
    }

    # Quantiles over VALID (in-range) values only
    x_ok <- val[!bad & is.finite(val)]
    if (length(x_ok)) {
      qs <- stats::quantile(x_ok, probs = c(0, .25, .5, .75, 1), na.rm = TRUE, names = FALSE)
      q_min <- qs[1]; q_q1 <- qs[2]; q_med <- qs[3]; q_q3 <- qs[4]; q_max <- qs[5]
    }
  }

  tibble::tibble(
    Variable = var,
    Rows = rows,
    Missing = sum(is_missing),
    `Missing %` = fmt_pct(Missing / pmax(Rows, 1)),
    Non_missing = non_missing,
    `Non_missing %` = fmt_pct(Non_missing / pmax(Rows, 1)),
    `Distinct (non-missing)` = distinct_nonmissing,
    `Invalid (non-missing)` = issue_n,
    `Invalid % (non-missing)` = fmt_pct(issue_n / pmax(non_missing, 1)),
    Rule = dplyr::coalesce(rule_text, issue_label),
    Min = round(q_min, 2),
    Q1 = round(q_q1, 2),
    Median = round(q_med, 2),
    Q3 = round(q_q3, 2),
    Max = round(q_max, 2)
  )
}

# Build summary table for a set of variables
build_summary <- function(df, vars, include_quartiles = TRUE) {
  present <- intersect(vars, names(df))
  if (!length(present)) return(tibble::tibble())

  out <- purrr::map_dfr(present, ~ summarize_var(df, .x))

  # Sort by missing percentage (descending) - most problematic variables first
  # Need to convert formatted percentage strings to numeric for sorting
  missing_numeric <- as.numeric(gsub("%", "", out$`Missing %`))
  out <- out %>%
    dplyr::mutate(missing_sort = missing_numeric) %>%
    dplyr::arrange(desc(missing_sort)) %>%
    dplyr::select(-missing_sort)

  # Admin: drop numeric distribution columns to keep it clean
  if (!include_quartiles) {
    out <- dplyr::select(out, -Min, -Q1, -Median, -Q3, -Max)
  }
  tibble::rowid_to_column(out, var = "No.")
}

# ── Build Admin & Clinical tables
adm_tbl  <- build_summary(data_df, vars_to_check_adm, include_quartiles = FALSE)
clin_tbl <- build_summary(data_df, vars_to_check, include_quartiles = TRUE)

# Pretty labels
col_label <- c(
  `No.` = "No.",
  Variable = "Variables",
  Rows = "Rows",
  Missing = "Missing (NA)",
  `Missing %` = "Missing (NA) %",
  Non_missing = "Non Missing",
  `Non_missing %` = "Non Missing %",
  `Distinct (non-missing)` = "Unique (non-missing)",
  `Invalid (non-missing)` = "Not Valid (non-missing)",
  `Invalid % (non-missing)` = "Not Valid %",
  Rule = "Validation Rules / Range",
  Min = "Min", Q1 = "Q1", Median = "Median", Q3 = "Q3", Max = "Max"
)
rename_for_print <- function(df, lab){
  nm <- names(df); names(df) <- ifelse(nm %in% names(lab), unname(lab[nm]), nm); df
}
adm_tbl_print  <- rename_for_print(adm_tbl,  col_label)
clin_tbl_print <- rename_for_print(clin_tbl, col_label)

# ── Print tables
if (requireNamespace("kableExtra", quietly = TRUE)) {
  if (nrow(adm_tbl_print)) {
    knitr::kable(
      adm_tbl_print,
      booktabs = TRUE,
      align = c("r", rep("l", ncol(adm_tbl_print) - 1)),
      caption = "Administration — Completeness & Validity"
    ) |>
      kableExtra::kable_styling(full_width = FALSE, bootstrap_options = c("striped","hover","condensed")) |>
      print()
  } else {
    cat(knitr::asis_output("> **Admin:** tidak ada kolom yang cocok.\n"))
  }

  if (nrow(clin_tbl_print)) {
    # Optional: highlight rows with any invalids in red-ish
    hi <- ifelse(clin_tbl_print[["Not Valid (non-missing)"]] > 0, "background-color:#ffe5e5;", "")

    knitr::kable(
      clin_tbl_print,
      booktabs = TRUE,
      align = c("r", rep("l", ncol(clin_tbl_print) - 1)),
      caption = "Clinical — Completeness, Validity, & In-range Distribution"
    ) |>
      kableExtra::kable_styling(full_width = FALSE, bootstrap_options = c("striped","hover","condensed")) |>
      kableExtra::row_spec(0, bold = TRUE) |>
      kableExtra::row_spec(1:nrow(clin_tbl_print), extra_css = hi) |>
      print()
  } else {
    cat(knitr::asis_output("> **Clinical:** tidak ada kolom yang cocok.\n"))
  }

} else {
  if (nrow(adm_tbl_print))
    print(knitr::kable(
      adm_tbl_print,
      align = c("r", rep("l", ncol(adm_tbl_print) - 1)),
      caption = "Admin — Kelengkapan & Validitas"
    ))
  if (nrow(clin_tbl_print))
    print(knitr::kable(
      clin_tbl_print,
      align = c("r", rep("l", ncol(clin_tbl_print) - 1)),
      caption = "Clinical — Kelengkapan, Validitas, & Ringkasan Numerik (nilai valid)"
    ))
}


```


# PART 4: OUTLIER DETECTION & VISUALIZATION {.tabset}

## 4.1 Outlier Detection & Cleaning

```{r outlier-detection, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}
# ══════════════════════════════════════════════════════════════════════════════
# PART 4.1: OUTLIER DETECTION & CLEANING
# ══════════════════════════════════════════════════════════════════════════════
# PURPOSE:
#   Identify and remove statistical outliers using IQR method
#
# WHAT THIS DOES:
#   1. Apply IQR (Interquartile Range) method to clinical variables
#   2. Flag values outside [Q1 - k*IQR, Q3 + k*IQR] as outliers
#   3. Convert outliers to NA (transparent cleaning)
#   4. Count outliers per variable
#   5. Display outlier summary table
#
# IQR METHOD:
#   - Q1 = 25th percentile, Q3 = 75th percentile
#   - IQR = Q3 - Q1 (interquartile range)
#   - Lower fence = Q1 - k*IQR
#   - Upper fence = Q3 + k*IQR
#   - k = multiplier (default: 3.0 for conservative detection)
#
# MULTIPLIER (k) INTERPRETATION:
#   - k = 1.5: Standard boxplot definition (flags ~1% as outliers)
#   - k = 3.0: Conservative (flags ~0.1% as outliers) ← DEFAULT
#   - Higher k = fewer outliers flagged = more permissive
#
# VARIABLES ANALYZED:
#   All clinical numeric variables:
#     - Weight, Height, Waist Circ
#     - Systole, Diastole
#     - Heart Rate, Respiratory Rate, Temperature
#
# WHY CLEAN OUTLIERS:
#   - Extreme outliers are usually data entry errors
#     Examples: Weight = 9999 kg, Height = 5 cm
#   - Distort statistical summaries (mean, SD)
#   - Bias heaping analysis (extreme values are often fabricated)
#   - Need clean data for valid trend analysis
#
# TRANSPARENCY:
#   - Outlier counts displayed in summary table
#   - Original values preserved (can be audited)
#   - Cleaning decision traceable via code
#
# OUTPUT:
#   - data_df_cleaned: Dataset with outliers converted to NA
#   - outlier_summary: Table showing outlier counts per variable
#   - Original data_df preserved for comparison if needed
#
# NOTES:
#   - Only affects numeric clinical variables
#   - Does NOT remove rows (only converts values to NA)
#   - Subsequent analyses use cleaned version
# ══════════════════════════════════════════════════════════════════════════════

# Keep a raw backup BEFORE conversion (for auditing / optional raw plots)
# NOTE: This creates a full copy in memory. Remove if memory is constrained.
data_df_raw <- data_df

# Which clinical columns have rules and exist in data
clin_vars_with_rules <- intersect(valid_rules$Variable, names(data_df))

# 1) Long flag table (row-wise outliers)
df_long_flag <- data_df %>%
  dplyr::mutate(row_id = dplyr::row_number()) %>%
  dplyr::select(
    row_id,
    dplyr::any_of(c("Tanggal","day")),
    dplyr::all_of(clin_vars_with_rules)
  ) %>%
  tidyr::pivot_longer(
    cols = dplyr::all_of(clin_vars_with_rules),
    names_to = "Variable", values_to = "raw"
  ) %>%
  dplyr::mutate(val = safe_num(raw)) %>%
  dplyr::left_join(valid_rules, by = "Variable") %>%
  dplyr::mutate(
    outlier = !is.na(val) & (val < min_ok | val > max_ok),
    ok      = is.na(val) | !outlier
  )

# 2) Per-variable outlier summary
range_summary <- df_long_flag %>%
  dplyr::group_by(Variable) %>%
  dplyr::summarise(
    non_missing = sum(!is.na(val)),
    outliers    = sum(outlier, na.rm = TRUE),
    outlier_pct = dplyr::if_else(non_missing > 0, outliers / non_missing, NA_real_)
  ) %>%
  dplyr::arrange(dplyr::desc(outliers))

if (nrow(range_summary)) {
  knitr::kable(
    range_summary,
    digits = 3,
    col.names = c("Variable","Non-missing","Outliers","Outlier %"),
    caption = "Outlier summary by variable"
  ) %>% kableExtra::kable_styling(full_width = FALSE) %>% print()
} else {
  cat(knitr::asis_output("> Tidak ada nilai outlier berdasarkan guardrails saat ini.\n"))
}

# 3) Table of exact rows/values that will be NA'ed
outliers_table <- df_long_flag %>%
  dplyr::filter(outlier) %>%
  dplyr::transmute(
    row_id,
    Tanggal = dplyr::coalesce(as.character(Tanggal), as.character(day)),
    Variable,
    raw_value     = raw,
    numeric_value = val,
    min_ok, max_ok
  ) %>%
  dplyr::arrange(Variable, row_id)

if (nrow(outliers_table)) {
  knitr::kable(outliers_table, digits = 3,
               caption = "Rows with outliers (will be converted to NA)") %>%
    kableExtra::kable_styling(full_width = FALSE) %>% print()
} else {
  cat(knitr::asis_output("> Tidak ada baris outlier untuk dikonversi ke NA.\n"))
}

# 4) Convert outliers → NA (row kept) + keep boolean flags `out_*`
if (!exists("keep_in_range")) {
  keep_in_range <- function(df_in, rules = valid_rules) {
    if (is.null(df_in) || !nrow(df_in)) return(df_in)
    vars <- intersect(rules$Variable, names(df_in))
    if (!length(vars)) return(df_in)

    patched <- df_in
    for (v in vars) {
      rule <- rules[rules$Variable == v, , drop = FALSE]
      x <- if (is.numeric(patched[[v]])) patched[[v]] else safe_num(patched[[v]])
      out <- !is.na(x) & (x < rule$min_ok[1] | x > rule$max_ok[1])
      patched[[v]] <- ifelse(out, NA_real_, x)          # out-of-range → NA
      patched[[paste0("out_", v)]] <- out               # keep a flag
    }
    patched
  }
}

# IMPORTANT: clean in place so everything after uses NA'ed values
data_df <- keep_in_range(data_df)


```

## 4.2 Boxplots by Interval

```{r boxplots-intervals, echo=TRUE, message=FALSE, warning=FALSE}
# ══════════════════════════════════════════════════════════════════════════════
# PART 4.2: BOXPLOTS BY INTERVAL
# ══════════════════════════════════════════════════════════════════════════════
# PURPOSE:
#   Generate comparative boxplots showing distribution changes over time
#
# WHAT THIS DOES:
#   1. Create boxplots for current vs previous periods
#   2. Support multiple interval types (monthly/weekly/daily)
#   3. Show distribution shifts visually
#   4. Use consistent scales for fair comparison
#
# BOXPLOT INTERPRETATION:
#   - Box = Interquartile range (IQR, middle 50% of data)
#   - Line inside box = Median (50th percentile)
#   - Whiskers = Extend to 1.5*IQR from box edges
#   - Points beyond whiskers = Potential outliers
#
# COMPARISON MODES:
#   - Current vs Previous: Side-by-side boxplots
#   - Helps identify:
#     * Shifts in central tendency (median changes)
#     * Changes in variability (IQR changes)
#     * Emergence of outliers (data quality degradation)
#
# VARIABLES PLOTTED:
#   All clinical numeric variables (after outlier cleaning)
#
# OUTPUT:
#   - Interactive ggplot2 boxplots
#   - Faceted by variable for easy comparison
#   - Color-coded by time period
#
# WHY THIS MATTERS:
#   - Visual detection of data quality changes
#   - Identifies sudden shifts (potential system changes)
#   - Complements numeric summaries with visual patterns
# ══════════════════════════════════════════════════════════════════════════════
# - If plotting RAW (or clin_tbl missing), we build a summary from rows.
# - pretty_var() defined in global-helpers chunk
# ══════════════════════════════════════════════════════════════════════════════

# Choose source for plots: "raw" (original) or "clean" (after outliers→NA)
PLOT_FROM <- if (exists("data_df_raw") && safe_df_check(data_df_raw)) "raw" else "clean"

# Helper to build a clin summary like clin_tbl directly from a dataframe
build_clin_tbl_from_df <- function(df, rules, vars){
  if (is.null(df) || !nrow(df)) return(tibble::tibble())
  present <- intersect(vars, names(df))
  if (!length(present)) return(tibble::tibble())

  one_var <- function(v){
    raw_chr <- as.character(df[[v]])
    is_miss <- is.na(raw_chr) | trimws(raw_chr) == ""
    x <- if (is.numeric(df[[v]])) df[[v]] else {
      if (!exists("safe_num")) {
        safe_num <- function(x){
          s <- as.character(x)
          s <- ifelse(stringr::str_detect(s, ",") & !stringr::str_detect(s, "\\."),
                      stringr::str_replace_all(s, ",", "."), s)
          suppressWarnings(readr::parse_number(s))
        }
      }
      safe_num(raw_chr)
    }
    rows <- length(raw_chr)
    non_missing <- sum(!is_miss)
    rule <- rules[rules$Variable == v, , drop = FALSE]
    invalid_n <- if (nrow(rule)) {
      sum(!is_miss & !is.na(x) & (x < rule$min_ok[1] | x > rule$max_ok[1]))
    } else 0L
    x_ok <- x[!is.na(x)]  # compute quantiles on numeric values (raw; includes outliers)
    qs <- if (length(x_ok)) stats::quantile(x_ok, probs = c(0, .25, .5, .75, 1), na.rm = TRUE, names = FALSE) else rep(NA_real_,5)
    tibble::tibble(
      Variable = v,
      Rows = rows,
      Missing = sum(is_miss),
      `Missing %` = sprintf("%.1f%%", 100 * (Missing / pmax(Rows, 1))),
      Non_missing = non_missing,
      `Non_missing %` = sprintf("%.1f%%", 100 * (Non_missing / pmax(Rows, 1))),
      `Distinct (non-missing)` = dplyr::n_distinct(raw_chr[!is_miss], na.rm = TRUE),
      `Invalid (non-missing)` = invalid_n,
      Rule = if (nrow(rule)) "Non-numeric / out-of-range" else "Numeric",
      Min = round(qs[1], 2), Q1 = round(qs[2], 2), Median = round(qs[3], 2),
      Q3 = round(qs[4], 2), Max = round(qs[5], 2)
    )
  }

  purrr::map_dfr(present, one_var) %>% dplyr::arrange(Variable)
}

# Decide the dataframe we summarize for plotting
plot_df <- if (identical(PLOT_FROM, "raw") && exists("data_df_raw")) data_df_raw else data_df

# Get a clin summary for plotting:
clin_tbl_plot <-
  if (identical(PLOT_FROM, "clean") && exists("clin_tbl") && nrow(clin_tbl)) {
    # reuse your previously-computed clean summary
    clin_tbl
  } else {
    # build from rows (raw or clean), using same valid_rules/vars_to_check
    build_clin_tbl_from_df(plot_df, valid_rules, vars_to_check)
  }

# If nothing to plot, exit early
if (!nrow(clin_tbl_plot)) {
  cat(knitr::asis_output("> Tidak ada variabel klinis untuk diplot.\n"))
} else {

  # ── Interval plot (uses summary row per variable)
  plot_clin_interval <- function(var, show_label = TRUE) {
    row <- clin_tbl_plot %>% dplyr::filter(Variable == var)
    if (!nrow(row)) { message("No row for: ", var); return(invisible(NULL)) }
    lbl <- pretty_var(var)
    miss_pct  <- ifelse(row$Rows[1] > 0, row$Missing[1] / row$Rows[1], NA_real_)
    inval_pct <- ifelse(row$Non_missing[1] > 0, row$`Invalid (non-missing)`[1] / row$Non_missing[1], NA_real_)
    vr <- valid_rules %>% dplyr::filter(Variable == var)

    # If all stats are NA, skip quietly
    if (all(is.na(row[,c("Min","Q1","Median","Q3","Max")]))) return(invisible(NULL))

    p <- ggplot2::ggplot(row, ggplot2::aes(y = 0)) +
      { if (nrow(vr))
        ggplot2::geom_rect(
          ggplot2::aes(xmin = vr$min_ok[1], xmax = vr$max_ok[1],
                       ymin = -0.14, ymax = 0.14,
                       fill = "Rentang valid (min_ok–max_ok)"),
          inherit.aes = FALSE)
      } +
      ggplot2::geom_errorbarh(ggplot2::aes(xmin = Min, xmax = Max, colour = "Min–Maks"),
                              height = 0.25, linewidth = 0.6, na.rm = TRUE) +
      ggplot2::geom_errorbarh(ggplot2::aes(xmin = Q1, xmax = Q3, colour = "IQR (Q1–Q3)"),
                              height = 0, linewidth = 4, na.rm = TRUE) +
      ggplot2::geom_point(ggplot2::aes(x = Median, colour = "Median", shape = "Median"),
                          size = 2.8, na.rm = TRUE) +
      { if (show_label)
        ggplot2::geom_text(ggplot2::aes(x = Median, label = sprintf("%.1f", Median)),
                           hjust = -0.2, size = 3.2, colour = "#2C7BE5", na.rm = TRUE)
      } +
      ggplot2::scale_fill_manual(name = "Keterangan",
        values = c("Rentang valid (min_ok–max_ok)" = "#E8F1FD")) +
      ggplot2::scale_color_manual(name = "Keterangan",
        values = c("Min–Maks"="grey40","IQR (Q1–Q3)"="#2C7BE5","Median"="#2C7BE5")) +
      ggplot2::scale_shape_manual(name = "Keterangan", values = c("Median" = 16)) +
      ggplot2::guides(fill = ggplot2::guide_legend(order = 1),
                      colour = ggplot2::guide_legend(order = 2),
                      shape = ggplot2::guide_legend(order = 2)) +
      ggplot2::labs(
        title    = paste0(lbl, " — Ringkasan Numerik (", toupper(PLOT_FROM), ")"),
        subtitle = paste0("N=", row$Non_missing[1],
                          " · missing ", sprintf("%.1f%%", 100*miss_pct),
                          " · invalid ", sprintf("%.1f%%", 100*inval_pct)),
        x = "Nilai", y = NULL
      ) +
      ggplot2::theme_minimal(base_size = 14) +
      ggplot2::theme(legend.position = "top",
                     legend.title = ggplot2::element_text(face = "bold"),
                     axis.text.y = ggplot2::element_blank(),
                     axis.ticks.y = ggplot2::element_blank(),
                     panel.grid.major.y = ggplot2::element_blank())

    print(p); invisible(p)
  }

  # ── Vertical boxplot (from summary stats)
  plot_clin_box <- function(var, show_median_label = TRUE) {
    row <- clin_tbl_plot %>% dplyr::filter(Variable == var)
    if (!nrow(row)) { message("No row for: ", var); return(invisible(NULL)) }
    lbl <- pretty_var(var)
    miss_pct  <- ifelse(row$Rows[1] > 0, row$Missing[1] / row$Rows[1], NA_real_)
    inval_pct <- ifelse(row$Non_missing[1] > 0, row$`Invalid (non-missing)`[1] / row$Non_missing[1], NA_real_)
    vr <- valid_rules %>% dplyr::filter(Variable == var)

    if (all(is.na(row[,c("Min","Q1","Median","Q3","Max")]))) return(invisible(NULL))

    df_plot <- tibble::tibble(
      x     = lbl,
      ymin  = row$Min, lower = row$Q1, middle= row$Median, upper = row$Q3, ymax  = row$Max
    )

    p <- ggplot2::ggplot(df_plot, ggplot2::aes(x = x)) +
      { if (nrow(vr))
        ggplot2::geom_rect(
          ggplot2::aes(ymin = vr$min_ok[1], ymax = vr$max_ok[1],
                       xmin = 0.6, xmax = 1.4,
                       fill = "Rentang valid (min_ok–max_ok)"),
          inherit.aes = FALSE)
      } +
      ggplot2::geom_boxplot(
        ggplot2::aes(ymin = ymin, lower = lower, middle = middle, upper = upper, ymax = ymax),
        stat = "identity", width = 0.5, fill = "#2C7BE5", alpha = 0.35,
        colour = "#2C7BE5", linewidth = 0.7, outlier.shape = NA
      ) +
      ggplot2::geom_point(ggplot2::aes(y = middle, colour = "Median", shape = "Median"), size = 2.8) +
      { if (show_median_label)
        ggplot2::geom_text(ggplot2::aes(y = middle, label = sprintf("%.1f", middle)),
                           vjust = -0.8, size = 3.2, colour = "#2C7BE5")
      } +
      ggplot2::scale_fill_manual(name = "Keterangan",
        values = c("Rentang valid (min_ok–max_ok)" = "#E8F1FD")) +
      ggplot2::scale_color_manual(name = "Keterangan", values = c("Median"="#2C7BE5")) +
      ggplot2::scale_shape_manual(name = "Keterangan", values = c("Median" = 16)) +
      ggplot2::labs(
        title    = paste0(lbl, " — Boxplot Vertikal (", toupper(PLOT_FROM), ")"),
        subtitle = paste0(
          "N=", row$Non_missing[1],
          " · missing ", sprintf("%.1f%%", 100 * miss_pct),
          " · invalid ", sprintf("%.1f%%", 100 * inval_pct),
          if (nrow(vr)) paste0(" · rentang valid [", vr$min_ok[1], "–", vr$max_ok[1], "]") else ""
        ),
        x = NULL, y = "Nilai"
      ) +
      ggplot2::theme_minimal(base_size = 14) +
      ggplot2::theme(plot.title.position = "plot")

    print(p); invisible(p)
  }

  # Loop through clinical vars that exist in the summary
  clin_present <- intersect(vars_to_check, unique(clin_tbl_plot$Variable))
  invisible(lapply(clin_present, plot_clin_box))
}

```


```{r}
# ─────────────────────────────────────────────────────────────────────────────
# Stage 4.5 — Exclude variables from subsequent analyses (histograms, pair windows)
# Put this RIGHT AFTER: data_df <- keep_in_range(data_df)
# ─────────────────────────────────────────────────────────────────────────────

EXCLUDE_VARS <- c("Temperature", "Respiratory Rate")

drop_excluded_cols <- function(df, exclude = EXCLUDE_VARS) {
  if (is.null(df) || !nrow(df)) return(df)
  # also drop any outlier-flag columns created earlier (e.g., out_Temperature)
  out_flags <- paste0("out_", exclude)
  dplyr::select(df, -dplyr::any_of(c(exclude, out_flags)))
}

# Remove excluded variables (and their out_* flags) from the scoped, cleaned dataset
data_df <- drop_excluded_cols(data_df)

# Optional (recommended): narrow the active sets used by downstream code
vars_to_check_active   <- setdiff(vars_to_check, EXCLUDE_VARS)
numeric_vars_active    <- setdiff(numeric_vars_all, EXCLUDE_VARS)
valid_rules_active     <- dplyr::filter(valid_rules, Variable %in% vars_to_check_active)

```

```{r, echo=TRUE}
# ─────────────────────────────────────────────────────────────────────────────
# Build CURRENT vs PREVIOUS windows (same length), cleaned with the same rules
# For data_scope == "range_pair": 
#   - Current = [date_from..date_to] if given; else end at selected_date/resolved_date with K days
#   - Previous = the K days immediately before Current
# Outputs:
#   df_hist_current, df_hist_previous, df_hist_both
# ─────────────────────────────────────────────────────────────────────────────

suppressPackageStartupMessages({ library(dplyr); library(tidyr); library(lubridate) })

# Helper getters
.get_param_date <- function(x) {
  tryCatch(as.Date(params[[x]]), error = function(e) NA, warning = function(w) NA)
}
get_chr_param <- function(x, default = NA_character_) {
  val <- tryCatch(as.character(params[[x]]), error = function(e) default)
  if (is.null(val) || is.na(val) || !nzchar(val)) default else val
}

# Window length K (heaping window)
K <- tryCatch(as.integer(params$heaping_window_days), error = function(e) NA_integer_)
if (is.na(K) || K < 1L) K <- 3L  # sensible default

scope <- tolower(get_chr_param("data_scope", "daily"))

# Resolve CURRENT window
date_from <- .get_param_date("date_from")
date_to   <- .get_param_date("date_to")
selected_date <- .get_param_date("selected_date")
if (is.na(selected_date) && tolower(get_chr_param("selected_date","")) == "latest") selected_date <- NA

# Prefer explicit date_from/date_to if present
if (!is.na(date_from) && !is.na(date_to)) {
  curr_start <- as.Date(min(date_from, date_to))
  curr_end   <- as.Date(max(date_from, date_to))
  len_days   <- as.integer(curr_end - curr_start) + 1L
} else {
  # No explicit range: use selected_date (if given) as anchor, else resolved_date
  anchor <- if (!is.na(selected_date)) selected_date else {
    # fallback to a pipeline-provided date (must exist earlier in your notebook)
    if (exists("resolved_date")) as.Date(resolved_date) else NA
  }
  if (is.na(anchor)) stop("No anchor date available: provide params$date_to/date_from or selected_date or resolved_date.")
  curr_end   <- anchor
  curr_start <- curr_end - days(K - 1L)
  len_days   <- K
}

# Ensure data_scope "range_pair" uses a pair of equal-length windows
if (identical(scope, "range_pair")) {
  prev_end   <- curr_start - days(1L)
  prev_start <- prev_end - days(len_days - 1L)
} else {
  # legacy behavior (same as your earlier chunk)
  prev_end   <- curr_start - 1
  prev_start <- prev_end - (len_days - 1L)
}

# 3) Latest-per-day (reuse your Stage 3 helper)
#    We keep the “latest record per day” behavior inside each window.
latest_curr <- latest_per_day(core_with_poli, curr_start, curr_end)
latest_prev <- latest_per_day(core_with_poli, prev_start, prev_end)

# 4) Collect & tag
data_range_curr <- core_with_poli %>%
  dplyr::inner_join(dplyr::select(latest_curr, day, file_name), by = c("day","file_name")) %>%
  dplyr::mutate(period = "Current") %>%
  dplyr::collect()

data_range_prev <- core_with_poli %>%
  dplyr::inner_join(dplyr::select(latest_prev, day, file_name), by = c("day","file_name")) %>%
  dplyr::mutate(period = "Previous") %>%
  dplyr::collect()

# 5) Apply the SAME schema & guardrails (ensures comparability) + exclude vars
valid_rules_active <- if (exists("valid_rules_active")) valid_rules_active else valid_rules

data_range_curr <- apply_schema(data_range_curr) |>
  keep_in_range(rules = valid_rules_active) |>
  drop_excluded_cols()

data_range_prev <- apply_schema(data_range_prev) |>
  keep_in_range(rules = valid_rules_active) |>
  drop_excluded_cols()

# 6) Expose frames for histograms & downstream plots (shared everywhere)
df_hist_current  <- data_range_curr
df_hist_previous <- data_range_prev
df_hist_both     <- dplyr::bind_rows(data_range_curr, data_range_prev)

# (Optional) status line
cat(knitr::asis_output(
  sprintf("> Pair windows — Current: %s → %s (%d hari) | Previous: %s → %s | Rows: %d + %d\n",
          format(curr_start, "%Y-%m-%d"), format(curr_end, "%Y-%m-%d"), len_days,
          format(prev_start, "%Y-%m-%d"),  format(prev_end,  "%Y-%m-%d"),
          nrow(df_hist_current), nrow(df_hist_previous))
))


```

## 4.3 Histogram 



```{r, echo=TRUE}

# ═════════════════════════════════════════════════════════════════════════════
# PART 4.3: HISTOGRAM GENERATION FOR CLINICAL VARIABLES
# ═════════════════════════════════════════════════════════════════════════════
# PURPOSE:
#   Generate histograms for clinical variables from cleaned, scoped data
#
# WHAT THIS DOES:
#   1. Defines make_var_histograms() function with minimal internal helpers
#   2. Uses explicit puskesmas_label and period_label from parameters
#   3. Creates histograms with proper titles showing facility and period
#   4. Applies to data_df which is already filtered to single puskesmas
#
# KEY OPTIMIZATION:
#   Since data is already filtered to a single puskesmas in earlier chunks,
#   we directly use parameters (puskesmas_label, period_label) instead of
#   inferring from data. This is more efficient and clearer.
#
# HELPER FUNCTIONS:
#   All helpers (.infer_period, .fd_binwidth, .make_limits, etc.) are defined
#   INSIDE make_var_histograms() to keep them encapsulated and avoid namespace
#   pollution. Only .infer_period() is kept for date range calculation when
#   period_label is not explicitly provided.
# ═════════════════════════════════════════════════════════════════════════════

make_var_histograms <- function(df_in,
                                vars,
                                bins = NULL, binwidth = NULL,
                                clamp = c("quantile","data","rules"),
                                q = c(.01,.99), pad = 0.03,
                                show_median = FALSE, show_mean = FALSE,
                                show_valid_band = FALSE, breaks_n = 6,
                                breaks_step = NULL, breaks_all_values = FALSE,
                                x_text_angle = 45,
                                base_size = 11,
                                title_size = NULL, subtitle_size = NULL,
                                axis_title_size = NULL, axis_text_size = NULL,
                                caption_size = NULL,
                                # NEW (optional)
                                puskesmas_label = NULL,
                                period_label    = NULL,
                                date_col        = NULL,           # <- NEW
                                date_format     = "%Y-%m-%d",     # <- NEW
                                annotate_top_n = 5,
                                annotate_pct_digits = 1,
                                annotate_box = TRUE,
                                annotate_y_pos = 0.90) {

  clamp <- match.arg(clamp)
  if (missing(df_in)) stop("Please provide df_in.")
  if (missing(vars))  vars <- names(df_in)

  # local helpers
  `%||%` <- function(a, b) if (!is.null(a)) a else b
  .fmt <- function(x, d = 0) format(round(x, d), trim = TRUE, nsmall = 0, scientific = FALSE)
  .paste_segs <- function(...) {
    segs <- c(...); segs <- segs[!is.na(segs) & nzchar(segs)]
    if (!length(segs)) "" else paste(segs, collapse = " · ")
  }
  
  # Helper: infer period from date column (only if period_label not provided)
  .infer_period <- function(df, date_col_name = NULL, fmt = "%Y-%m-%d") {
    if (is.null(date_col_name)) return(NA_character_)
    if (!date_col_name %in% names(df)) return(NA_character_)
    d <- df[[date_col_name]]
    if (inherits(d, "POSIXt")) d <- as.Date(d)
    if (!inherits(d, "Date"))  d <- suppressWarnings(as.Date(d))
    d <- d[!is.na(d)]
    if (!length(d)) return(NA_character_)
    rng <- range(d, na.rm = TRUE)
    if (identical(rng[1], rng[2])) format(rng[1], fmt) else paste(format(rng[1], fmt), "–", format(rng[2], fmt))
  }
  .fd_binwidth <- function(x) {
    x <- x[is.finite(x)]
    n <- length(x); if (n < 2) return(NA_real_)
    bw <- 2 * IQR(x) / (n^(1/3))
    if (!is.finite(bw) || bw <= 0) {
      rng <- diff(range(x)); if (!is.finite(rng) || rng <= 0) return(NA_real_)
      rng / 30
    } else bw
  }
  .make_limits <- function(x, clamp, q, pad, vr) {
    x <- x[is.finite(x)]
    if (!length(x)) return(c(NA_real_, NA_real_))
    if (clamp == "quantile") {
      qs <- stats::quantile(x, probs = q, na.rm = TRUE); lim <- c(qs[[1]], qs[[2]])
    } else if (clamp == "data") {
      lim <- range(x, na.rm = TRUE)
    } else { # rules
      stopifnot(!is.null(vr), nrow(vr) == 1); lim <- c(vr$min_ok[1], vr$max_ok[1])
    }
    if (!all(is.finite(lim)) || lim[1] >= lim[2]) return(range(x, na.rm = TRUE))
    span <- lim[2] - lim[1]; lim + c(-1, 1) * (pad * span)
  }
  .make_breaks <- function(n = 6) {
    if (requireNamespace("scales", quietly = TRUE)) scales::breaks_extended(n) else function(x) pretty(x, n)
  }
  .theme_holtzy <- function(base_size = 12,
                            plot_title_size = NULL, subtitle_size = NULL,
                            axis_title_size = NULL, axis_text_size = NULL, caption_size = NULL) {
    plot_title_size <- plot_title_size %||% (base_size + 2)
    subtitle_size   <- subtitle_size   %||% (base_size - 1)
    axis_title_size <- axis_title_size %||% (base_size - 2)
    axis_text_size  <- axis_text_size  %||% (base_size - 3)
    caption_size    <- caption_size    %||% (base_size - 4)
    if (requireNamespace("hrbrthemes", quietly = TRUE)) {
      hrbrthemes::theme_ipsum(
        base_size       = base_size,
        plot_title_size = plot_title_size,
        subtitle_size   = subtitle_size,
        axis_title_size = axis_title_size,
        caption_size    = caption_size
      ) + ggplot2::theme(axis.text = ggplot2::element_text(size = axis_text_size))
    } else {
      ggplot2::theme_minimal(base_size = base_size) +
        ggplot2::theme(plot.title   = ggplot2::element_text(size = plot_title_size),
                       plot.subtitle= ggplot2::element_text(size = subtitle_size),
                       axis.title   = ggplot2::element_text(size = axis_title_size),
                       axis.text    = ggplot2::element_text(size = axis_text_size),
                       plot.caption = ggplot2::element_text(size = caption_size))
    }
  }

  # Get labels directly from parameters (data already filtered to single puskesmas)
  pusk_label  <- if (!is.null(puskesmas_label)) as.character(puskesmas_label) else NA_character_
  period_disp <- if (!is.null(period_label)) {
    as.character(period_label)
  } else if (!is.null(date_col)) {
    .infer_period(df_in, date_col_name = date_col, fmt = date_format)
  } else {
    NA_character_
  }

  # numeric parsing - robust version that handles various data types
  safe_num <- function(x) {
    if (is.numeric(x)) return(as.numeric(x))
    if (inherits(x, c("Date","POSIXt","difftime","hms"))) return(as.numeric(x))
    if (is.logical(x)) return(as.numeric(x))
    if (is.factor(x))  x <- as.character(x)
    if (is.list(x)) {
      x <- vapply(x, function(e) {
        if (length(e) == 0) NA_character_ else as.character(e[[1]])
      }, character(1))
    }
    s <- as.character(x)
    s <- ifelse(stringr::str_detect(s, ",") & !stringr::str_detect(s, "\\."),
                stringr::str_replace_all(s, ",", "."), s)
    suppressWarnings(readr::parse_number(s))
  }

  dat <- df_in %>%
    dplyr::select(dplyr::all_of(intersect(vars, names(df_in)))) %>%
    dplyr::mutate(.N = dplyr::row_number()) %>%
    tidyr::pivot_longer(cols = dplyr::all_of(vars), names_to = "Variable", values_to = "raw") %>%
    dplyr::mutate(Value = safe_num(raw)) %>%
    dplyr::filter(!is.na(Value))

  plots <- list()
  for (v in unique(dat$Variable)) {
    dv <- dplyr::filter(dat, Variable == v); if (!nrow(dv)) next
    vr <- if (exists("valid_rules")) dplyr::filter(valid_rules, Variable == v) else tibble::tibble()

    # x-limits
    xlim <- if (clamp == "rules" && nrow(vr)) {
      .make_limits(dv$Value, "rules", q, pad, vr)
    } else if (clamp == "data") {
      .make_limits(dv$Value, "data", q, pad, NULL)
    } else {
      .make_limits(dv$Value, "quantile", q, pad, NULL)
    }
    dvp <- dplyr::filter(dv, Value >= xlim[1], Value <= xlim[2])

    # bins
    if (is.null(binwidth)) {
      bw <- .fd_binwidth(dvp$Value); use_bins <- !is.finite(bw) || is.na(bw)
    } else { bw <- binwidth; use_bins <- FALSE }

    # derive breaks
    if (!use_bins) {
      start <- floor(xlim[1] / bw) * bw
      end   <- ceiling(xlim[2] / bw) * bw
      plot_breaks <- seq(start, end + bw, by = bw)
      eff_bw <- bw
    } else {
      eff_bins <- if (!is.null(bins)) bins else 30
      plot_breaks <- seq(xlim[1], xlim[2], length.out = eff_bins + 1)
      eff_bw <- diff(plot_breaks)[1]
    }

    # titles
    title_text <- .paste_segs(
      paste0("Variabel: ", v),
      if (!is.na(pusk_label))  paste0("Puskesmas: ", pusk_label) else NA,
      if (!is.na(period_disp)) paste0("Periode: ", period_disp) else NA
    )
    subtitle_text <- .paste_segs(
      paste0("N=", sum(!is.na(dvp$Value))),
      if (nrow(vr)) paste0("Rentang valid [", vr$min_ok[1], "–", vr$max_ok[1], "]") else NA,
      paste0("Bin size = ", .fmt(eff_bw, 0))
    )

    # plot
    p <- ggplot2::ggplot(dvp, ggplot2::aes(x = Value)) +
      { if (show_valid_band && nrow(vr))
        ggplot2::annotate("rect",
          xmin = max(xlim[1], vr$min_ok[1]), xmax = min(xlim[2], vr$max_ok[1]),
          ymin = 0, ymax = Inf, fill = "#E8F1FD") } +
      ggplot2::geom_histogram(binwidth = eff_bw, boundary = plot_breaks[1],
                              fill = "#69b3a2", color = "#e9ecef", alpha = 0.9, closed = "left") +
      ggplot2::scale_x_continuous(limits = xlim,
        breaks = if (!is.null(breaks_step)) seq(floor(xlim[1]), ceiling(xlim[2]), by = breaks_step)
                 else (.make_breaks(breaks_n))) +
      ggplot2::labs(title = title_text, subtitle = subtitle_text, x = v, y = "Frekuensi") +
      .theme_holtzy(base_size, title_size, subtitle_size, axis_title_size, axis_text_size, caption_size)

    if (!is.null(x_text_angle)) {
      p <- p + ggplot2::theme(axis.text.x = ggplot2::element_text(angle = x_text_angle, hjust = 1, vjust = 1))
    }
    plots[[v]] <- p
  }
  plots
}


# ---- Render histograms from cleaned, scoped data_df ----
# Use explicit puskesmas name from parameters since data is already filtered
hplots <- make_var_histograms(
  df_in  = data_df,
  vars   = intersect(valid_rules$Variable, names(data_df)),
  binwidth = 1,
  clamp  = "rules",
  breaks_step   = 5,
  x_text_angle  = 45,
  base_size     = 10, axis_text_size = 8,
  puskesmas_label = params$puskesmas,  # Explicitly pass from parameters
  date_col = "day",                    # Use the day column for period inference
  annotate_top_n = 5, annotate_pct_digits = 1, annotate_box = TRUE, annotate_y_pos = 0.92
)

for (nm in names(hplots)) print(hplots[[nm]])


```

## 4.4 Histogram Comparisons (Current vs Previous)

```{r, echo=TRUE}
# ══════════════════════════════════════════════════════════════════════════════
# PART 4.3: HISTOGRAM COMPARISONS (CURRENT VS PREVIOUS)
# ══════════════════════════════════════════════════════════════════════════════
# PURPOSE:
#   Create paired histograms to visualize distribution shifts between time periods
#
# WHAT THIS DOES:
#   1. Generate histograms for current window (top)
#   2. Generate histograms for previous window (bottom)
#   3. Use identical bin breaks for accurate comparison
#   4. Align Y-axis scales to highlight magnitude changes
#   5. Stack vertically for easy visual comparison
#
# HISTOGRAM COMPARISON STRATEGY:
#   - Shared X-axis breaks → same bins in both periods
#   - Shared Y-axis max → fair comparison of frequencies
#   - Vertical stacking → easy to spot shifts
#
# WHAT TO LOOK FOR:
#   1. Shape Changes:
#      - Normal → skewed distribution (data quality issue)
#      - Smooth → spiky (heaping/rounding introduced)
#   
#   2. Peak Shifts:
#      - Central tendency moved (mean/median change)
#      - May indicate real health changes or data issues
#   
#   3. Heaping Patterns:
#      - Spikes at multiples of 5 or 10 (digit preference)
#      - Indicates potential fabrication or excessive rounding
#   
#   4. Range Changes:
#      - Spread increased/decreased (variability change)
#      - Truncation (values artificially limited)
#
# TECHNICAL DETAILS:
#   - Uses make_var_histograms() helper function
#   - Applies valid_rules for X-axis limits
#   - Clamps to clinical plausibility ranges
#   - Excludes variables specified in exclude_vars
#
# INPUT REQUIREMENTS:
#   - df_hist_current: Current period data (cleaned)
#   - df_hist_previous: Previous period data (cleaned)
#   - valid_rules: Clinical plausibility ranges
#   - Excluded variables already removed
#
# OUTPUT:
#   - Paired histograms per variable (stacked vertically)
#   - Top = Current window with caption
#   - Bottom = Previous window with caption
#   - Shared scales for accurate comparison
#
# WHY THIS MATTERS:
#   - Visual detection of distributional anomalies
#   - Identifies sudden changes in data patterns
#   - Supports heaping analysis with visual evidence
#   - Helps diagnose data quality interventions effectiveness
# ══════════════════════════════════════════════════════════════════════════════

suppressPackageStartupMessages({ library(patchwork) })

# 0) Guards (fallbacks if pair frames not present)
if (!exists("df_hist_current") || !exists("df_hist_previous")) {
  stop("Pair frames df_hist_current / df_hist_previous not found. Build them before this chunk.")
}

# 1) Variables eligible to plot = in rules AND present in BOTH frames
vars_rules <- if (exists("valid_rules_active")) valid_rules_active$Variable else valid_rules$Variable
vars_to_plot <- intersect(vars_rules, intersect(names(df_hist_current), names(df_hist_previous)))
if (!length(vars_to_plot)) {
  cat(knitr::asis_output("> No common clinical variables found for pair comparison.\n"))
}

# 2) Common plotting args
# Use explicit puskesmas from parameters since data is already filtered
common_args <- list(
  vars                = vars_to_plot,
  binwidth            = 1,          # 1-unit bins (matches your preference)
  clamp               = "rules",    # force same X-range by guardrails (min_ok–max_ok)
  breaks_step         = 5,
  x_text_angle        = 45,
  base_size           = 10,
  axis_text_size      = 8,
  puskesmas_label     = params$puskesmas,  # Explicitly pass from parameters
  date_col            = "Tanggal",         # Use Tanggal column created by apply_schema()
  annotate_top_n      = 5,
  annotate_pct_digits = 1,
  annotate_box        = TRUE,
  annotate_y_pos      = 0.92
)

# 3) Build the two sets (using your helper)
h_curr <- do.call(make_var_histograms, c(list(df_in = df_hist_current),  common_args))
h_prev <- do.call(make_var_histograms, c(list(df_in = df_hist_previous), common_args))

# 4) Helper to compute shared Y limit per variable using the SAME breaks
#    (because clamp="rules" and binwidth=1, we can derive breaks from min_ok/max_ok)
.safe_num <- function(x) {
  s <- as.character(x)
  s <- ifelse(stringr::str_detect(s, ",") & !stringr::str_detect(s, "\\."),
              stringr::str_replace_all(s, ",", "."), s)
  suppressWarnings(readr::parse_number(s))
}

.compute_shared_ymax <- function(v, df_cur, df_prev, rules, binwidth = 1) {
  vr <- dplyr::filter(rules, Variable == v)
  # If no rule row, fall back to data range across both
  if (!nrow(vr)) {
    x_all <- c(.safe_num(df_cur[[v]]), .safe_num(df_prev[[v]]))
    x_all <- x_all[is.finite(x_all)]
    if (!length(x_all)) return(1)
    xlim <- range(x_all, na.rm = TRUE)
  } else {
    xlim <- c(vr$min_ok[1], vr$max_ok[1])
  }
  # Build shared breaks aligned to the left edge
  start <- floor(xlim[1] / binwidth) * binwidth
  end   <- ceiling(xlim[2] / binwidth) * binwidth
  brks  <- seq(start, end + binwidth, by = binwidth)

  # Hist counts for current & previous using the SAME breaks
  xc <- .safe_num(df_cur[[v]]); xc <- xc[is.finite(xc)]
  xp <- .safe_num(df_prev[[v]]); xp <- xp[is.finite(xp)]
  hc <- if (length(xc)) hist(xc, breaks = brks, plot = FALSE, right = FALSE, include.lowest = TRUE)$counts else 0
  hp <- if (length(xp)) hist(xp, breaks = brks, plot = FALSE, right = FALSE, include.lowest = TRUE)$counts else 0

  max(c(hc, hp, 1))
}

# 5) Print: stack per variable with shared Y (coord_cartesian)
for (v in vars_to_plot) {
  p_top <- h_curr[[v]]
  p_bot <- h_prev[[v]]

  # Skip gracefully if a plot is missing
  if (is.null(p_top) && is.null(p_bot)) next
  if (is.null(p_top)) p_top <- patchwork::plot_spacer()
  if (is.null(p_bot)) p_bot <- patchwork::plot_spacer()

  # Compute a shared Y max for this variable
  rules_ref <- if (exists("valid_rules_active")) valid_rules_active else valid_rules
  y_max <- .compute_shared_ymax(v, df_cur = df_hist_current, df_prev = df_hist_previous,
                                rules = rules_ref, binwidth = common_args$binwidth)

  # Add captions + shared Y limit
  p_top <- p_top + labs(caption = "Current window")  + coord_cartesian(ylim = c(0, y_max * 1.05))
  p_bot <- p_bot + labs(caption = "Previous window") + coord_cartesian(ylim = c(0, y_max * 1.05))

  print(p_top / p_bot)
}


```


# PART 5: HEAPING ANALYSIS (DIGIT PREFERENCE DETECTION) {.tabset}

## 5.1 Heaping Trace System

```{r heaping-trace-setup, echo=TRUE, message=FALSE, warning=FALSE}
# ══════════════════════════════════════════════════════════════════════════════
# PART 5.1: HEAPING TRACE SYSTEM
# ══════════════════════════════════════════════════════════════════════════════
# PURPOSE:
#   Non-invasive instrumentation for transparent heaping calculations
#
# WHAT THIS DOES:
#   1. Set up trace collection environment (HEAP_TRACE)
#   2. Define functions for recording calculation steps
#   3. Enable step-by-step auditing of heaping analysis
#   4. Optionally persist trace data for external review
#
# HEAPING ANALYSIS OVERVIEW:
#   Heaping = digit preference in measurements
#   - Legitimate: People prefer reporting round numbers (50 kg vs 47 kg)
#   - Concerning: Excessive heaping may indicate:
#     * Data fabrication (made-up numbers tend to cluster on 0, 5)
#     * Inadequate measurement tools (analog scales)
#     * Poor training (staff rounding excessively)
#
# DETECTION METHOD:
#   - Extract last digit from integer values (0-9)
#   - Extract last tenth from decimal values (0.1-0.9)
#   - Calculate proportion of each digit/tenth
#   - Flag "preferred" digits appearing ≥ threshold (default 15%)
#   - Expected uniform: ~10% per digit (0-9), ~11.1% per tenth (.1-.9)
#
# TRACE SYSTEM FUNCTIONS:
#   - heaping_trace_start(window_id): Initialize trace bucket for a time window
#   - trace_push(window_id, step, df): Record intermediate data frame
#   - trace_get(window_id, step): Retrieve recorded step
#   - trace_keys(): List all available trace windows
#   - save_heaping_trace(path): Persist trace to RDS file
#
# CONFIGURATION:
#   HEAP_TRACE_ENABLED = TRUE:  Collect traces (for debugging/audit)
#   HEAP_TRACE_ENABLED = FALSE: Skip tracing (faster, less memory)
#
# WHY TRACE:
#   - Calculation transparency (auditable by external reviewers)
#   - Debugging (inspect intermediate steps if results unexpected)
#   - Validation (verify calculation logic is correct)
#   - Reproducibility (exact steps recorded for replication)
#
# PERFORMANCE:
#   - Minimal overhead when enabled (~1-2% runtime increase)
#   - No overhead when disabled
#   - Memory usage: ~1-10 MB per trace depending on data size
#
# OUTPUT:
#   - HEAP_TRACE environment with nested lists of data frames
#   - Each trace contains: df_base, df_long, int_counts, dec_counts, etc.
# ══════════════════════════════════════════════════════════════════════════════

# Configuration
HEAP_TRACE_ENABLED <- TRUE   # set FALSE to disable tracing

# Initialize trace storage environment (consolidated)
if (!exists("HEAP_TRACE") || !is.environment(HEAP_TRACE)) {
  HEAP_TRACE <- new.env(parent = emptyenv())
}

# Start a new window bucket: id = date or date-range label
heaping_trace_start <- function(window_id) {
  if (!HEAP_TRACE_ENABLED) return(invisible(NULL))
  HEAP_TRACE[[window_id]] <- list()
  invisible(NULL)
}

# Push any intermediate table with a step name (e.g., "df_long", "int_counts")
trace_push <- function(window_id, step, df) {
  if (!HEAP_TRACE_ENABLED) return(invisible(NULL))
  if (is.null(HEAP_TRACE[[window_id]])) heaping_trace_start(window_id)
  HEAP_TRACE[[window_id]][[step]] <- df
  invisible(NULL)
}

# Convenience getter
trace_get <- function(window_id, step = NULL) {
  if (!exists("HEAP_TRACE") || !is.environment(HEAP_TRACE)) {
    stop("HEAP_TRACE environment missing")
  }
  if (is.null(HEAP_TRACE[[window_id]])) return(NULL)
  if (is.null(step)) return(HEAP_TRACE[[window_id]])  # Return entire bucket
  HEAP_TRACE[[window_id]][[step]]
}

# List all available trace keys
trace_keys <- function() {
  if (!exists("HEAP_TRACE") || !is.environment(HEAP_TRACE)) return(character(0))
  sort(ls(envir = HEAP_TRACE, all.names = FALSE))
}

# Optional: persist trace for audit
save_heaping_trace <- function(path = "heaping_trace.rds") {
  if (!HEAP_TRACE_ENABLED) {
    message("Trace collection is disabled (HEAP_TRACE_ENABLED = FALSE)")
    return(invisible(NULL))
  }
  if (!exists("HEAP_TRACE")) {
    warning("No HEAP_TRACE environment found")
    return(invisible(NULL))
  }
  saveRDS(as.list(HEAP_TRACE), path)
  message("Trace saved to: ", path)
  invisible(path)
}

```

## 5.2 Heaping Calculation

```{r heaping-calculation, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}
# ══════════════════════════════════════════════════════════════════════════════
# PART 5.2: HEAPING ANALYSIS - CALCULATE PREFERRED DIGIT PROPORTIONS
# ══════════════════════════════════════════════════════════════════════════════
# PURPOSE:
#   Calculate digit preference (heaping) metrics for clinical variables
#
# WHAT THIS DOES:
#   1. Extract last digits from integer measurements (0-9)
#   2. Extract last tenths from decimal measurements (0.1-0.9)
#   3. Calculate proportion of each digit/tenth across all measurements
#   4. Flag "preferred" digits appearing ≥ threshold (default 15%)
#   5. Generate summary tables of heaping patterns
#   6. Support daily vs rolling window analysis modes
#
# HEAPING CALCULATION STEPS:
#   
#   Step 1: Data Preparation
#     - Filter to non-missing clinical values
#     - Classify each value as integer or decimal
#   
#   Step 2: Digit Extraction
#     - Integers: Last digit = floor(abs(value)) % 10
#       Examples: 50 → 0, 147 → 7, 5 → 5
#     - Decimals: Last tenth = round(value % 1, 1)
#       Examples: 50.5 → 0.5, 1.75 → 0.8 (rounded)
#   
#   Step 3: Count Frequencies
#     - Count occurrences of each digit (0-9) for integers
#     - Count occurrences of each tenth (.1-.9) for decimals
#   
#   Step 4: Calculate Proportions
#     - Proportion = count / total_measurements
#     - Convert to percentage for reporting
#   
#   Step 5: Flag Preferred Digits
#     - If proportion ≥ threshold (15%) → "preferred"
#     - Expected uniform: ~10% per digit, ~11.1% per tenth
#     - Deviation indicates heaping
#   
#   Step 6: Summarize
#     - List preferred digits/tenths
#     - Calculate total heaping proportion
#     - Aggregate across time windows if range mode
#
# PARAMETERS:
#   - heaping_mode: "daily" or "range"
#     * daily: Single date analysis
#     * range: Multiple days (rolling windows or single aggregation)
#   
#   - heaping_window_days: Size of rolling window (e.g., 3 days)
#   - heaping_range_style: "single" (aggregate all) or "rolling" (sliding windows)
#   - heaping_stride_days: Step size for rolling windows
#   - value_prop_threshold: Digit preference threshold (default 0.15 = 15%)
#
# INTERPRETATION:
#   - Low heaping (<10% in any digit): Good data quality
#   - Moderate heaping (10-20%): Acceptable, monitor
#   - High heaping (>20%): Concerning, investigate
#   - Preference for 0, 5: Most common pattern (rounding)
#
# VARIABLES ANALYZED:
#   All clinical numeric variables EXCEPT excluded ones:
#     - Weight, Height, Waist Circ (analyzed)
#     - Systole, Diastole, Heart Rate (analyzed)
#     - Respiratory Rate, Temperature (excluded by default - high natural variation)
#
# OUTPUT TABLES:
#   - combined_all: Detailed heaping per variable per window
#   - preferred_digits_sum: Summary of preferred digits
#   - labs_tbl: Window labels and date ranges
#
# TRACING:
#   - Uses HEAP_TRACE system for calculation transparency
#   - Each window recorded with intermediate steps
#   - Enables auditing and validation
#
# WHY THIS MATTERS:
#   - Heaping is a sensitive indicator of data fabrication
#   - Helps identify which variables have quality issues
#   - Tracks improvement after training/interventions
#   - Provides quantitative metric for data quality reports
# ══════════════════════════════════════════════════════════════════════════════

suppressPackageStartupMessages({
  library(dplyr); library(tidyr); library(stringr); library(lubridate)
})

# Use the scoped + cleaned dataset, then drop excluded cols if helper exists
if (exists("drop_excluded_cols") && is.function(drop_excluded_cols)) {
  data_df <- drop_excluded_cols(data_df)
}

# ---- Parameters with validation ----
heaping_mode        <- tryCatch(tolower(as.character(params$heaping_mode)), error = function(e) "daily")
if (!heaping_mode %in% c("daily","range")) {
  warning("Invalid heaping_mode: '", params$heaping_mode, "'. Using 'daily'.")
  heaping_mode <- "daily"
}

heaping_window_days <- tryCatch(as.integer(params$heaping_window_days), error = function(e) 3L)
if (is.na(heaping_window_days) || heaping_window_days < 1L) {
  warning("Invalid heaping_window_days. Using default: 3")
  heaping_window_days <- 3L
}

heaping_range_style <- tryCatch(tolower(as.character(params$heaping_range_style)), error = function(e) "single")
if (!heaping_range_style %in% c("single","rolling")) {
  warning("Invalid heaping_range_style: '", params$heaping_range_style, "'. Using 'single'.")
  heaping_range_style <- "single"
}

heaping_stride_days <- tryCatch(as.integer(params$heaping_stride_days), error = function(e) 1L)
if (is.na(heaping_stride_days) || heaping_stride_days < 1L) {
  warning("Invalid heaping_stride_days. Using default: 1")
  heaping_stride_days <- 1L
}

# ── Heaping Detection Thresholds ──
# Threshold for identifying "preferred" last digits in heaping analysis
# A digit/tenth is considered "preferred" if it appears >= threshold proportion
# 
# value_prop_threshold = 0.15 means:
# - For integers (0-9): uniform expectation is 10% per digit
# - For decimals (.1-.9): uniform expectation is ~11.11% per tenth
# - A digit appearing ≥15% is flagged as potentially "heaped" (data fabrication indicator)
# 
# Rationale: In natural measurement data, last digits should be roughly uniform.
# Heaping (digit preference) suggests:
#   1) Rounding behavior (e.g., preference for 0, 5)
#   2) Data fabrication (fabricated data often shows non-random digit patterns)
#   3) Measurement tool limitations (e.g., analog scales read to nearest 5)
value_prop_threshold <- 0.15  # 15% = threshold for flagging "preferred" digits
clin_vars0 <- if (safe_df_check(data_df)) intersect(vars_to_check, names(data_df)) else character(0)
exclude_vars <- c("Respiratory Rate","Temperature")
clin_vars <- setdiff(clin_vars0, exclude_vars)

# Initialize result containers
heaping_ready <- FALSE
combined_all <- NULL
preferred_digits_sum <- NULL
labs_tbl <- NULL
title_prefix <- ""; title_suffix <- ""

# ---- Validation guards ----
if (!safe_df_check(data_df)) {
  cat(knitr::asis_output("> **Tidak ada data** (data_df is missing or empty).\n"))
} else if (!"Tanggal" %in% names(data_df)) {
  cat(knitr::asis_output("> **Kolom `Tanggal` tidak tersedia** dalam data_df.\n"))
} else if (!length(clin_vars)) {
  cat(knitr::asis_output("> **Tidak ada variabel klinis** (setelah pengecualian).\n"))
} else {

  # Robust coercion to Date + numeric for friendly clinical cols
  df_base <- data_df %>%
    mutate(
      Tanggal = dplyr::case_when(
        inherits(Tanggal, "Date")    ~ Tanggal,
        inherits(Tanggal, "POSIXt")  ~ as.Date(Tanggal),
        TRUE ~ suppressWarnings(as.Date(Tanggal))
      )
    ) %>%
    filter(!is.na(Tanggal)) %>%
    mutate(across(all_of(clin_vars), ~ {
      s <- as.character(.); s <- gsub("[^0-9,.-]", "", s); s <- gsub(",", ".", s, fixed = TRUE)
      suppressWarnings(as.numeric(s))
    }))

  # Core heaping computation for an input window (keeps your math)
  compute_combined <- function(df_win, tanggal_key, tanggal_label) {
  if (is.null(df_win) || !nrow(df_win)) return(NULL)

  # ── START a trace bucket for this window (daily date or range label)
  heaping_trace_start(tanggal_label)
  trace_push(tanggal_label, "df_base", df_win)   # window-scoped input (your base for this window)

  # (your existing code) build df_long
  df_long <- df_win %>%
    select(any_of(c("poliruangan","Staff Name")), all_of(clin_vars)) %>%
    tidyr::pivot_longer(cols = all_of(clin_vars), names_to = "Variabel", values_to = "Value") %>%
    filter(!is.na(Value)) %>%
    mutate(
      is_int          = is_integerish(Value),
      int_last_digit  = if_else(is_int, floor(abs(Value)) %% 10, NA_real_),
      frac_last_tenth = if_else(!is_int, round((Value %% 1), 1), NA_real_)
    )

  trace_push(tanggal_label, "df_long", df_long)

  # (your existing) denominators
  denoms <- df_long %>%
    group_by(Variabel) %>%
    summarise(Total_n = n(), .groups = "drop")
  trace_push(tanggal_label, "denoms", denoms)

  # (your existing) integer preferred summary — you can ALSO capture counts for transparency
  # counts (for trace/readability only; does not change your math)
  int_counts <- df_long %>%
    filter(is_int, !is.na(int_last_digit)) %>%
    mutate(int_last_digit = as.integer(int_last_digit)) %>%
    count(Variabel, int_last_digit, name = "n", .drop = FALSE) %>%
    tidyr::complete(Variabel, int_last_digit = 0:9, fill = list(n = 0)) %>%
    left_join(denoms, by = "Variabel") %>%
    mutate(prop = if_else(Total_n > 0, n / Total_n, NA_real_),
           prop_pct = round(prop * 100, 1))
  trace_push(tanggal_label, "int_counts", int_counts)

  int_full <- df_long %>%
    filter(is_int) %>%
    group_by(Variabel, int_last_digit) %>%
    summarise(n = n(), .groups = "drop") %>%
    left_join(denoms, by = "Variabel") %>%
    mutate(prop = n / Total_n, prop_pct = round(prop * 100, 1)) %>%
    group_by(Variabel) %>%
    summarise(
      Int_Preferred_Digits      = paste0(sort(int_last_digit[prop >= value_prop_threshold]), collapse = ", "),
      Int_Preferred_Digits_Prop = paste0(sprintf("%.1f%%", prop_pct[prop >= value_prop_threshold]), collapse = ", "),
      Int_pref_n                = sum(n[prop >= value_prop_threshold], na.rm = TRUE),

      .groups = "drop"
    ) %>%
    mutate(
      Int_Preferred_Digits      = ifelse(Int_Preferred_Digits == "", "-", Int_Preferred_Digits),
      Int_Preferred_Digits_Prop = ifelse(Int_Preferred_Digits_Prop == "", "-", Int_Preferred_Digits_Prop)
    )
  trace_push(tanggal_label, "int_full", int_full)

  # (your existing) decimal preferred summary — also capture counts for trace
  dec_counts <- df_long %>%
    filter(!is_int, !is.na(frac_last_tenth), frac_last_tenth != 0) %>%
    mutate(frac_last_tenth = round(frac_last_tenth, 1)) %>%
    count(Variabel, frac_last_tenth, name = "n", .drop = FALSE) %>%
    tidyr::complete(Variabel, frac_last_tenth = seq(0.1, 0.9, by = 0.1), fill = list(n = 0)) %>%
    left_join(denoms, by = "Variabel") %>%
    mutate(prop = if_else(Total_n > 0, n / Total_n, NA_real_),
           prop_pct = round(prop * 100, 1))
  trace_push(tanggal_label, "dec_counts", dec_counts)

  dec_full <- df_long %>%
    filter(!is_int, !is.na(frac_last_tenth), frac_last_tenth != 0) %>%
    group_by(Variabel, frac_last_tenth) %>%
    summarise(n = n(), .groups = "drop") %>%
    left_join(denoms, by = "Variabel") %>%
    mutate(prop = n / Total_n, prop_pct = round(prop * 100, 1)) %>%
    group_by(Variabel) %>%
    summarise(
      Dec_Preferred_Tenths      = paste0(sort(frac_last_tenth[prop >= value_prop_threshold]), collapse = ", "),
      Dec_Preferred_Tenths_Prop = paste0(sprintf("%.1f%%", prop_pct[prop >= value_prop_threshold]), collapse = ", "),
      Dec_pref_n                = sum(n[prop >= value_prop_threshold], na.rm = TRUE),

      .groups = "drop"
    ) %>%
    mutate(
      Dec_Preferred_Tenths      = ifelse(Dec_Preferred_Tenths == "", "-", Dec_Preferred_Tenths),
      Dec_Preferred_Tenths_Prop = ifelse(Dec_Preferred_Tenths_Prop == "", "-", Dec_Preferred_Tenths_Prop)
    )
  trace_push(tanggal_label, "dec_full", dec_full)

  # (your existing) combine (no double-count)
  combined <- denoms %>%
    left_join(int_full, by = "Variabel") %>%
    left_join(dec_full, by = "Variabel") %>%
    mutate(
      Int_pref_n           = coalesce(Int_pref_n, 0L),
      Dec_pref_n           = coalesce(Dec_pref_n, 0L),
      Total_pref_n         = Int_pref_n + Dec_pref_n,
      Total_Preferred_Prop = ifelse(Total_n > 0, round(100 * Total_pref_n / Total_n, 2), NA_real_),
      Tanggal_key          = as.Date(tanggal_key),
      Tanggal_label        = tanggal_label
    ) %>%
    select(
      Tanggal_key, Tanggal_label, Variabel, Total_n, Total_pref_n,
      Int_Preferred_Digits,  Int_Preferred_Digits_Prop, 
      Dec_Preferred_Tenths,  Dec_Preferred_Tenths_Prop,  
      Total_Preferred_Prop
    )

  trace_push(tanggal_label, "combined", combined)

  combined
}

  # --- build combined_all by mode ---
  if (heaping_mode == "daily") {
    days <- sort(unique(df_base$Tanggal))
    combined_all <- bind_rows(lapply(days, function(d) {
      compute_combined(df_base %>% filter(Tanggal == d), d, format(d, "%Y-%m-%d"))
    }))
    title_prefix <- "Daily Trends in Heaping: Average and by Variable (Total Preferred Last Digits)"

  } else if (heaping_range_style == "single") {
    d_end   <- max(df_base$Tanggal, na.rm = TRUE)
    d_start <- max(min(df_base$Tanggal, na.rm = TRUE), d_end - days(heaping_window_days - 1))
    combined_all <- compute_combined(
      df_base %>% filter(Tanggal >= d_start, Tanggal <= d_end),
      tanggal_key   = d_end,
      tanggal_label = paste0(format(d_start, "%Y-%m-%d"), "–", format(d_end, "%Y-%m-%d"))
    )
    title_prefix <- "Heaping (Total Preferred) — Aggregated"
    title_suffix <- paste0(" (last ", heaping_window_days, " days: ", format(d_start), "–", format(d_end), ")")

  } else { # range + rolling
    days <- sort(unique(df_base$Tanggal))
    ends <- if (length(days)) days[seq(1, length(days), by = heaping_stride_days)] else days
    combined_all <- bind_rows(lapply(ends, function(d_end) {
      d_start <- max(min(days), d_end - days(heaping_window_days - 1))
      compute_combined(
        df_base %>% filter(Tanggal >= d_start, Tanggal <= d_end),
        tanggal_key   = d_end,
        tanggal_label = paste0(format(d_start, "%Y-%m-%d"), "–", format(d_end, "%Y-%m-%d"))
      )
    }))
    title_prefix <- "Heaping (Total Preferred) — Rolling Window"
    title_suffix <- paste0(" (window=", heaping_window_days, " days; stride=", heaping_stride_days, ")")
  }

  if (!is.null(combined_all) && nrow(combined_all)) {
    # For table + axis mapping later
    preferred_digits_sum <- combined_all %>%
      transmute(
        Tanggal_key, Tanggal_label, Variabel,
        label = ifelse(is.na(Total_Preferred_Prop), "-", sprintf("%.2f%% (%d/%d)", Total_Preferred_Prop, Total_pref_n, Total_n)),
        pct = Total_Preferred_Prop, num = Total_pref_n, den = Total_n
      )
    labs_tbl <- combined_all %>% distinct(Tanggal_key, Tanggal_label) %>% arrange(Tanggal_key)
    heaping_ready <- TRUE
  } else {
    cat(knitr::asis_output("> **Tidak ada data** untuk membangun tabel/plot heaping.\n"))
  }
}

```

```{r last-digit-histogram, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}
# ─────────────────────────────────────────────────────────────────────────────
# Comparison Histograms (Previous K-day window vs Current K-day window)
# + Adds summary tables (n / N / %) BEFORE plots
# ─────────────────────────────────────────────────────────────────────────────
suppressPackageStartupMessages({
  library(dplyr); library(tidyr); library(stringr); library(lubridate); library(ggplot2)
  library(knitr); library(kableExtra)
})

# helper in case not defined in this chunk
`%||%` <- function(a, b) if (!is.null(a)) a else b

histcmp_int_plots <- list()
histcmp_dec_plots <- list()

# Window length (K days)
k_days <- tryCatch(as.integer(params$heaping_window_days), error = function(e) 3L)
if (is.na(k_days) || k_days < 1L) k_days <- 3L

# Anchor day: prefer params$date_to, else max(Tanggal) present
anchor_day <- tryCatch(as.Date(params$date_to), silent = TRUE)
if (inherits(anchor_day, "try-error") || is.na(anchor_day)) {
  if (!"Tanggal" %in% names(data_df)) {
    data_df$Tanggal <- suppressWarnings(as.Date(data_df$day))
  } else if (!inherits(data_df$Tanggal, "Date")) {
    data_df$Tanggal <- suppressWarnings(as.Date(data_df$Tanggal))
  }
  anchor_day <- suppressWarnings(max(data_df$Tanggal, na.rm = TRUE))
}

# Guard: rely on df_base/clin_vars from the main heaping chunk
if (!exists("df_base") || !nrow(df_base)) {
  cat(knitr::asis_output("> **Tidak ada `df_base`** dari chunk heaping utama. Jalankan chunk utama dulu.\n"))
  knitr::knit_exit()
}
if (!exists("clin_vars") || !length(clin_vars)) {
  cat(knitr::asis_output("> **Tidak ada `clin_vars`** (setelah pengecualian). Jalankan chunk utama dulu.\n"))
  knitr::knit_exit()
}
if (!exists("is_integerish")) {
  is_integerish <- function(x, tol = 1e-8) is.finite(x) & abs(x - round(x)) < tol
}

# Define equal-length windows
curr_end   <- anchor_day
curr_start <- curr_end - days(k_days - 1L)
prev_end   <- curr_start - days(1L)
prev_start <- prev_end - days(k_days - 1L)

curr_label <- paste0(format(curr_start, "%Y-%m-%d"), "–", format(curr_end, "%Y-%m-%d"))
prev_label <- paste0(format(prev_start, "%Y-%m-%d"), "–", format(prev_end, "%Y-%m-%d"))

df_curr <- df_base %>% filter(Tanggal >= curr_start, Tanggal <= curr_end)
df_prev <- df_base %>% filter(Tanggal >= prev_start, Tanggal <= prev_end)

# ---- Reuse the SAME digit extraction + counting logic as the main chunk ----
digit_hist_counts_from_df <- function(df_in, vars) {
  if (is.null(df_in) || !nrow(df_in)) return(list(int = NULL, dec = NULL))

  dl <- df_in %>%
    select(any_of(c("Tanggal","poliruangan","Staff Name")), all_of(vars)) %>%
    tidyr::pivot_longer(cols = all_of(vars), names_to = "Variabel", values_to = "Value") %>%
    filter(!is.na(Value)) %>%
    mutate(
      is_int          = is_integerish(Value),
      int_last_digit  = if_else(is_int, floor(abs(Value)) %% 10, NA_real_),
      frac_last_tenth = if_else(!is_int, round((Value %% 1), 1), NA_real_)
    )

  # Integer last digit 0..9
  int_df <- dl %>%
    filter(is_int, !is.na(int_last_digit)) %>%
    mutate(int_last_digit = as.integer(int_last_digit)) %>%
    count(Variabel, int_last_digit, name = "n", .drop = FALSE) %>%
    tidyr::complete(Variabel, int_last_digit = 0:9, fill = list(n = 0)) %>%
    group_by(Variabel) %>%
    mutate(Total_n = sum(n), prop_pct = if_else(Total_n > 0, 100 * n / Total_n, NA_real_)) %>%
    ungroup()

  # Decimal tenths 0.1..0.9 (exclude 0.0)
  dec_df <- dl %>%
    filter(!is_int, !is.na(frac_last_tenth), frac_last_tenth != 0) %>%
    mutate(frac_last_tenth = round(frac_last_tenth, 1)) %>%
    count(Variabel, frac_last_tenth, name = "n", .drop = FALSE) %>%
    tidyr::complete(Variabel, frac_last_tenth = seq(0.1, 0.9, by = 0.1), fill = list(n = 0)) %>%
    group_by(Variabel) %>%
    mutate(Total_n = sum(n), prop_pct = if_else(Total_n > 0, 100 * n / Total_n, NA_real_)) %>%
    ungroup()

  list(int = int_df, dec = dec_df)
}

curr_sum <- digit_hist_counts_from_df(df_curr, clin_vars)
prev_sum <- digit_hist_counts_from_df(df_prev, clin_vars)

if (exists("trace_push")) {
  trace_push(prev_label, "histcmp_int_counts", prev_sum$int)
  trace_push(prev_label, "histcmp_dec_counts", prev_sum$dec)
  trace_push(curr_label, "histcmp_int_counts", curr_sum$int)
  trace_push(curr_label, "histcmp_dec_counts", curr_sum$dec)
}

# Combine with Period labels
int_both <- NULL; dec_both <- NULL
if (!is.null(curr_sum$int) && nrow(curr_sum$int)) {
  int_both <- bind_rows(
    mutate(curr_sum$int, Period = "Current"),
    mutate(prev_sum$int %||% curr_sum$int[0,], Period = "Previous")
  )
}
if (!is.null(curr_sum$dec) && nrow(curr_sum$dec)) {
  dec_both <- bind_rows(
    mutate(curr_sum$dec, Period = "Current"),
    mutate(prev_sum$dec %||% curr_sum$dec[0,], Period = "Previous")
  )
}

# Threshold and baselines (use your global value_prop_threshold if present)
thr_pct       <- (get0("value_prop_threshold", ifnotfound = 0.15)) * 100
baseline_int  <- 10            # uniform last-digit baseline: 10%
baseline_dec  <- 100/9         # uniform tenths baseline: ~11.11%

# ── NEW: Pretty tables BEFORE plots ───────────────────────────────────────────
fmt_pct <- function(x) ifelse(is.na(x), NA, sprintf("%.1f%%", x))

# Integer-digit tables
if (!is.null(int_both) && nrow(int_both)) {
  for (v in sort(unique(int_both$Variabel))) {
    dv <- int_both %>% filter(Variabel == v)

    tbl <- dv %>%
      transmute(
        Digit = as.character(int_last_digit),
        Period,
        Numerator  = n,
        Denominator = Total_n,
        Percent   = fmt_pct(prop_pct)
      ) %>%
      arrange(Period, as.integer(Digit)) %>%
      tidyr::pivot_wider(names_from = Period, values_from = c(Numerator, Denominator, Percent))

    # Ensure sane column order even if Previous is missing
    ord <- c(
      "Digit",
      intersect(c("Numerator_Previous","Denominator_Previous","Percent_Previous"), names(tbl)),
      intersect(c("Numerator_Current","Denominator_Current","Percent_Current"), names(tbl))
    )
    tbl <- tbl %>% select(all_of(ord))

    k <- knitr::kable(
      tbl, align = "r",
      caption = sprintf("Integer last-digit counts — %s (Previous: %s · Current: %s)", v, prev_label, curr_label)
    ) %>% kableExtra::kable_styling(full_width = FALSE, font_size = 11)

    if (all(c("Numerator_Previous","Denominator_Previous","Percent_Previous") %in% names(tbl)) &&
        all(c("Numerator_Current","Denominator_Current","Percent_Current") %in% names(tbl))) {
      k <- k %>% kableExtra::add_header_above(c(" " = 1, "Previous" = 3, "Current" = 3))
    } else if (all(c("Numerator_Previous","Denominator_Previous","Percent_Previous") %in% names(tbl))) {
      k <- k %>% kableExtra::add_header_above(c(" " = 1, "Previous" = 3))
    } else if (all(c("Numerator_Current","Denominator_Current","Percent_Current") %in% names(tbl))) {
      k <- k %>% kableExtra::add_header_above(c(" " = 1, "Current" = 3))
    }

    print(k)
    if (exists("trace_push")) trace_push(curr_label, paste0("histcmp_int_table_", v), tbl)
  }
} else {
  cat(knitr::asis_output("> **Tidak ada data** untuk tabel digit integer (perbandingan)."))
}

# Decimal-digit tables
if (!is.null(dec_both) && nrow(dec_both)) {
  for (v in sort(unique(dec_both$Variabel))) {
    dv <- dec_both %>% filter(Variabel == v)

    tbl <- dv %>%
      transmute(
        Digit = format(frac_last_tenth, nsmall = 1),
        Period,
        Numerator  = n,
        Denominator = Total_n,
        Percent   = fmt_pct(prop_pct)
      ) %>%
      arrange(Period, as.numeric(Digit)) %>%
      tidyr::pivot_wider(names_from = Period, values_from = c(Numerator, Denominator, Percent))

    ord <- c(
      "Digit",
      intersect(c("Numerator_Previous","Denominator_Previous","Percent_Previous"), names(tbl)),
      intersect(c("Numerator_Current","Denominator_Current","Percent_Current"), names(tbl))
    )
    tbl <- tbl %>% select(all_of(ord))

    k <- knitr::kable(
      tbl, align = "r",
      caption = sprintf("Decimal-tenths counts — %s (Previous: %s · Current: %s)", v, prev_label, curr_label)
    ) %>% kableExtra::kable_styling(full_width = FALSE, font_size = 11)

    if (all(c("Numerator_Previous","Denominator_Previous","Percent_Previous") %in% names(tbl)) &&
        all(c("Numerator_Current","Denominator_Current","Percent_Current") %in% names(tbl))) {
      k <- k %>% kableExtra::add_header_above(c(" " = 1, "Previous" = 3, "Current" = 3))
    } else if (all(c("Numerator_Previous","Denominator_Previous","Percent_Previous") %in% names(tbl))) {
      k <- k %>% kableExtra::add_header_above(c(" " = 1, "Previous" = 3))
    } else if (all(c("Numerator_Current","Denominator_Current","Percent_Current") %in% names(tbl))) {
      k <- k %>% kableExtra::add_header_above(c(" " = 1, "Current" = 3))
    }

    print(k)
    if (exists("trace_push")) trace_push(curr_label, paste0("histcmp_dec_table_", v), tbl)
  }
} else {
  cat(knitr::asis_output("> **Tidak ada data** untuk tabel digit desimal (perbandingan)."))
}

# Helpers for formatting
fmt_n   <- function(x) ifelse(is.na(x), "0", format(as.integer(x), big.mark = ",", scientific = FALSE))
pct_lab <- function(p, n) ifelse(is.finite(p) & n > 0, sprintf("%.1f%%", p), NA_character_)

# ---- Integer last-digit plots (adds labels + N in subtitle) ----
if (!is.null(int_both) && nrow(int_both)) {
  for (v in sort(unique(int_both$Variabel))) {
    dv <- int_both %>%
      filter(Variabel == v) %>%
      group_by(int_last_digit) %>%
      filter(any(is.finite(prop_pct))) %>%
      ungroup()
    if (!nrow(dv)) next

    # Denominators per period
    N_prev <- dv %>% filter(Period == "Previous") %>% summarise(N = unique(Total_n)) %>% pull(N)
    N_curr <- dv %>% filter(Period == "Current")  %>% summarise(N = unique(Total_n)) %>% pull(N)

    p <- ggplot(
      dv,
      aes(x = factor(int_last_digit, levels = as.character(0:9)),
          y = prop_pct, fill = Period)
    ) +
      geom_col(position = position_dodge(width = 0.8), width = 0.75, alpha = 0.9) +
      # Percentage labels on top of bars
      geom_text(
        aes(label = pct_lab(prop_pct, n)),
        position = position_dodge(width = 0.8),
        vjust = -0.35, size = 3.2, na.rm = TRUE
      ) +
      # Baseline (dotted) + Threshold (dashed)
      geom_hline(yintercept = baseline_int, linetype = "dotted",  linewidth = 0.6, colour = "grey50") +
      geom_hline(yintercept = thr_pct,      linetype = "dashed",  linewidth = 0.8, colour = "#D55E00") +
      # Add a little headroom so labels don't get clipped
      scale_y_continuous(limits = c(0, 100), expand = expansion(mult = c(0, .08))) +
      labs(
        title = paste0("Last-digit Histogram — ", v),
        subtitle = paste0(
          "Previous (", prev_label, ")  vs  Current (", curr_label, ")\n",
          "N_prev = ", fmt_n(N_prev), " · N_curr = ", fmt_n(N_curr)
        ),
        x = "Last digit (0–9)", y = "Proporsi (%)",
        caption = sprintf("Baseline: 10%% (dotted) · Threshold: %.0f%% (dashed)", thr_pct)
      ) +
      theme_minimal(base_size = 12) +
      theme(legend.position = "bottom")
    histcmp_int_plots[[v]] <- p
    print(p)
  }
} else {
  cat(knitr::asis_output("> **Tidak ada data** untuk histogram digit integer (perbandingan)."))
}

# ---- Decimal tenths plots (adds labels + N in subtitle) ----
if (!is.null(dec_both) && nrow(dec_both)) {
  dec_levels_chr <- format(seq(0.1, 0.9, by = 0.1), nsmall = 1)
  for (v in sort(unique(dec_both$Variabel))) {
    dv <- dec_both %>%
      filter(Variabel == v) %>%
      group_by(frac_last_tenth) %>%
      filter(any(is.finite(prop_pct))) %>%
      ungroup()
    if (!nrow(dv)) next

    # Denominators per period
    N_prev <- dv %>% filter(Period == "Previous") %>% summarise(N = unique(Total_n)) %>% pull(N)
    N_curr <- dv %>% filter(Period == "Current")  %>% summarise(N = unique(Total_n)) %>% pull(N)

    p <- ggplot(
      dv,
      aes(x = factor(frac_last_tenth, levels = as.numeric(dec_levels_chr)),
          y = prop_pct, fill = Period)
    ) +
      geom_col(position = position_dodge(width = 0.8), width = 0.75, alpha = 0.9) +
      # Percentage labels on top of bars
      geom_text(
        aes(label = pct_lab(prop_pct, n)),
        position = position_dodge(width = 0.8),
        vjust = -0.35, size = 3.2, na.rm = TRUE
      ) +
      # Baseline (dotted) + Threshold (dashed)
      geom_hline(yintercept = baseline_dec, linetype = "dotted",  linewidth = 0.6, colour = "grey50") +
      geom_hline(yintercept = thr_pct,      linetype = "dashed",  linewidth = 0.8, colour = "#D55E00") +
      scale_x_discrete(labels = dec_levels_chr) +
      scale_y_continuous(limits = c(0, 100), expand = expansion(mult = c(0, .08))) +
      labs(
        title = paste0("Tenths Histogram — ", v),
        subtitle = paste0(
          "Previous (", prev_label, ")  vs  Current (", curr_label, ")\n",
          "N_prev = ", fmt_n(N_prev), " · N_curr = ", fmt_n(N_curr)
        ),
        x = "Digit desimal (0.1–0.9)", y = "Proporsi (%)",
        caption = sprintf("Baseline: %.2f%% (dotted) · Threshold: %.0f%% (dashed)", baseline_dec, thr_pct)
      ) +
      theme_minimal(base_size = 12) +
      theme(legend.position = "bottom")
    histcmp_dec_plots[[v]] <- p
    print(p)
  }
} else {
  cat(knitr::asis_output("> **Tidak ada data** untuk histogram digit desimal (perbandingan)."))
}




```


```{r}
suppressPackageStartupMessages({ library(dplyr); library(ggplot2); library(scales); library(lubridate); })


# ─────────────────────────────────────────────────────────────────────────────
# Heaping — Trend plot (ALWAYS last one month)
# Requires: combined_all (built by your main heaping calculation chunk)
# Produces: plotheaping (ggplot)
# ─────────────────────────────────────────────────────────────────────────────
suppressPackageStartupMessages({
  library(dplyr)
  library(ggplot2)
  library(lubridate)
  library(knitr)
})

plotheaping <- NULL

if (!exists("combined_all") || is.null(combined_all) || !nrow(combined_all)) {
  cat(knitr::asis_output("> **Tidak ada data** untuk mem-plot tren heaping."))
} else {
  # Base data
  plot_data_all <- combined_all %>%
    dplyr::select(Tanggal_key, Variabel, Total_Preferred_Prop) %>%
    dplyr::filter(!is.na(Total_Preferred_Prop), !is.na(Tanggal_key))

  if (!nrow(plot_data_all)) {
    cat(knitr::asis_output("> **Tidak ada data** untuk mem-plot tren heaping."))
  } else {
    # ---- Anchor & 1-month window ----
    anchor_day <- tryCatch(as.Date(params$date_to), error = function(e) NA)
    if (is.na(anchor_day)) {
      anchor_day <- suppressWarnings(max(plot_data_all$Tanggal_key, na.rm = TRUE))
    }
    window_start <- anchor_day - days(30)
    window_end   <- anchor_day

    # Filter to the same 1-month window
    plot_data <- plot_data_all %>%
      dplyr::filter(Tanggal_key >= window_start, Tanggal_key <= window_end)

    if (!nrow(plot_data)) {
      cat(knitr::asis_output("> **Tidak ada data** pada jendela 1 bulan terakhir untuk tren heaping."))
    } else {
      # Average per date
      avg_data <- plot_data %>%
        dplyr::group_by(Tanggal_key) %>%
        dplyr::summarise(Average_Prop = mean(Total_Preferred_Prop, na.rm = TRUE), .groups = "drop")

      # Colors
      var_levels <- sort(unique(plot_data$Variabel))
      pal <- setNames(scales::hue_pal()(length(var_levels)), var_levels)

      # Title
      heaping_mode        <- tryCatch(tolower(as.character(params$heaping_mode)),        error = function(e) "daily")
      heaping_range_style <- tryCatch(tolower(as.character(params$heaping_range_style)), error = function(e) "single")
      heaping_window_days <- tryCatch(as.integer(params$heaping_window_days),            error = function(e) NA_integer_)
      heaping_stride_days <- tryCatch(as.integer(params$heaping_stride_days),            error = function(e) NA_integer_)

      title_prefix <- if (heaping_mode == "daily") {
        "Daily Trends in Heaping: Average and by Variable (Total Preferred Last Digits)"
      } else if (heaping_range_style == "rolling") {
        sprintf("Heaping (Total Preferred) — Rolling Window (window=%s days; stride=%s)",
                ifelse(is.na(heaping_window_days), "?", heaping_window_days),
                ifelse(is.na(heaping_stride_days), "?", heaping_stride_days))
      } else {
        sprintf("Heaping (Total Preferred) — Aggregated (last %s days)",
                ifelse(is.na(heaping_window_days), "?", heaping_window_days))
      }

      # Tick breaks weekly; fallback to unique keys if sparse
      break_vals <- seq(window_start, window_end, by = "1 week")
      if (length(break_vals) < 2L) break_vals <- sort(unique(plot_data$Tanggal_key))

      # Build the plot
      p <- ggplot(
        plot_data,
        aes(x = Tanggal_key, y = Total_Preferred_Prop, color = Variabel, group = Variabel)
      ) +
        geom_line(linewidth = 1) +
        geom_point(size = 2) +
        geom_line(
          data = avg_data,
          aes(Tanggal_key, Average_Prop, color = "Rata-rata", linetype = "Rata-rata", group = 1),
          linewidth = 1.2, inherit.aes = FALSE
        ) +
        scale_color_manual(values = c(pal, "Rata-rata" = "black"),
                           breaks = c("Rata-rata", var_levels)) +
        scale_linetype_manual(values = c("Rata-rata" = "dashed"), breaks = "Rata-rata") +
        scale_x_date(
          limits = c(window_start, window_end),
          breaks  = break_vals,
          labels  = function(x) format(as.Date(x), "%y-%m-%d"),
          expand  = c(0.01, 0.01)
        ) +
        coord_cartesian(ylim = c(0, 100)) +
        labs(
          title   = paste0(title_prefix, " — Last 1 Month"),
          x       = "Date",
          y       = "Total Preferred Digits (%)",
          color   = NULL,
          linetype= NULL
        ) +
        theme_minimal(base_size = 13) +
        theme(
          axis.text.x  = element_text(size = 9, margin = margin(t = 6)),
          plot.margin  = margin(5.5, 5.5, 18, 5.5),
          legend.position = "bottom",
          legend.direction = "horizontal",
          legend.box = "horizontal"
        )

      if (exists("guide_axis")) p <- p + guides(x = guide_axis(n.dodge = 2))

      plotheaping <- p
      print(plotheaping)
    }
  }
}



```


```{r heaping-debug, echo=TRUE, results='asis', message=FALSE, warning=FALSE}
# ══════════════════════════════════════════════════════════════════════════════
# Heaping Analysis — Step-by-Step Debug & Transparency
# ══════════════════════════════════════════════════════════════════════════════
# Displays all intermediate calculation steps with validation checks
# for auditing and debugging heaping detection
# ══════════════════════════════════════════════════════════════════════════════

suppressPackageStartupMessages({
  library(dplyr); library(tidyr); library(stringr)
  library(ggplot2); library(knitr); library(kableExtra); library(lubridate)
})

options(dplyr.summarise.inform = FALSE)

# ══════════════════════════════════════════════════════════════════════════════
# Debug Configuration
# ══════════════════════════════════════════════════════════════════════════════
DEBUG_SHOW <- TRUE            # Master switch (set FALSE to hide all debug output)
DEBUG_N    <- 12              # Number of rows to preview per table
debug_var  <- NULL            # Focus on specific variable (e.g., "Height") or NULL for all
debug_date <- NULL            # Focus on specific date (e.g., as.Date("2025-08-15")) or NULL for all
poli_filter <- NULL           # Filter by Poli/Ruangan (e.g., "UGD") or NULL for no filter

# ══════════════════════════════════════════════════════════════════════════════
# Pretty Debug Helpers
# ══════════════════════════════════════════════════════════════════════════════
emit_h3  <- function(txt) cat(knitr::asis_output(paste0("\n\n### ", txt, "\n")))
emit_note<- function(txt) cat(knitr::asis_output(paste0("> ", txt, "\n")))

peek_tbl <- function(df, title, n = DEBUG_N) {
  emit_h3(title)
  if (!nrow(df)) { emit_note("(**empty**)"); return(invisible(NULL)) }
  if (isTRUE(knitr::is_html_output())) {
    tb <- knitr::kable(head(df, n)) |>
      kableExtra::kable_styling(full_width = TRUE,
                                bootstrap_options = c("striped","hover","condensed"))
  } else {
    tb <- knitr::kable(head(df, n), format = "simple")
  }
  print(tb)
  invisible(tb)
}

ok_x <- function(ok, msg_ok, msg_bad) {
  sym <- if (isTRUE(ok)) "✅" else "❌"
  emit_note(paste(sym, if (isTRUE(ok)) msg_ok else msg_bad))
}

# ══════════════════════════════════════════════════════════════════════════════
# Use traced data from HEAP_TRACE or fall back to data_df
# ══════════════════════════════════════════════════════════════════════════════
if (exists("HEAP_TRACE") && is.environment(HEAP_TRACE) && length(trace_keys()) > 0) {
  emit_note(paste0("**Using traced data from HEAP_TRACE.** Available keys: ", paste(trace_keys(), collapse = ", ")))
  
  # Get the most recent trace key (or specify manually)
  trace_key <- trace_keys()[length(trace_keys())]  # Latest window
  emit_note(paste0("Displaying trace for: **", trace_key, "**"))
  
  bucket <- trace_get(trace_key)
  if (!is.null(bucket)) {
    # Extract traced intermediate steps
    if (DEBUG_SHOW) {
      #if (!is.null(bucket$df_base))    peek_tbl(bucket$df_base, "Step 0 — df_base (window input)")
      if (!is.null(bucket$df_long))    peek_tbl(bucket$df_long, "Step 1 — df_long (long format with features)")
      if (!is.null(bucket$denoms))     peek_tbl(bucket$denoms, "Step 2 — denoms (Total_n per Variabel)")
      if (!is.null(bucket$int_counts)) peek_tbl(bucket$int_counts, "Step 3a — int_counts (digit 0-9 counts)")
      if (!is.null(bucket$int_full))   peek_tbl(bucket$int_full, "Step 3b — int_full (preferred digits summary)")
      if (!is.null(bucket$dec_counts)) peek_tbl(bucket$dec_counts, "Step 4a — dec_counts (tenths .1-.9 counts)")
      if (!is.null(bucket$dec_full))   peek_tbl(bucket$dec_full, "Step 4b — dec_full (preferred tenths summary)")
      if (!is.null(bucket$combined))   peek_tbl(bucket$combined, "Step 5 — combined (integer + decimal, no double-count)")
    }
  }
} else if (exists("data_df") && safe_df_check(data_df)) {
  emit_note("**No HEAP_TRACE found.** Performing fresh heaping analysis on data_df...")
  
  # ════════════════════════════════════════════════════════════════════════════
  # Fresh Analysis (if no trace available)
  # ════════════════════════════════════════════════════════════════════════════
  
  # Determine which variables to analyze
  clin_vars <- if (exists("vars_to_check_active")) {
    intersect(vars_to_check_active, names(data_df))
  } else if (exists("vars_to_check")) {
    intersect(vars_to_check, names(data_df))
  } else {
    character(0)
  }
  
  # Apply exclusions
  if (exists("EXCLUDE_VARS")) {
    clin_vars <- setdiff(clin_vars, EXCLUDE_VARS)
  }
  
  if (!length(clin_vars) || !"Tanggal" %in% names(data_df)) {
    cat(knitr::asis_output("> **Cannot perform analysis:** No clinical variables or Tanggal column missing.\n"))
  } else {
    
    # Step 0: Optional Poli filter
    #df0 <- data_df
    #if (!is.null(poli_filter) && any(str_detect(names(df0), regex("poli|ruangan", ignore_case = TRUE)))) {
    #  poli_col <- names(df0)[str_detect(names(df0), regex("poli|ruangan", ignore_case = TRUE))][1]
    #  df0 <- df0 %>% filter(str_detect(.data[[poli_col]], regex(poli_filter, ignore_case = TRUE)))
    #  if (DEBUG_SHOW) emit_note(paste("Filtered by", poli_col, "containing:", poli_filter))
    #}
    
    # Step 1: Coerce to numeric
    df_base <- df0 %>%
      mutate(
        Tanggal = case_when(
          inherits(Tanggal, "Date")   ~ Tanggal,
          inherits(Tanggal, "POSIXt") ~ as.Date(Tanggal),
          TRUE                        ~ suppressWarnings(as.Date(Tanggal))
        )
      ) %>%
      filter(!is.na(Tanggal)) %>%
      mutate(across(all_of(clin_vars), safe_num))
    
    if (DEBUG_SHOW) {
      emit_h3("Step 1 — After Numeric Coercion")
      na_scan <- clin_vars %>% purrr::map_dfr(function(v){
        x <- df_base[[v]]
        tibble(
          Variabel = v,
          n = length(x),
          n_nonNA = sum(!is.na(x)),
          n_NA = sum(is.na(x)),
          prop_NA = round(mean(is.na(x))*100, 2),
          min = suppressWarnings(min(x, na.rm = TRUE)),
          max = suppressWarnings(max(x, na.rm = TRUE))
        )
      })
      peek_tbl(na_scan, "Coercion summary (NA rate, min, max)")
    }
    
    # Step 2: Long format + features
    df_long <- df_base %>%
      select(Tanggal, any_of(c("poliruangan","Staff Name")), all_of(clin_vars)) %>%
      pivot_longer(cols = all_of(clin_vars), names_to = "Variabel", values_to = "Value") %>%
      filter(!is.na(Value)) %>%
      mutate(
        is_int          = is_integerish(Value),
        int_last_digit  = if_else(is_int, floor(abs(Value)) %% 10, NA_real_),
        frac_last_tenth = if_else(!is_int, round((Value %% 1), 1), NA_real_)
      )
    
    if (!is.null(debug_var))  df_long <- df_long %>% filter(Variabel == debug_var)
    if (!is.null(debug_date)) df_long <- df_long %>% filter(Tanggal == debug_date)
    
    if (DEBUG_SHOW) {
      peek_tbl(df_long, "Step 2 — df_long (long format with features)")
      
      counts_mode <- df_long %>% count(Variabel, Tanggal, is_int, name = "n")
      peek_tbl(counts_mode %>% arrange(desc(n)), "Integer vs Decimal counts per Variable/Date")
    }
    
    # Step 3: Denominators
    denoms <- df_long %>%
      group_by(Tanggal, Variabel) %>%
      summarise(Total_n = n(), .groups = "drop")
    
    if (DEBUG_SHOW) peek_tbl(denoms, "Step 3 — Denominators (Total_n per Variable/Date)")
    
    # Step 4: Integer analysis
    int_counts <- df_long %>%
      filter(is_int) %>%
      count(Tanggal, Variabel, int_last_digit, name = "n") %>%
      left_join(denoms, by = c("Tanggal","Variabel")) %>%
      mutate(prop = n / Total_n, prop_pct = round(prop * 100, 1))
    
    threshold <- get0("value_prop_threshold", ifnotfound = 0.15)
    
    int_full <- int_counts %>%
      group_by(Tanggal, Variabel) %>%
      summarise(
        Int_Preferred_Digits      = paste0(sort(int_last_digit[prop >= threshold]), collapse = ", "),
        Int_Preferred_Digits_Prop = paste0(sprintf("%.1f%%", (prop*100)[prop >= threshold]), collapse = ", "),
        Int_pref_n                = sum(n[prop >= threshold], na.rm = TRUE),
    
        .groups = "drop"
      ) %>%
      mutate(
        Int_Preferred_Digits      = ifelse(Int_Preferred_Digits == "", "-", Int_Preferred_Digits),
        Int_Preferred_Digits_Prop = ifelse(Int_Preferred_Digits_Prop == "", "-", Int_Preferred_Digits_Prop)
      )
    
    if (DEBUG_SHOW) {
      peek_tbl(int_counts %>% arrange(desc(n)), "Step 4a — Integer last digit distribution")
      peek_tbl(int_full, "Step 4b — Integer preferred summary")
      
      # Sanity check
      int_vs_den <- int_counts %>% group_by(Tanggal, Variabel) %>%
        summarise(n_int = sum(n), Total_n = first(Total_n), .groups = "drop") %>%
        mutate(ok = n_int <= Total_n)
      ok_x(all(int_vs_den$ok), 
           "Integer counts ≤ Total_n (valid)", 
           "Some integer counts > Total_n (ERROR)")
    }
    
    # Step 5: Decimal analysis
    dec_counts <- df_long %>%
      filter(!is_int, !is.na(frac_last_tenth), frac_last_tenth != 0) %>%
      count(Tanggal, Variabel, frac_last_tenth, name = "n") %>%
      left_join(denoms, by = c("Tanggal","Variabel")) %>%
      mutate(prop = n / Total_n, prop_pct = round(prop * 100, 1))
    
    dec_full <- dec_counts %>%
      group_by(Tanggal, Variabel) %>%
      summarise(
        Dec_Preferred_Tenths      = paste0(sort(frac_last_tenth[prop >= threshold]), collapse = ", "),
        Dec_Preferred_Tenths_Prop = paste0(sprintf("%.1f%%", (prop*100)[prop >= threshold]), collapse = ", "),
        Dec_pref_n                = sum(n[prop >= threshold], na.rm = TRUE),
       
        .groups = "drop"
      ) %>%
      mutate(
        Dec_Preferred_Tenths      = ifelse(Dec_Preferred_Tenths == "", "-", Dec_Preferred_Tenths),
        Dec_Preferred_Tenths_Prop = ifelse(Dec_Preferred_Tenths_Prop == "", "-", Dec_Preferred_Tenths_Prop)
      )
    
    if (DEBUG_SHOW) {
      peek_tbl(dec_counts %>% arrange(desc(n)), "Step 5a — Decimal tenths distribution")
      peek_tbl(dec_full, "Step 5b — Decimal preferred summary")
    }
    
    # Step 6: Combine (no double-count)
    combined <- denoms %>%
      left_join(int_full, by = c("Tanggal","Variabel")) %>%
      left_join(dec_full, by = c("Tanggal","Variabel")) %>%
      mutate(
        Int_pref_n   = coalesce(Int_pref_n, 0L),
        Dec_pref_n   = coalesce(Dec_pref_n, 0L),
        Total_pref_n = Int_pref_n + Dec_pref_n,
        Total_Preferred_Prop = ifelse(Total_n > 0, round(100 * Total_pref_n / Total_n, 2), NA_real_)
      )
    
    if (DEBUG_SHOW) {
      peek_tbl(combined, "Step 6 — Combined (integer + decimal, no double-count)")
      
      # Final sanity check
      ok_x(all(combined$Total_pref_n == (combined$Int_pref_n + combined$Dec_pref_n)),
           "Total_pref_n = Int_pref_n + Dec_pref_n (valid)",
           "Mismatch in Total_pref_n calculation (ERROR)")
    }
    
    emit_note("✅ **Analysis complete.** All intermediate steps displayed above.")
  }
} else {
  cat(knitr::asis_output("> **No data available** for heaping analysis (neither HEAP_TRACE nor data_df found).\n"))
}

```


```{r echo=TRUE, message=FALSE, warning=FALSE, results='asis'}

suppressPackageStartupMessages({ library(openxlsx); library(lubridate) })

dir.create("outputs", showWarnings = FALSE, recursive = TRUE)
stamp <- format(Sys.time(), "%Y%m%d_%H%M%S")
xlsx_path <- file.path("outputs", paste0("heaping_export_", stamp, ".xlsx"))

wb <- createWorkbook()

addWorksheet(wb, "preferred_digits_sum")
writeDataTable(wb, "preferred_digits_sum", preferred_digits_sum)
freezePane(wb, "preferred_digits_sum", firstRow = TRUE)

addWorksheet(wb, "combined_all")
writeDataTable(wb, "combined_all", combined_all)
freezePane(wb, "combined_all", firstRow = TRUE)

addWorksheet(wb, "labs_tbl")
writeDataTable(wb, "labs_tbl", labs_tbl)
freezePane(wb, "labs_tbl", firstRow = TRUE)

params_df <- tibble::tibble(
  heaping_mode, heaping_window_days, heaping_range_style, heaping_stride_days,
  value_prop_threshold, title_prefix, title_suffix,
  exported_at = format(Sys.time(), "%Y-%m-%d %H:%M:%S %Z")
)
addWorksheet(wb, "params")
writeDataTable(wb, "params", params_df)

# (Optional) include your ggplot if you have one (plotheaping)
if (exists("plotheaping") && !is.null(plotheaping)) {
  img_path <- file.path("outputs", paste0("plotheaping_", stamp, ".png"))
  ggplot2::ggsave(img_path, plot = plotheaping, width = 10, height = 5, dpi = 150)
  addWorksheet(wb, "plots")
  insertImage(wb, "plots", img_path, startRow = 1, startCol = 1, width = 24, height = 12, units = "cm")
}

saveWorkbook(wb, xlsx_path, overwrite = TRUE)
cat(knitr::asis_output(paste0("> ✅ Exported to **", xlsx_path, "**")))

```


# PART 6: INTERRUPTED TIME SERIES (ITS) ANALYSIS - ARIMAX PER VARIABLE {.tabset}

## 6.1 ITS Setup & Helper Functions

```{r its-setup, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}

# ══════════════════════════════════════════════════════════════════════════════
# PART 6.1: ITS ARIMAX SETUP & HELPER FUNCTIONS
# ══════════════════════════════════════════════════════════════════════════════
# PURPOSE:
#   Configure Interrupted Time Series (ITS) analysis with ARIMAX modeling
#   for ROLLING WINDOW HEAPING DATA
#
# WHAT THIS DOES:
#   1. Set up ITS configuration parameters
#   2. Define helper functions for intervention regressors
#   3. Load required time series packages (forecast, etc.)
#   4. Prepare infrastructure for per-variable ARIMAX models
#
# ⚠️ IMPORTANT: ROLLING WINDOW DATA CONSIDERATIONS:
#   This ITS analysis is designed to work with 3-day rolling window heaping data
#   from Section 5.2. Key differences from daily data:
#   
#   1. **Observation Frequency**: With stride=3 days, we have ~2.3 observations per week
#      - This affects time series frequency parameter
#      - Fourier terms adjusted accordingly (K=1 for sparse, K=2 for dense)
#   
#   2. **No Daily Gap Filling**: We use window end dates directly
#      - Respects the rolling window structure
#      - Avoids artificial interpolation of missing heaping values
#   
#   3. **Gap Detection**: Enhanced checks for data sparsity
#      - Flags gaps >14 days between observations
#      - Requires minimum 28-day span for weekly seasonality
#      - Minimum 10 observations (vs 14 for daily data)
#   
#   4. **Bounded Outcome**: Heaping % is bounded [0, 100]
#      - Forecasts are clipped to valid range
#      - ARIMAX treats as unbounded, we apply post-forecast bounds
#   
#   5. **Future Regressors**: Properly extrapolated for interventions
#      - Steps: Continue at constant level
#      - Ramps: Continue linear growth
#      - Pulses: Decay to zero (no future pulses assumed)
#
# INTERRUPTED TIME SERIES (ITS) OVERVIEW:
#   ITS is a quasi-experimental design for evaluating intervention effects
#   - Compares trends BEFORE vs AFTER intervention
#   - Controls for baseline trends and seasonality
#   - Quantifies both immediate (level change) and sustained (slope change) effects
#   - Does NOT require control group (uses historical baseline)
#
# ARIMAX MODEL:
#   ARIMAX = AutoRegressive Integrated Moving Average with eXogenous variables
#   - ARIMA: Models baseline time series (trend + seasonality + noise)
#   - X: Adds intervention indicators as external regressors
#   - Auto.arima: Automatically selects best ARIMA(p,d,q) orders
#
# INTERVENTION TYPES MODELED:
#   
#   1. LEVEL CHANGE (Step Function):
#      - Immediate, sustained shift in outcome level
#      - Regressor: cumsum indicator (0,0,0,1,1,1,...)
#      - Example: Training session → immediate improvement
#      - make_cumsum_series() creates this regressor
#   
#   2. TREND CHANGE (Ramp Function):
#      - Gradual, accumulating effect over time
#      - Regressor: ramp (0,0,0,1,2,3,...)
#      - Example: Supervision → progressive improvement
#      - make_ramp_series() creates this regressor
#   
#   3. PULSE (Temporary Effect):
#      - Short-term spike, returns to baseline
#      - Regressor: single spike (0,0,0,1,0,0,...)
#      - Example: One-time data cleaning
#
# CONFIGURATION PARAMETERS:
#   - events_path: Path to Excel file with intervention dates/types
#   - events_sheet: Sheet name in Excel (default: "events")
#   - horizon: Forecast horizon in days (default: 14)
#   - seed: Random seed for reproducibility
#   - Column mappings for events file (Place, event_type, dates, intensity)
#
# HELPER FUNCTIONS:
#   - parse_dt(): Robust datetime parsing with multiple format support
#   - make_cumsum_series(): Build level change regressor
#   - make_ramp_series(): Build trend change regressor
#
# INPUTS REQUIRED:
#   - Heaping data from previous chunks (combined_all)
#   - Events Excel file with columns:
#     * Place: Facility name
#     * event_type: "level", "trend", or "pulse"
#     * start_datetime: When intervention announced/planned
#     * effective_datetime: When intervention takes effect (use this if present)
#     * intensity: Weight/strength of intervention (1-10 scale)
#     * timezone: Timezone for datetimes
#
# OUTPUT:
#   - ITS_CONFIG: Configuration list
#   - Helper functions available for use in subsequent chunks
#   - Informational message confirming setup
#
# WHY ITS + ARIMAX:
#   - Rigorous quasi-experimental design
#   - Accounts for autocorrelation in time series data
#   - Quantifies intervention effect size with confidence intervals
#   - Provides forecasts to project future trends
#   - Publication-ready methodology (widely accepted)
#
# ASSUMPTIONS:
#   - Sufficient pre-intervention data (≥10-20 time points)
#   - No other major changes coinciding with intervention
#   - Linear effects (or model can capture non-linearity)
#   - Stationary after differencing (ARIMA handles this)
# ══════════════════════════════════════════════════════════════════════════════
suppressPackageStartupMessages({
  library(forecast); library(ggplot2); library(knitr); library(kableExtra)
})

# ITS configuration parameters
ITS_CONFIG <- list(
  events_path             = params$events_path,  # Path to events file (set to NULL if no events)
  events_sheet = "events",              # Sheet name in the Excel file
  horizon = 3,                         # Forecast horizon (days)
  seed = 123,
  # Events file column names (adjust if your file differs)
  events_place_col      = "Place",
  events_type_col       = "event_type",
  events_start_dt_col   = "start_datetime",
  events_effective_dt_col = "effective_datetime",
  events_intensity_col  = "intensity",
  events_timezone_col   = "timezone"
)

set.seed(ITS_CONFIG$seed)

# Helper to parse datetimes robustly
parse_dt <- function(x) {
  suppressWarnings(lubridate::parse_date_time(
    as.character(x),
    orders = c("Y-m-d H:M:S","Y-m-d H:M","m/d/Y H:M","d/m/Y H:M","Y/m/d H:M","d-b-Y H:M","d-B-Y H:M","Y-m-d"),
    tz = "UTC"
  ))
}

# Helper functions for intervention regressors
make_cumsum_series <- function(dates, weights, grid_dates) {
  s <- rep(0, length(grid_dates))
  for (i in seq_along(dates)) s <- s + as.integer(grid_dates >= dates[i]) * weights[i]
  s
}

make_ramp_series <- function(dates, weights, grid_dates) {
  r <- rep(0, length(grid_dates))
  for (i in seq_along(dates)) r <- r + pmax(0, as.integer(grid_dates - dates[i]) + 1) * weights[i]
  r
}

cat(knitr::asis_output("> **ITS ARIMAX Analysis** — Analyzing heaping trends with intervention modeling per variable\n"))
```

```{r its-load-events, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}
# ══════════════════════════════════════════════════════════════════════════════
# Load and Process Events (if available)
# ══════════════════════════════════════════════════════════════════════════════

evt <- NULL
events_available <- FALSE

if (!is.null(ITS_CONFIG$events_path) && file.exists(ITS_CONFIG$events_path)) {
  tryCatch({
    # Get sheet name (default to first sheet if not specified)
    sheet_name <- if (!is.null(ITS_CONFIG$events_sheet)) {
      ITS_CONFIG$events_sheet
    } else {
      # List all sheets and use first one
      sheets <- readxl::excel_sheets(ITS_CONFIG$events_path)
      if (length(sheets) == 0) stop("No sheets found in Excel file")
      cat(knitr::asis_output(sprintf("> ℹ️ Using first sheet: '%s'\n", sheets[1])))
      sheets[1]
    }
    
    # Read the specified sheet
    evt_raw <- readxl::read_excel(
      path = ITS_CONFIG$events_path,
      sheet = sheet_name
    )
    
    cat(knitr::asis_output(sprintf("> ℹ️ Reading sheet '%s' from events file\n", sheet_name)))
    cat(knitr::asis_output(sprintf("> ℹ️ Found %d rows with columns: %s\n", 
                                   nrow(evt_raw), 
                                   paste(names(evt_raw), collapse = ", "))))
    
    evt <- evt_raw %>%
      mutate(
        tz  = if (ITS_CONFIG$events_timezone_col %in% names(.)) 
                coalesce(.data[[ITS_CONFIG$events_timezone_col]], "Asia/Makassar") 
              else "Asia/Makassar",
        sd  = if (ITS_CONFIG$events_start_dt_col %in% names(.)) 
                .data[[ITS_CONFIG$events_start_dt_col]] else NA,
        ed  = if (ITS_CONFIG$events_effective_dt_col %in% names(.)) 
                .data[[ITS_CONFIG$events_effective_dt_col]] else NA,
        sd  = parse_dt(sd),
        ed  = parse_dt(ed),
        eff = if_else(!is.na(ed), ed, sd),
        eff_date = as.Date(eff),
        intensity = if (ITS_CONFIG$events_intensity_col %in% names(.))
                      suppressWarnings(as.numeric(.data[[ITS_CONFIG$events_intensity_col]]))
                    else NA_real_,
        intensity = if_else(is.na(intensity), 3, intensity),
        type = if (ITS_CONFIG$events_type_col %in% names(.)) 
                 .data[[ITS_CONFIG$events_type_col]] else "event",
        type = tolower(as.character(type))
      ) %>%
      filter(!is.na(eff_date))
    
    events_available <- nrow(evt) > 0
    
    if (events_available) {
      cat(knitr::asis_output(sprintf("> ✅ Loaded **%d events** from sheet '%s'\n", 
                                     nrow(evt), sheet_name)))
      
      # Show events summary
      evt_summary <- evt %>%
        group_by(type) %>%
        summarise(
          n_events = n(),
          first_date = min(eff_date),
          last_date = max(eff_date),
          avg_intensity = mean(intensity, na.rm = TRUE),
          .groups = "drop"
        )
      
      kable(evt_summary, 
            caption = "Events Summary by Type",
            col.names = c("Event Type", "Count", "First Date", "Last Date", "Avg Intensity")) %>%
        kable_styling(full_width = FALSE) %>%
        print()
      
      # Show detailed events list
      cat("\n")
      evt_detail <- evt %>%
        arrange(eff_date) %>%
        transmute(
          Date = format(eff_date, "%Y-%m-%d"),
          `Event Type` = type,
          Intensity = intensity,
          Place = if ("Place" %in% names(.)) Place else "—"
        )
      
      kable(evt_detail,
            caption = "Detailed Events Timeline",
            align = c("l", "l", "c", "l")) %>%
        kable_styling(full_width = FALSE, font_size = 11) %>%
        print()
      
      # Events timeline visualization
      cat("\n\n")
      
      evt_vis <- evt %>%
        arrange(eff_date) %>%
        mutate(
          type_num = as.numeric(factor(type)),
          label = paste0(type, "\n", format(eff_date, "%b %d"))
        )
      
      p_timeline <- ggplot(evt_vis, aes(x = eff_date, y = type_num, color = type)) +
        geom_segment(aes(xend = eff_date, y = 0, yend = type_num), 
                     linewidth = 1, alpha = 0.7) +
        geom_point(aes(size = intensity), alpha = 0.8) +
        geom_text(aes(label = format(eff_date, "%b %d")), 
                  vjust = -1, size = 3, show.legend = FALSE) +
        scale_size_continuous(range = c(3, 8), name = "Intensity") +
        scale_y_continuous(breaks = unique(evt_vis$type_num),
                          labels = unique(evt_vis$type[order(evt_vis$type_num)])) +
        labs(
          title = "Events Timeline",
          subtitle = "Vertical lines show when interventions occurred",
          x = "Date",
          y = "Event Type",
          color = "Event Type"
        ) +
        theme_minimal(base_size = 12) +
        theme(
          legend.position = "bottom",
          panel.grid.minor = element_blank(),
          axis.text.y = element_text(size = 10)
        )
      
      print(p_timeline)
      cat("\n")
    }
    
  }, error = function(e) {
    error_msg <- e$message
    cat(knitr::asis_output(sprintf("> ⚠️ **Could not load events file**: %s\n\n", error_msg)))
    
    # Provide helpful troubleshooting tips
    cat(knitr::asis_output("**Troubleshooting tips:**\n\n"))
    cat(knitr::asis_output(sprintf("- **File path**: `%s`\n", ITS_CONFIG$events_path)))
    cat(knitr::asis_output(sprintf("- **Sheet name**: `%s`\n", 
                                   if (!is.null(ITS_CONFIG$events_sheet)) ITS_CONFIG$events_sheet else "(auto-detect)")))
    cat(knitr::asis_output("- **Common causes**:\n"))
    cat(knitr::asis_output("  1. File is currently open in Excel (close it and re-run)\n"))
    cat(knitr::asis_output("  2. File is corrupted (try opening in Excel to verify)\n"))
    cat(knitr::asis_output("  3. Sheet name is incorrect (check spelling - case sensitive)\n"))
    cat(knitr::asis_output("  4. File permissions issue (check you have read access)\n"))
    cat(knitr::asis_output("  5. OneDrive sync issue (wait for sync to complete)\n\n"))
    
    # Try to list available sheets if possible
    tryCatch({
      sheets <- readxl::excel_sheets(ITS_CONFIG$events_path)
      if (length(sheets) > 0) {
        cat(knitr::asis_output(sprintf("- **Available sheets in file**: %s\n\n", 
                                       paste(sprintf("`%s`", sheets), collapse = ", "))))
      }
    }, error = function(e2) {
      cat(knitr::asis_output("- Could not list sheets (file may be locked or corrupted)\n\n"))
    })
    
    cat(knitr::asis_output("> ℹ️ Continuing analysis **without events** (trend-only mode)\n"))
  })
} else {
  if (is.null(ITS_CONFIG$events_path)) {
    cat(knitr::asis_output("> ℹ️ No events file specified. Analyzing trends without interventions.\n"))
  } else {
    cat(knitr::asis_output(sprintf("> ⚠️ Events file not found: `%s`\n", ITS_CONFIG$events_path)))
    cat(knitr::asis_output("> ℹ️ Continuing analysis **without events** (trend-only mode)\n"))
  }
}
```

```{r its-prepare-data, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}
# ══════════════════════════════════════════════════════════════════════════════
# Prepare ITS Data per Variable
# ══════════════════════════════════════════════════════════════════════════════

if (!exists("preferred_digits_sum") || !nrow(preferred_digits_sum)) {
  stop("preferred_digits_sum not found. Run heaping calculation chunks first.")
}

# Get unique variables to analyze
its_variables <- unique(preferred_digits_sum$Variabel)

# Display configuration info
cat(knitr::asis_output(sprintf("> **ITS Configuration**: Using heaping data with **%d-day rolling windows** (stride=%d days)\n", 
                               params$heaping_window_days, 
                               params$heaping_stride_days)))
cat(knitr::asis_output(sprintf("> **Analyzing %d variables**: %s\n", 
                               length(its_variables), 
                               paste(its_variables, collapse = ", "))))

# Data quality check
data_quality <- preferred_digits_sum %>%
  group_by(Variabel) %>%
  summarise(
    n_windows = n(),
    n_missing = sum(is.na(pct)),
    pct_missing = round(100 * n_missing / n(), 1),
    date_range_days = as.numeric(max(Tanggal_key, na.rm = TRUE) - min(Tanggal_key, na.rm = TRUE)),
    .groups = "drop"
  )

cat("\n")
kable(data_quality,
      caption = "Heaping Data Quality Summary by Variable",
      col.names = c("Variable", "N Windows", "N Missing", "% Missing", "Date Range (days)")) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover")) %>%
  print()

# Warn about sparse data
sparse_vars <- data_quality %>% filter(pct_missing > 50 | n_windows < 10)
if (nrow(sparse_vars) > 0) {
  cat(knitr::asis_output(sprintf("\n> ⚠️ **Warning**: %d variable(s) have sparse data (>50%% missing or <10 windows): %s\n", 
                                 nrow(sparse_vars), 
                                 paste(sparse_vars$Variabel, collapse = ", "))))
  cat(knitr::asis_output("> ITS analysis may fail or produce unreliable results for these variables.\n"))
}

# Create list to store results per variable
its_results <- list()

```

```{r its-analysis-function, echo=TRUE, message=FALSE, warning=FALSE}
# ══════════════════════════════════════════════════════════════════════════════
# ITS Analysis Function (per variable)
# ══════════════════════════════════════════════════════════════════════════════
run_its_analysis <- function(var_name, heap_data, events_data = NULL, config = ITS_CONFIG) {

  # 1) Subset & siapkan data variabel
  heap_var <- heap_data %>%
    filter(Variabel == var_name) %>%
    transmute(
      date = as.Date(Tanggal_key),
      y    = pct,   # persen heaping (0..100)
      num  = num,
      den  = den
    ) %>%
    arrange(date)

  if (nrow(heap_var) == 0 || all(is.na(heap_var$y))) {
    return(list(success = FALSE, message = "No usable data for this variable"))
  }

  model_df <- heap_var %>% filter(!is.na(y))

  # 2) Kualitas data minimum
  if (nrow(model_df) < 10) {
    return(list(success = FALSE,
                message = sprintf("Only %d observations (need ≥10 for reliable ARIMAX)", nrow(model_df))))
  }

  # 3) Karakteristik sampling: step size & gap
  diffs <- diff(model_df$date)
  step_days <- suppressWarnings(as.integer(round(stats::median(as.numeric(diffs), na.rm = TRUE))))
  if (is.na(step_days) || step_days <= 0) step_days <- 3L  # default stride

  gaps <- model_df %>%
    arrange(date) %>%
    mutate(gap_days = as.numeric(date - lag(date))) %>%
    summarise(
      max_gap      = max(gap_days, na.rm = TRUE),
      n_large_gaps = sum(gap_days > 14, na.rm = TRUE),
      date_span    = as.numeric(max(date) - min(date)),
      .groups = "drop"
    )

  if (is.finite(gaps$max_gap) && gaps$max_gap > 21) {
    return(list(success = FALSE,
                message = sprintf("Data too sparse: max gap = %d days (>21)", gaps$max_gap)))
  }
  if (is.finite(gaps$n_large_gaps) && gaps$n_large_gaps > 3) {
    return(list(success = FALSE,
                message = sprintf("Data too fragmented: %d gaps >14 days", gaps$n_large_gaps)))
  }
  if (is.finite(gaps$date_span) && gaps$date_span < 28) {
    return(list(success = FALSE,
                message = sprintf("Date span too short: %d days (need ≥28 for weekly patterns)", gaps$date_span)))
  }

  grid_dates <- model_df$date

  # 4) Bangun regresor intervensi (pulse/step/ramp) pada grid_dates
  X_df  <- NULL
  xcols <- character(0)

  if (!is.null(events_data) && nrow(events_data) > 0) {

    # Pulse by type per tanggal
    pulse_by_type <- events_data %>%
      transmute(date = as.Date(eff_date),
                type = as.character(type),
                w    = as.numeric(intensity)) %>%
      group_by(date, type) %>%
      summarise(w = sum(w, na.rm = TRUE), .groups = "drop") %>%
      tidyr::complete(date = grid_dates, type = unique(type), fill = list(w = 0)) %>%
      tidyr::pivot_wider(names_from = type, values_from = w, values_fill = 0) %>%
      arrange(date)

    types <- setdiff(names(pulse_by_type), "date")
    if (length(types) == 0) types <- "event"

    step_df <- tibble(date = grid_dates)
    ramp_df <- tibble(date = grid_dates)

    for (tp in types) {
      dts <- as.Date(events_data$eff_date[events_data$type == tp])
      wts <- as.numeric(events_data$intensity[events_data$type == tp])
      if (length(dts) == 0) {
        step_df[[tp]] <- 0
        ramp_df[[tp]] <- 0
      } else {
        step_df[[tp]] <- make_cumsum_series(dts, wts, grid_dates)
        ramp_df[[tp]] <- make_ramp_series(dts, wts, grid_dates)
      }
    }

    X_df <- tibble(date = grid_dates) %>%
      dplyr::left_join(dplyr::rename_with(pulse_by_type, ~ paste0("pulse_", .x), -date), by = "date") %>%
      dplyr::left_join(dplyr::rename_with(step_df,       ~ paste0("step_",  .x), -date), by = "date") %>%
      dplyr::left_join(dplyr::rename_with(ramp_df,       ~ paste0("ramp_",  .x), -date), by = "date") %>%
      mutate(across(-date, ~ replace(., is.na(.), 0)))

    # Distributed lag 3 langkah untuk pulse (kausal)
    decay_kernel <- c(1, 0.7, 0.5)
    for (nm in names(X_df)[grepl("^pulse_", names(X_df))]) {
      z <- stats::filter(X_df[[nm]], filter = decay_kernel, method = "convolution", sides = 1)
      X_df[[paste0(nm, "_dl")]] <- replace(as.numeric(z), is.na(z), 0)
    }

    xcols <- c(
      names(X_df)[grepl("^pulse_.*_dl$", names(X_df))],
      names(X_df)[grepl("^step_",        names(X_df))],
      names(X_df)[grepl("^ramp_",        names(X_df))]
    )
    xcols <- setdiff(xcols, "date")
  }

  # Gabungkan X ke model_df
  if (!is.null(X_df)) {
    model_df <- model_df %>% left_join(X_df, by = "date") %>% arrange(date)
  }

  if (nrow(model_df) < 10) {
    return(list(success = FALSE,
                message = sprintf("Insufficient data after merging: %d observations", nrow(model_df))))
  }

  # 5) Siapkan ts + Fourier (hanya jika freq > 1)
  y <- model_df$y

  obs_per_week <- 7 / step_days
  freq <- max(1L, as.integer(round(obs_per_week)))

  y_ts <- stats::ts(y, frequency = freq)

  ft <- NULL
  K_fourier <- NULL
  if (freq > 1) {
    K_fourier <- if (freq >= 4) 2 else 1
    K_fourier <- max(1L, min(K_fourier, floor(freq / 2)))
    ft <- forecast::fourier(y_ts, K = K_fourier)
  }

  # 6) Fit ARIMA/ARIMAX + drop kolom X yang konstan
  X_train <- if (length(xcols) > 0) as.matrix(model_df[, xcols, drop = FALSE]) else NULL
  if (!is.null(X_train)) {
    keep <- which(apply(X_train, 2, function(z) sd(z, na.rm = TRUE)) > 0)
    if (length(keep) == 0) X_train <- NULL else X_train <- X_train[, keep, drop = FALSE]
  }

  xreg_train <- NULL
  interventions_used <- FALSE

  if (!is.null(X_train) && !is.null(ft)) {
    xreg_train <- cbind(X_train, ft)
    fit <- tryCatch(
      forecast::auto.arima(y_ts, xreg = xreg_train, seasonal = FALSE,
                           stepwise = TRUE, approximation = TRUE),
      error = function(e) {
        interventions_used <<- FALSE
        forecast::auto.arima(y_ts, xreg = ft, seasonal = FALSE,
                             stepwise = TRUE, approximation = TRUE)
      }
    )
    interventions_used <- any(grepl("^pulse_|^step_|^ramp_", names(coef(fit))))
  } else if (!is.null(X_train) && is.null(ft)) {
    xreg_train <- X_train
    fit <- tryCatch(
      forecast::auto.arima(y_ts, xreg = xreg_train, seasonal = FALSE,
                           stepwise = TRUE, approximation = TRUE),
      error = function(e) {
        interventions_used <<- FALSE
        forecast::auto.arima(y_ts, seasonal = FALSE,
                             stepwise = TRUE, approximation = TRUE)
      }
    )
    interventions_used <- any(grepl("^pulse_|^step_|^ramp_", names(coef(fit))))
  } else if (is.null(X_train) && !is.null(ft)) {
    xreg_train <- ft
    fit <- forecast::auto.arima(y_ts, xreg = xreg_train, seasonal = FALSE,
                                stepwise = TRUE, approximation = TRUE)
    interventions_used <- FALSE
  } else {
    fit <- forecast::auto.arima(y_ts, seasonal = FALSE,
                                stepwise = TRUE, approximation = TRUE)
    interventions_used <- FALSE
  }

  # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
  # NEW: Tentukan persis kolom xreg yang DIPAKAI model (berdasarkan coef names)
  coef_nms <- names(coef(fit))
  reg_names_used <- character(0)
  if (!is.null(X_train)) {
    reg_names_used <- c(reg_names_used, intersect(colnames(X_train), coef_nms))
  }
  if (!is.null(ft)) {
    reg_names_used <- c(reg_names_used, intersect(colnames(ft), coef_nms))
  }
  reg_names_used <- unique(reg_names_used)
  # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

  # 7) Fitted, counterfactual, efek titik
  mu_hat <- as.numeric(fitted(fit))
  mu_cf  <- mu_hat
  if (!is.null(X_train) && length(reg_names_used) > 0) {
    inter_names <- intersect(colnames(X_train), reg_names_used)   # hanya intervensi, exclude Fourier
    if (length(inter_names) > 0) {
      b   <- coef(fit)[inter_names]
      eff <- as.matrix(X_train[, inter_names, drop = FALSE]) %*% b
      mu_cf <- as.numeric(mu_hat - eff)
    }
  }

  # Clip untuk tampilan; effect dihitung vs counterfactual ter-clip agar realistis
  fitted_clip         <- pmax(0, pmin(100, mu_hat))
  counterfactual_clip <- pmax(0, pmin(100, mu_cf))
  point_effect        <- model_df$y - counterfactual_clip

  df_out <- model_df %>%
    mutate(
      fitted         = fitted_clip,
      counterfactual = counterfactual_clip,
      point_effect   = point_effect
    )

  # 8) Ringkas efek rata-rata setelah intervensi (kalau ada event)
  post_start <- if (!is.null(events_data) && nrow(events_data) > 0) {
    min(as.Date(events_data$eff_date), na.rm = TRUE)
  } else NA

  avg_effect <- if (!is.na(post_start)) {
    df_out %>%
      filter(date >= post_start) %>%
      summarize(
        avg_effect = mean(point_effect, na.rm = TRUE),
        cum_effect = sum(point_effect,  na.rm = TRUE),
        .groups = "drop"
      )
  } else {
    tibble(avg_effect = NA_real_, cum_effect = NA_real_)
  }

  # 9) Forecast: horizon(hari) -> langkah (step_days)
  h_steps <- max(1L, as.integer(ceiling(config$horizon / step_days)))
  last_date <- max(model_df$date)
  future_dates <- seq(last_date + step_days, by = step_days, length.out = h_steps)

  # Siapkan future xreg sesuai kolom yang DIPAKAI model
  ft_future <- if (!is.null(ft)) forecast::fourierf(y_ts, K = K_fourier, h = h_steps) else NULL

  X_future <- NULL
  if (!is.null(X_train)) {
    X_future_df <- tibble(date = future_dates)
    types <- if (!is.null(events_data)) unique(as.character(events_data$type)) else character(0)

    if (!is.null(events_data) && length(types) > 0) {
      for (tp in types) {
        dts <- as.Date(events_data$eff_date[events_data$type == tp])
        wts <- as.numeric(events_data$intensity[events_data$type == tp])

        if (paste0("step_", tp) %in% colnames(X_train)) {
          X_future_df[[paste0("step_", tp)]] <- make_cumsum_series(dts, wts, future_dates)
        }
        if (paste0("ramp_", tp) %in% colnames(X_train)) {
          X_future_df[[paste0("ramp_", tp)]] <- make_ramp_series(dts, wts, future_dates)
        }
        pulse_cols <- grep(paste0("^pulse_", tp), colnames(X_train), value = TRUE)
        for (pc in pulse_cols) X_future_df[[pc]] <- 0
        pulse_dl_cols <- grep(paste0("^pulse_", tp, ".*_dl$"), colnames(X_train), value = TRUE)
        for (pc in pulse_dl_cols) X_future_df[[pc]] <- 0
      }
    }

    # Pastikan semua kolom xreg intervensi yang dipakai training ada di future (isi 0 bila tidak)
    for (nm in colnames(X_train)) {
      if (!nm %in% names(X_future_df)) X_future_df[[nm]] <- 0
    }
    X_future <- as.matrix(X_future_df[, colnames(X_train), drop = FALSE])
  }

  # Rakit raw future design (X_future + ft_future)
  mats <- list()
  if (!is.null(X_train))   mats[[1]] <- X_future
  if (!is.null(ft_future)) mats[[2]] <- ft_future
  xreg_future_raw <- if (length(mats) > 0) do.call(cbind, mats) else NULL

  # Bangun newxreg hanya dengan kolom yang DIPAKAI model, dan urutan yang sama
  xreg_future <- NULL
  if (!is.null(xreg_future_raw) && length(reg_names_used) > 0) {
    missing <- setdiff(reg_names_used, colnames(xreg_future_raw))
    if (length(missing)) {
      for (nm in missing) {
        xreg_future_raw <- cbind(
          xreg_future_raw,
          setNames(matrix(0, nrow = nrow(xreg_future_raw), ncol = 1), nm)
        )
      }
    }
    xreg_future <- xreg_future_raw[, reg_names_used, drop = FALSE]
  }

  # Forecast with/without xreg sesuai hasil fit
  fc <- if (is.null(xreg_future)) {
    forecast::forecast(fit, h = h_steps)
  } else {
    forecast::forecast(fit, xreg = xreg_future, h = h_steps)
  }

  # Clip hasil forecast
  df_fc <- tibble(
    date = future_dates,
    mean = pmax(0, pmin(100, as.numeric(fc$mean))),
    lo80 = pmax(0, as.numeric(fc$lower[, "80%"])),
    hi80 = pmin(100, as.numeric(fc$upper[, "80%"])),
    lo95 = pmax(0, as.numeric(fc$lower[, "95%"])),
    hi95 = pmin(100, as.numeric(fc$upper[, "95%"]))
  )

  # 10) Tabel koefisien intervensi (jika ada di model)
  coef_tbl <- NULL
  if (!is.null(X_train) && interventions_used) {
    beta_names  <- names(coef(fit))
    inter_names <- intersect(colnames(X_train), beta_names)
    if (length(inter_names) > 0) {
      est <- coef(fit)[inter_names]
      V   <- vcov(fit)
      se  <- sqrt(diag(V))[inter_names]
      z   <- est / se
      p   <- 2 * pnorm(abs(z), lower.tail = FALSE)
      coef_tbl <- tibble(
        term      = inter_names,
        estimate  = as.numeric(est),
        std_error = as.numeric(se),
        t_stat    = as.numeric(z),
        p_value   = as.numeric(p)
      )
    }
  }

  list(
    success            = TRUE,
    variable           = var_name,
    fit                = fit,
    data               = df_out,
    forecast           = df_fc,
    effects            = avg_effect,
    coefficients       = coef_tbl,
    n_obs              = nrow(model_df),
    date_range         = c(min(model_df$date), max(model_df$date)),
    frequency          = freq,
    obs_per_week       = round(obs_per_week, 2),
    step_days          = step_days,
    interventions_used = interventions_used
  )
}


```

```{r its-run-all, echo=TRUE, message=FALSE, warning=FALSE, results='asis'}
# ══════════════════════════════════════════════════════════════════════════════
# Run ITS Analysis for All Variables
# ══════════════════════════════════════════════════════════════════════════════

for (var in its_variables) {
  cat(sprintf("\n\n## %s\n\n", var))
  
  result <- tryCatch({
    run_its_analysis(var, preferred_digits_sum, evt, ITS_CONFIG)
  }, error = function(e) {
    list(success = FALSE, message = e$message)
  })
  
  if (!result$success) {
    cat(knitr::asis_output(sprintf("> ⚠️ **Analysis failed for %s**: %s\n", var, result$message)))
    next
  }
  
  its_results[[var]] <- result
  
  # Print summary info
  cat(knitr::asis_output(sprintf("> **Date range**: %s to %s (%d observations, ~%.1f obs/week)\n", 
                                 result$date_range[1], result$date_range[2], result$n_obs, result$obs_per_week)))
  cat(knitr::asis_output(sprintf("> **Time series frequency**: %d (used for seasonal modeling)\n", 
                                 result$frequency)))
  
  # Indicate if interventions were included
  if (!is.null(evt) && nrow(evt) > 0) {
    if (result$interventions_used) {
      cat(knitr::asis_output("> **Model type**: ARIMAX with interventions ✅\n"))
    } else {
      cat(knitr::asis_output("> **Model type**: ARIMA only (interventions failed - trend-only model) ⚠️\n"))
    }
  } else {
    cat(knitr::asis_output("> **Model type**: ARIMA only (no events specified)\n"))
  }
  
  # Model summary
  cat("\n### Model Summary\n\n")
  cat("```\n")
  print(summary(result$fit))
  cat("```\n\n")
  
  # Effects table
  if (nrow(result$effects) > 0) {
    cat("\n### Intervention Effects\n\n")
    result$effects %>%
      kable(digits = 3, 
            caption = paste("Average and cumulative effect for", var),
            col.names = c("Average Effect (%)", "Cumulative Effect (%)")) %>%
      kable_styling(full_width = FALSE) %>%
      print()
    cat("\n")
  }
  
  # Coefficients table
  if (!is.null(result$coefficients) && nrow(result$coefficients) > 0) {
    cat("\n### Intervention Coefficients\n\n")
    result$coefficients %>%
      mutate(
        sig = case_when(
          p_value < 0.001 ~ "***",
          p_value < 0.01  ~ "**",
          p_value < 0.05  ~ "*",
          p_value < 0.1   ~ ".",
          TRUE ~ ""
        )
      ) %>%
      kable(digits = 4, 
            caption = paste("Intervention coefficients for", var),
            col.names = c("Term", "Estimate", "Std Error", "t-statistic", "p-value", "")) %>%
      kable_styling(full_width = FALSE) %>%
      print()
    cat("\n")
  }
  
  # Plot: Observed vs Counterfactual
  cat("\n### Observed vs Counterfactual\n\n")
  
  p <- ggplot(result$data, aes(date)) +
    geom_line(aes(y = y, color = "Observed"), linewidth = 1) +
    geom_line(aes(y = counterfactual, color = "Counterfactual"), linewidth = 1, linetype = 2) +
    geom_line(aes(y = fitted, color = "Fitted"), linewidth = 0.7, alpha = 0.6) +
    scale_color_manual(
      values = c("Observed" = "#2C7BE5", "Counterfactual" = "#E74C3C", "Fitted" = "#95A5A6"),
      name = NULL
    ) +
    labs(
      title = paste(var, "— ITS Analysis (Observed vs Counterfactual)"),
      subtitle = "Blue = Observed data | Red dashed = Counterfactual (no intervention) | Grey = Model fit",
      x = NULL, 
      y = "Heaping: Total Preferred Digits (%)"
    ) +
    theme_minimal(base_size = 12) +
    theme(legend.position = "bottom")
  
  # Add event lines with labels if available
  if (!is.null(evt) && nrow(evt) > 0) {
    # Prepare event data with distinct colors per type
    evt_plot <- evt %>%
      distinct(eff_date, type, intensity) %>%
      arrange(eff_date)
    
    event_types <- unique(evt_plot$type)
    event_colors <- setNames(
      c("#D62728", "#FF7F0E", "#2CA02C", "#9467BD", "#8C564B", "#E377C2")[1:length(event_types)],
      event_types
    )
    
    # Add vertical lines for each event with type-specific colors
    for (et in event_types) {
      evt_sub <- evt_plot %>% filter(type == et)
      p <- p + geom_vline(
        data = evt_sub,
        aes(xintercept = eff_date),
        linewidth = 0.8, 
        color = event_colors[et], 
        alpha = 0.7,
        linetype = "dashed"
      )
    }
    
    # Add event labels at the top of the plot
    y_max <- max(c(result$data$y, result$data$counterfactual), na.rm = TRUE)
    evt_labels <- evt_plot %>%
      group_by(type) %>%
      slice(1) %>%
      ungroup() %>%
      mutate(
        label = paste0(type, "\n(", format(eff_date, "%b %d"), ")"),
        y_pos = y_max * 0.98
      )
    
    p <- p + geom_text(
      data = evt_labels,
      aes(x = eff_date, y = y_pos, label = label),
      angle = 90, vjust = -0.3, hjust = 1,
      size = 3, color = "black", fontface = "bold"
    )
    
    # Update subtitle to mention events
    event_summary <- evt_plot %>%
      group_by(type) %>%
      summarise(n = n(), .groups = "drop") %>%
      mutate(label = paste0(type, " (n=", n, ")")) %>%
      pull(label) %>%
      paste(collapse = ", ")
    
    p <- p + labs(
      subtitle = paste0(
        "Blue = Observed | Red dashed = Counterfactual (no intervention) | Grey = Fitted\n",
        "Events: ", event_summary
      )
    )
  }
  
  print(p)
  cat("\n")
  
  # Plot: Point Effects Over Time
  cat("\n### Point Effects Over Time\n\n")
  
  p_effects <- ggplot(result$data, aes(date, point_effect)) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "gray50", linewidth = 0.5) +
    geom_line(linewidth = 1, color = "#E67E22") +
    geom_point(size = 2, color = "#E67E22", alpha = 0.6) +
    labs(
      title = paste(var, "— Intervention Effects on Heaping"),
      subtitle = "Point-wise effect (Observed - Counterfactual). Positive = intervention increased heaping",
      x = NULL,
      y = "Effect on Heaping (%)"
    ) +
    theme_minimal(base_size = 12) +
    theme(legend.position = "bottom")
  
  # Add event lines with labels
  if (!is.null(evt) && nrow(evt) > 0) {
    evt_plot <- evt %>%
      distinct(eff_date, type, intensity) %>%
      arrange(eff_date)
    
    event_types <- unique(evt_plot$type)
    event_colors <- setNames(
      c("#D62728", "#FF7F0E", "#2CA02C", "#9467BD", "#8C564B", "#E377C2")[1:length(event_types)],
      event_types
    )
    
    # Add vertical lines for each event type with distinct colors
    for (et in event_types) {
      evt_sub <- evt_plot %>% filter(type == et)
      p_effects <- p_effects + geom_vline(
        data = evt_sub,
        aes(xintercept = eff_date),
        linewidth = 0.8, 
        color = event_colors[et], 
        alpha = 0.7,
        linetype = "dashed"
      )
    }
    
    # Add event labels
    y_range <- range(result$data$point_effect, na.rm = TRUE)
    y_label_pos <- if (y_range[2] > abs(y_range[1])) y_range[2] * 0.95 else y_range[1] * 0.95
    
    evt_labels <- evt_plot %>%
      group_by(type) %>%
      slice(1) %>%
      ungroup() %>%
      mutate(
        label = paste0(type, "\n", format(eff_date, "%b %d")),
        y_pos = y_label_pos
      )
    
    p_effects <- p_effects + geom_text(
      data = evt_labels,
      aes(x = eff_date, y = y_pos, label = label),
      angle = 90, vjust = ifelse(y_label_pos > 0, -0.3, 1.3), hjust = 1,
      size = 3, color = "black", fontface = "bold"
    )
    
    # Update subtitle
    event_summary <- evt_plot %>%
      group_by(type) %>%
      summarise(n = n(), .groups = "drop") %>%
      mutate(label = paste0(type, " (n=", n, ")")) %>%
      pull(label) %>%
      paste(collapse = ", ")
    
    p_effects <- p_effects + labs(
      subtitle = paste0(
        "Point-wise effect (Observed - Counterfactual). Positive = intervention increased heaping\n",
        "Events: ", event_summary
      )
    )
  }
  
  print(p_effects)
  cat("\n")
  
  # Residual diagnostics
  cat("\n### Residual Diagnostics\n\n")
  checkresiduals(result$fit)
  cat("\n")
  
  # Forecast table
  cat("\n### Forecast (Next", ITS_CONFIG$horizon, "days)\n\n")
  result$forecast %>%
    kable(digits = 2, 
          caption = paste("Forecast for", var),
          col.names = c("Date", "Mean", "Lower 80%", "Upper 80%", "Lower 95%", "Upper 95%")) %>%
    kable_styling(full_width = FALSE) %>%
    print()
  cat("\n")
  
  # Forecast plot
  cat("\n### Forecast Plot\n\n")
  
  # Combine historical and forecast
  hist_tail <- result$data %>%
    tail(30) %>%
    select(date, y) %>%
    mutate(type = "Historical")
  
  fc_plot_data <- result$forecast %>%
    select(date, mean, lo80, hi80, lo95, hi95) %>%
    mutate(type = "Forecast")
  
  p_fc <- ggplot() +
    geom_line(data = hist_tail, aes(date, y), linewidth = 1, color = "#2C7BE5") +
    geom_ribbon(data = fc_plot_data, 
                aes(date, ymin = lo95, ymax = hi95), 
                fill = "#95A5A6", alpha = 0.2) +
    geom_ribbon(data = fc_plot_data, 
                aes(date, ymin = lo80, ymax = hi80), 
                fill = "#95A5A6", alpha = 0.3) +
    geom_line(data = fc_plot_data, 
              aes(date, mean), 
              linewidth = 1, color = "#E67E22") +
    labs(
      title = paste(var, "— Forecast"),
      subtitle = sprintf("Last 30 days + %d-day forecast (80%% and 95%% intervals)", ITS_CONFIG$horizon),
      x = NULL,
      y = "Heaping: Total Preferred Digits (%)"
    ) +
    theme_minimal(base_size = 12)
  
  print(p_fc)
  cat("\n")
}

# Summary across all variables
if (length(its_results) > 0) {
  cat("\n\n## Summary Across All Variables\n\n")
  
  summary_df <- map_dfr(its_results, function(r) {
    if (r$success) {
      tibble(
        Variable = r$variable,
        N_Obs = r$n_obs,
        Start_Date = as.character(r$date_range[1]),
        End_Date = as.character(r$date_range[2]),
        Avg_Effect = r$effects$avg_effect,
        Cum_Effect = r$effects$cum_effect,
        ARIMA_Order = paste0(arimaorder(r$fit), collapse = ",")
      )
    }
  })
  
  cat("\n### Summary Table\n\n")
  summary_df %>%
    kable(digits = 3,
          caption = "ITS Analysis Summary — All Variables",
          col.names = c("Variable", "N Obs", "Start Date", "End Date", 
                       "Avg Effect (%)", "Cum Effect (%)", "ARIMA Order")) %>%
    kable_styling(full_width = FALSE) %>%
    print()
  
  # Intervention coefficients comparison across variables
  if (!is.null(evt) && nrow(evt) > 0) {
    cat("\n\n### Intervention Effects Comparison\n\n")
    
    # Combine all point effects for comparison plot
    all_effects <- map_dfr(its_results, function(r) {
      if (r$success && !is.null(r$data)) {
        r$data %>%
          select(date, point_effect) %>%
          mutate(Variable = r$variable)
      }
    })
    
    if (nrow(all_effects) > 0) {
      # Event data for overlay
      evt_plot <- evt %>%
        distinct(eff_date, type) %>%
        arrange(eff_date)
      
      event_types <- unique(evt_plot$type)
      event_colors <- setNames(
        c("#D62728", "#FF7F0E", "#2CA02C", "#9467BD", "#8C564B", "#E377C2")[1:length(event_types)],
        event_types
      )
      
      # Faceted plot showing effects across all variables
      p_comparison <- ggplot(all_effects, aes(date, point_effect, color = Variable)) +
        geom_hline(yintercept = 0, linetype = "dashed", color = "gray40", linewidth = 0.4) +
        geom_line(linewidth = 0.7, alpha = 0.8) +
        facet_wrap(~ Variable, ncol = 2, scales = "free_y") +
        labs(
          title = "Intervention Effects Comparison Across Variables",
          subtitle = "How did each variable respond to interventions? (Dashed lines = events)",
          x = NULL,
          y = "Effect on Heaping (%)"
        ) +
        theme_minimal(base_size = 11) +
        theme(
          legend.position = "none",
          strip.background = element_rect(fill = "#f0f0f0", color = NA),
          strip.text = element_text(face = "bold", size = 10)
        )
      
      # Add event lines
      for (et in event_types) {
        evt_sub <- evt_plot %>% filter(type == et)
        p_comparison <- p_comparison + geom_vline(
          data = evt_sub,
          aes(xintercept = eff_date),
          linewidth = 0.5, 
          color = event_colors[et], 
          alpha = 0.6,
          linetype = "dashed"
        )
      }
      
      print(p_comparison)
      cat("\n")
      
      # Combined overlay plot (all variables together)
      cat("\n### All Variables Overlay\n\n")
      
      p_overlay <- ggplot(all_effects, aes(date, point_effect, color = Variable)) +
        geom_hline(yintercept = 0, linetype = "dashed", color = "gray40", linewidth = 0.4) +
        geom_line(linewidth = 0.9, alpha = 0.7) +
        labs(
          title = "Intervention Effects — All Variables Overlay",
          subtitle = "Do variables respond similarly to interventions?",
          x = NULL,
          y = "Effect on Heaping (%)",
          color = "Variable"
        ) +
        theme_minimal(base_size = 12) +
        theme(legend.position = "bottom")
      
      # Add event lines
      for (et in event_types) {
        evt_sub <- evt_plot %>% filter(type == et)
        p_overlay <- p_overlay + geom_vline(
          data = evt_sub,
          aes(xintercept = eff_date),
          linewidth = 0.6, 
          color = event_colors[et], 
          alpha = 0.6,
          linetype = "dotted"
        )
      }
      
      print(p_overlay)
      cat("\n")
    }
    
    # Coefficient comparison table with FDR correction
    cat("\n### Intervention Coefficients by Variable\n\n")
    
    coef_comparison <- map_dfr(its_results, function(r) {
      if (r$success && !is.null(r$coefficients) && nrow(r$coefficients) > 0) {
        r$coefficients %>%
          mutate(Variable = r$variable) %>%
          select(Variable, term, estimate, std_error, p_value)
      }
    })
    
    if (nrow(coef_comparison) > 0) {
      # Apply FDR correction for multiple testing
      coef_comparison <- coef_comparison %>%
        group_by(term) %>%
        mutate(
          p_value_fdr = p.adjust(p_value, method = "fdr"),
          sig_raw = case_when(
            p_value < 0.001 ~ "***",
            p_value < 0.01  ~ "**",
            p_value < 0.05  ~ "*",
            p_value < 0.1   ~ ".",
            TRUE ~ ""
          ),
          sig_fdr = case_when(
            p_value_fdr < 0.001 ~ "***",
            p_value_fdr < 0.01  ~ "**",
            p_value_fdr < 0.05  ~ "*",
            p_value_fdr < 0.1   ~ ".",
            TRUE ~ ""
          )
        ) %>%
        ungroup()
      
      # Wide format table showing both raw and FDR-adjusted significance
      coef_wide <- coef_comparison %>%
        mutate(
          est_label = sprintf("%.3f%s", estimate, sig_fdr),  # Use FDR-adjusted significance
          p_label = sprintf("%.4f", p_value_fdr)
        ) %>%
        select(Variable, term, est_label, p_label) %>%
        tidyr::pivot_wider(names_from = term, values_from = c(est_label, p_label), values_fill = "—")
      
      kable(coef_wide,
            caption = "Intervention Coefficients Across Variables (FDR-adjusted: *** p<0.001, ** p<0.01, * p<0.05, . p<0.1)",
            align = c("l", rep("r", ncol(coef_wide) - 1))) %>%
        kable_styling(full_width = FALSE, font_size = 10) %>%
        print()
      
      cat("\n> **Interpretation**: \n")
      cat("> - Positive coefficients indicate the intervention *increased* heaping; negative coefficients indicate *decreased* heaping.\n")
      cat("> - P-values are **FDR-adjusted** to control for multiple testing across variables.\n")
      
      # Summary of significant effects
      sig_effects <- coef_comparison %>%
        filter(p_value_fdr < 0.05) %>%
        arrange(p_value_fdr)
      
      if (nrow(sig_effects) > 0) {
        cat("\n\n#### Significant Effects (FDR-adjusted p < 0.05)\n\n")
        sig_effects %>%
          transmute(
            Variable = Variable,
            Term = term,
            Estimate = sprintf("%.3f", estimate),
            `Std Error` = sprintf("%.3f", std_error),
            `P-value` = sprintf("%.4f", p_value),
            `P-value (FDR)` = sprintf("%.4f", p_value_fdr),
            Direction = ifelse(estimate > 0, "↑ Increased heaping", "↓ Decreased heaping")
          ) %>%
          kable(align = c("l", "l", "r", "r", "r", "r", "l")) %>%
          kable_styling(full_width = FALSE) %>%
          print()
      } else {
        cat("\n> ℹ️ No statistically significant effects after FDR correction.\n")
      }
    } else {
      cat("> No intervention coefficients available for comparison.\n")
    }
  }
}

```


